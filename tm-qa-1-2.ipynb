{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "IPython.notebook.set_autosave_interval(180000)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autosaving every 180 seconds\n"
     ]
    }
   ],
   "source": [
    "%autosave 180"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import convex as cx\n",
    "import requests\n",
    "import time\n",
    "import itertools\n",
    "import re\n",
    "#import numpy\n",
    "from copy import copy\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "#from pprint import pprint\n",
    "import time\n",
    "import json\n",
    "import os\n",
    "import networkx as nx\n",
    "from math import sqrt\n",
    "import spacy\n",
    "from hdt import HDTDocument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdt_wd = HDTDocument(\"data/kb/wikidata2018_09_11.hdt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nlp = spacy.load(\"en_core_web_lg\")\n",
    "nlp = spacy.load(\"/data/users/romain.claret/tm/wiki-kb-linked-entities/nlp_custom_6\")\n",
    "#print(nlp.pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load settings\n",
    "with open( \"settings-tmqa-1.json\", \"r\") as settings_data:\n",
    "    settings = json.load(settings_data)\n",
    "    use_cache = settings['use_cache']\n",
    "    save_cache = settings['save_cache']\n",
    "    cache_path = settings['cache_path']\n",
    "#cache_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_cache_data():\n",
    "    if save_cache:\n",
    "        with open(os.path.join(cache_path,'statements_dict.json'), 'wb') as outfile:\n",
    "            outfile.write(json.dumps(statements_dict, separators=(',',':')).encode('utf8'))\n",
    "        with open(os.path.join(cache_path,'wd_labels_dict.json'), 'wb') as outfile:\n",
    "            outfile.write(json.dumps(wd_labels_dict, separators=(',',':')).encode('utf8'))\n",
    "        with open(os.path.join(cache_path,'wd_word_ids_dict.json'), 'wb') as outfile:\n",
    "            outfile.write(json.dumps(wd_word_ids_dict, separators=(',',':')).encode('utf8'))\n",
    "        with open(os.path.join(cache_path,'wd_predicate_ids_dict.json'), 'wb') as outfile:\n",
    "            outfile.write(json.dumps(wd_predicate_ids_dict, separators=(',',':')).encode('utf8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load statements cache\n",
    "use_cache = False\n",
    "if use_cache:\n",
    "    path_statements_dict = \"statements_dict.json\"\n",
    "    path_wd_labels_dict = 'wd_labels_dict.json'\n",
    "    path_wd_word_ids_dict = 'wd_word_ids_dict.json'\n",
    "    path_wd_predicate_ids_dict = 'wd_predicate_ids_dict.json'\n",
    "else:\n",
    "    path_statements_dict = \"statements_dict_empty.json\"\n",
    "    path_wd_labels_dict = 'wd_labels_dict_empty.json'\n",
    "    path_wd_word_ids_dict = 'wd_word_ids_dict_empty.json'\n",
    "    path_wd_predicate_ids_dict = 'wd_predicate_ids_dict_empty.json'\n",
    "\n",
    "with open(os.path.join(cache_path,path_statements_dict), \"rb\") as data:\n",
    "    statements_dict = json.load(data)\n",
    "with open(os.path.join(cache_path,path_wd_labels_dict), \"rb\") as data:\n",
    "    wd_labels_dict = json.load(data)\n",
    "with open(os.path.join(cache_path,path_wd_word_ids_dict), \"rb\") as data:\n",
    "    wd_word_ids_dict = json.load(data)\n",
    "with open(os.path.join(cache_path,path_wd_predicate_ids_dict), \"rb\") as data:\n",
    "    wd_predicate_ids_dict = json.load(data)\n",
    "\n",
    "#print(\"len(statements_dict)\",len(statements_dict))\n",
    "#print(\"len(wd_labels_dict)\",len(wd_labels_dict))\n",
    "#print(\"len(wd_word_ids_dict)\",len(wd_word_ids_dict))\n",
    "#print(\"len(wd_predicate_ids_dict)\",len(wd_predicate_ids_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_kb_ents(text):\n",
    "    #doc = nlp_kb(text)\n",
    "    doc = nlp(text)\n",
    "    #for ent in doc.ents:\n",
    "    #    print(\" \".join([\"ent\", ent.text, ent.label_, ent.kb_id_]))\n",
    "    return doc.ents\n",
    "        \n",
    "#ent_text_test = (\n",
    "#    \"In The Hitchhiker's Guide to the Galaxy, written by Douglas Adams, \"\n",
    "#    \"Douglas reminds us to always bring our towel, even in China or Brazil. \"\n",
    "#    \"The main character in Doug's novel is the man Arthur Dent, \"\n",
    "#    \"but Dougledydoug doesn't write about George Washington or Homer Simpson.\"\n",
    "#)\n",
    "#\n",
    "#en_text_test_2 = (\"Which actor voiced the Unicorn in The Last Unicorn?\")\n",
    "#\n",
    "#print([ent.kb_id_ for ent in get_kb_ents(ent_text_test)])\n",
    "#[ent.kb_id_ for ent in get_kb_ents(en_text_test_2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nlp(sentence):\n",
    "    return nlp(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#questions = [ \n",
    "#    \"Which actor voiced the Unicorn in The Last Unicorn?\",\n",
    "#    \"And Alan Arkin was behind...?\",\n",
    "#    \"And Alan Arkin be behind...? Why How when which was happy make fun\",\n",
    "#    \"Who is the composer of the soundtrack?\",\n",
    "#    \"So who performed the songs?\",\n",
    "#    \"Genre of this band's music?\",\n",
    "#    \"By the way, who was the director?\"\n",
    "#            ]\n",
    "#\n",
    "#q_test = str(\"Which actor voiced the Unicorn in The Last Unicorn? \"+\n",
    "#    \"And Alan Arkin was behind...? \"+\n",
    "#    \"And Alan Arkin be behind...? Why How when which was happy make fun. \"+\n",
    "#    \"Who is the composer of the soundtrack? \"+\n",
    "#    \"So who performed songs? \"+\n",
    "#    \"Genre of this band's music? \"+\n",
    "#    \"By the way, who was the director? \")\n",
    "#\n",
    "#q_test_2 = \"Who is the wife of Barack Obama?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#q0_nlp = get_nlp(questions[0])\n",
    "#q0_nlp_test = get_nlp(q_test)\n",
    "#q0_nlp_test_2 = get_nlp(q_test_2)\n",
    "#print(q0_nlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_wd_entity(to_check):\n",
    "    pattern = re.compile('^Q[0-9]*$')\n",
    "    if pattern.match(to_check.strip()): return True\n",
    "    else: return False\n",
    "\n",
    "def is_wd_predicate(to_check):\n",
    "    pattern = re.compile('^P[0-9]*$')\n",
    "    if pattern.match(to_check.strip()): return True\n",
    "    else: return False\n",
    "    \n",
    "def is_valide_wd_id(to_check):\n",
    "    if is_wd_entity(to_check) or is_wd_predicate(to_check): return True\n",
    "    else: return False\n",
    "\n",
    "#print(is_wd_entity(\"Q155\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO redo the functions and optimize\n",
    "# TODO add cache\n",
    "def is_entity_or_literal(wd_object):\n",
    "    if is_wd_entity(wd_object.strip()):\n",
    "        return True\n",
    "    pattern = re.compile('^[A-Za-z0-9]*$')\n",
    "    if len(wd_object) == 32 and pattern.match(wd_object.strip()):\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "# return if the given string is a literal or a date\n",
    "def is_literal_or_date (answer): \n",
    "    return not('www.wikidata.org' in answer)\n",
    "\n",
    "# return if the given string describes a year in the format YYYY\n",
    "def is_year(year):\n",
    "    pattern = re.compile('^[0-9][0-9][0-9][0-9]$')\n",
    "    if not(pattern.match(year.strip())):\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "\n",
    "# return if the given string is a date\n",
    "def is_date(date):\n",
    "    pattern = re.compile('^[0-9]+ [A-z]+ [0-9][0-9][0-9][0-9]$')\n",
    "    if not(pattern.match(date.strip())):\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "\n",
    "# return if the given string is a timestamp\n",
    "def is_timestamp(timestamp):\n",
    "    pattern = re.compile('^[0-9][0-9][0-9][0-9]-[0-9][0-9]-[0-9][0-9]T00:00:00Z')\n",
    "    if not(pattern.match(timestamp.strip())):\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "\n",
    "# convert the given month to a number\n",
    "def convert_month_to_number(month):\n",
    "    return{\n",
    "        \"january\" : \"01\",\n",
    "        \"february\" : \"02\",\n",
    "        \"march\" : \"03\",\n",
    "        \"april\" : \"04\",\n",
    "        \"may\" : \"05\",\n",
    "        \"june\" : \"06\",\n",
    "        \"july\" : \"07\",\n",
    "        \"august\" : \"08\",\n",
    "        \"september\" : \"09\", \n",
    "        \"october\" : \"10\",\n",
    "        \"november\" : \"11\",\n",
    "        \"december\" : \"12\"\n",
    "    }[month.lower()]\n",
    "\n",
    "# convert a date from the wikidata frontendstyle to timestamp style\n",
    "def convert_date_to_timestamp (date):\t\n",
    "    sdate = date.split(\" \")\n",
    "    # add the leading zero\n",
    "    if (len(sdate[0]) < 2):\n",
    "        sdate[0] = \"0\" + sdate[0]\n",
    "    return sdate[2] + '-' + convert_month_to_number(sdate[1]) + '-' + sdate[0] + 'T00:00:00Z'\n",
    "\n",
    "# convert a year to timestamp style\n",
    "def convert_year_to_timestamp(year):\n",
    "    return year + '-01-01T00:00:00Z'\n",
    "\n",
    "# get the wikidata id of a wikidata url\n",
    "def wikidata_url_to_wikidata_id(url):\n",
    "    if not url:\n",
    "        return False\n",
    "    if \"XMLSchema#dateTime\" in url or \"XMLSchema#decimal\" in url:\n",
    "        date = url.split(\"\\\"\", 2)[1]\n",
    "        date = date.replace(\"+\", \"\")\n",
    "        return date\n",
    "    if(is_literal_or_date(url)):\n",
    "        if is_year(url):\n",
    "            return convert_year_to_timestamp(url)\n",
    "        if is_date(url):\n",
    "            return convert_date_to_timestamp(url)\n",
    "        else:\n",
    "            url = url.replace(\"\\\"\", \"\")\n",
    "            return url\n",
    "    else:\n",
    "        url_array = url.split('/')\n",
    "        # the wikidata id is always in the last component of the id\n",
    "        return url_array[len(url_array)-1]\n",
    "    \n",
    "# fetch all statements where the given qualifier statement occurs as subject\n",
    "def get_all_statements_with_qualifier_as_subject(qualifier):\n",
    "    statements = []\n",
    "    triples, cardinality = hdt_wd.search_triples(qualifier, \"\", \"\")\n",
    "    for triple in triples:\n",
    "        sub, pre, obj = triple\n",
    "        # only consider triples with a wikidata-predicate\n",
    "        if pre.startswith(\"http://www.wikidata.org/\"):\n",
    "            statements.append({'entity': sub, 'predicate': pre, 'object': obj})\n",
    "    return statements\n",
    "\n",
    "# fetch the statement where the given qualifier statement occurs as object\n",
    "def get_statement_with_qualifier_as_object(qualifier):\n",
    "    triples, cardinality = hdt_wd.search_triples(\"\", \"\", qualifier)\n",
    "    for triple in triples:\n",
    "        sub, pre, obj = triple\n",
    "        # only consider triples with a wikidata-predicate\n",
    "        if pre.startswith(\"http://www.wikidata.org/\") and sub.startswith(\"http://www.wikidata.org/entity/Q\"):\n",
    "            return (sub, pre, obj)\n",
    "    return False\n",
    "\n",
    "# returns all statements that involve the given entity\n",
    "def get_all_statements_of_entity(entity_id):\n",
    "    # check entity pattern\n",
    "    if not is_wd_entity(entity_id.strip()):\n",
    "        return False\n",
    "    if statements_dict.get(entity_id) != None:\n",
    "        #print(\"saved statement\")\n",
    "        return statements_dict[entity_id]\n",
    "    entity = \"http://www.wikidata.org/entity/\"+entity_id\n",
    "    statements = []\n",
    "    # entity as subject\n",
    "    triples_sub, cardinality_sub = hdt_wd.search_triples(entity, \"\", \"\")\n",
    "    # entity as object\n",
    "    triples_obj, cardinality_obj = hdt_wd.search_triples(\"\", \"\", entity)\n",
    "    if cardinality_sub + cardinality_obj > 5000:\n",
    "        statements_dict[entity_id] = []\n",
    "        return []\n",
    "    # iterate through all triples in which the entity occurs as the subject\n",
    "    for triple in triples_sub:\n",
    "        sub, pre, obj = triple\n",
    "        # only consider triples with a wikidata-predicate or if it is an identifier predicate\n",
    "        if not pre.startswith(\"http://www.wikidata.org/\"):# or (wikidata_url_to_wikidata_id(pre) in identifier_predicates):\n",
    "            continue\n",
    "        # object is statement\n",
    "        if obj.startswith(\"http://www.wikidata.org/entity/statement/\"):\n",
    "            qualifier_statements = get_all_statements_with_qualifier_as_subject(obj)\n",
    "            qualifiers = []\n",
    "            for qualifier_statement in qualifier_statements:\n",
    "                if qualifier_statement['predicate'] == \"http://www.wikidata.org/prop/statement/\" + wikidata_url_to_wikidata_id(pre):\n",
    "                        obj = qualifier_statement['object']\n",
    "                elif is_entity_or_literal(wikidata_url_to_wikidata_id(qualifier_statement['object'])):\n",
    "                    qualifiers.append({\n",
    "                        \"qualifier_predicate\":{\n",
    "                            \"id\": wikidata_url_to_wikidata_id(qualifier_statement['predicate'])\n",
    "                        }, \n",
    "                        \"qualifier_object\":{\t\n",
    "                            \"id\": wikidata_url_to_wikidata_id(qualifier_statement['object'])\n",
    "                        }})\n",
    "            statements.append({'entity': {'id': wikidata_url_to_wikidata_id(sub)}, 'predicate': {'id': wikidata_url_to_wikidata_id(pre)}, 'object': {'id': wikidata_url_to_wikidata_id(obj)}, 'qualifiers': qualifiers})\n",
    "        else:\n",
    "            statements.append({'entity': {'id': wikidata_url_to_wikidata_id(sub)}, 'predicate': {'id': wikidata_url_to_wikidata_id(pre)}, 'object': {'id': wikidata_url_to_wikidata_id(obj)}, 'qualifiers': []})\n",
    "    # iterate through all triples in which the entity occurs as the object\n",
    "    for triple in triples_obj:\n",
    "        sub, pre, obj = triple\n",
    "        # only consider triples with an entity as subject and a wikidata-predicate or if it is an identifier predicate\n",
    "        if not sub.startswith(\"http://www.wikidata.org/entity/Q\"):# or not pre.startswith(\"http://www.wikidata.org/\") or wikidata_url_to_wikidata_id(pre) in identifier_predicates:\n",
    "            continue\n",
    "        if sub.startswith(\"http://www.wikidata.org/entity/statement/\"):\n",
    "            statements_with_qualifier_as_object =  get_statement_with_qualifier_as_object(sub, process)\n",
    "            # if no statement was found continue\n",
    "            if not statements_with_qualifier_as_object:\n",
    "                continue\n",
    "            main_sub, main_pred, main_obj = statements_with_qualifier_as_object\n",
    "            qualifier_statements = get_all_statements_with_qualifier_as_subject(sub)\n",
    "            qualifiers = []\n",
    "            for qualifier_statement in qualifier_statements:\n",
    "                if wikidata_url_to_wikidata_id(qualifier_statement['predicate']) == wikidata_url_to_wikidata_id(main_pred):\n",
    "                    main_obj = qualifier_statement['object']\n",
    "                elif is_entity_or_literal(wikidata_url_to_wikidata_id(qualifier_statement['object'])):\n",
    "                    qualifiers.append({\n",
    "                        \"qualifier_predicate\":{\"id\": wikidata_url_to_wikidata_id(qualifier_statement['predicate'])}, \n",
    "                        \"qualifier_object\":{\"id\": wikidata_url_to_wikidata_id(qualifier_statement['object'])}\n",
    "                    })\n",
    "            statements.append({\n",
    "                            'entity': {'id': wikidata_url_to_wikidata_id(main_sub)},\n",
    "                            'predicate': {'id': wikidata_url_to_wikidata_id(main_pred)},\n",
    "                            'object': {'id': wikidata_url_to_wikidata_id(main_obj)},\n",
    "                            'qualifiers': qualifiers\n",
    "                              })\n",
    "        else:\n",
    "            statements.append({'entity': {'id': wikidata_url_to_wikidata_id(sub)}, 'predicate': {'id': wikidata_url_to_wikidata_id(pre)}, 'object': {'id': wikidata_url_to_wikidata_id(obj)}, 'qualifiers': []})\n",
    "    # cache the data\n",
    "    statements_dict[entity_id] = statements\n",
    "    return statements\n",
    "\n",
    "#print(len(get_all_statements_of_entity(\"Q16614390\")))\n",
    "#save_cache_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wd_ids_online(name, is_predicate=False, top_k=3):\n",
    "    name = name.split('(')[0]\n",
    "    \n",
    "    if is_predicate and wd_predicate_ids_dict.get(name) != None:\n",
    "        #print(\"saved predicate online\")\n",
    "        return wd_predicate_ids_dict[name]\n",
    "    elif not is_predicate and wd_word_ids_dict.get(name) != None:\n",
    "        #print(\"saved word online\")\n",
    "        return wd_word_ids_dict[name]\n",
    "\n",
    "    request_successfull = False\n",
    "    entity_ids = \"\"\n",
    "    while not request_successfull:\n",
    "        try:\n",
    "            if is_predicate:\n",
    "                entity_ids = requests.get('https://www.wikidata.org/w/api.php?action=wbsearchentities&format=json&language=en&type=property&limit=' + str(top_k) + '&search='+name).json()\n",
    "            else:\n",
    "                entity_ids = requests.get('https://www.wikidata.org/w/api.php?action=wbsearchentities&format=json&language=en&limit=' + str(top_k) + '&search='+name).json()\n",
    "            request_successfull = True\n",
    "        except:\n",
    "            time.sleep(5)\n",
    "    results = entity_ids.get(\"search\")\n",
    "    if not results:\n",
    "        if is_predicate: wd_predicate_ids_dict[name] = \"\"\n",
    "        else: wd_word_ids_dict[name] = \"\"\n",
    "        return \"\"\n",
    "    if not len(results):\n",
    "        if is_predicate: wd_predicate_ids_dict[name] = \"\"\n",
    "        else: wd_word_ids_dict[name] = \"\"\n",
    "        return \"\"\n",
    "    res = []\n",
    "    for result in results:\n",
    "        res.append(result['id'])\n",
    "    \n",
    "    if is_predicate: wd_predicate_ids_dict[name] = res\n",
    "    else: wd_word_ids_dict[name] = res\n",
    "    \n",
    "    return res\n",
    "#print(get_wd_ids_online(\"voiced\", is_predicate=False, top_k=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# very computational\n",
    "def get_most_similar(word, topn=5):\n",
    "  word = nlp.vocab[str(word)]\n",
    "  queries = [w for w in word.vocab if w.is_lower == word.is_lower and w.prob >= -15]\n",
    "  by_similarity = sorted(queries, key=lambda w: word.similarity(w), reverse=True)\n",
    "  return [(w.lower_,w.similarity(word)) for w in by_similarity[:topn+1] if w.lower_ != word.lower_]\n",
    "\n",
    "#print(get_most_similar(\"voiced\", topn=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wd_ids(word, is_predicate=False, top_k=3, limit=10):\n",
    "    if is_predicate and wd_predicate_ids_dict.get(word) != None:\n",
    "        #print(\"saved predicate local\")\n",
    "        return wd_predicate_ids_dict[word]\n",
    "    elif not is_predicate and wd_word_ids_dict.get(word) != None:\n",
    "        #print(\"saved word local\")\n",
    "        return wd_word_ids_dict[word]\n",
    "    \n",
    "    language = \"en\"\n",
    "    word_formated = str(\"\\\"\"+word+\"\\\"\"+\"@\"+language)\n",
    "    to_remove = len(\"http://www.wikidata.org/entity/\")\n",
    "    t_name, card_name = hdt_wd.search_triples(\"\", \"http://schema.org/name\", word_formated, limit=top_k)\n",
    "    #print(\"names cardinality of \\\"\" + word+\"\\\": %i\" % card_name)\n",
    "    t_alt, card_alt = hdt_wd.search_triples(\"\", 'http://www.w3.org/2004/02/skos/core#altLabel', word_formated, limit=top_k)\n",
    "    #print(\"alternative names cardinality of \\\"\" + word+\"\\\": %i\" % card_alt)\n",
    "    results = list(set(\n",
    "        [t[0][to_remove:] for t in t_name if is_valide_wd_id(t[0][to_remove:])] + \n",
    "        [t[0][to_remove:] for t in t_alt if is_valide_wd_id(t[0][to_remove:])]\n",
    "           ))\n",
    "    \n",
    "    if is_predicate: results = [r for r in results if is_wd_predicate(r)]\n",
    "        \n",
    "    # cache the data\n",
    "    if is_predicate: wd_predicate_ids_dict[word] = results\n",
    "    else: wd_word_ids_dict[word] = results\n",
    "    \n",
    "    return results if limit<=0 else results[:limit]\n",
    "     \n",
    "    \n",
    "#get_wd_ids(\"The Last Unicorn\", top_k=0, limit=10)\n",
    "#print(get_wd_ids(\"wife\", is_predicate=False , top_k=0, limit=0))\n",
    "#print(get_wd_ids(\"voiced\", is_predicate=False , top_k=0, limit=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wd_label(from_id):\n",
    "    #print(\"from_id\",from_id)\n",
    "    if is_valide_wd_id(from_id):\n",
    "        if wd_labels_dict.get(from_id) != None:\n",
    "            #print(\"saved label local\")\n",
    "            return wd_labels_dict[from_id]\n",
    "        \n",
    "        language = \"en\"\n",
    "        id_url = \"http://www.wikidata.org/entity/\"+from_id\n",
    "        t_name, card_name = hdt_wd.search_triples(id_url, \"http://schema.org/name\", \"\")\n",
    "        name = [t[2].split('\\\"@en')[0].replace(\"\\\"\", \"\") for t in t_name if \"@\"+language in t[2]]\n",
    "        result = name[0] if name else ''\n",
    "        wd_labels_dict[from_id] = result #caching\n",
    "        return result\n",
    "        \n",
    "    else:\n",
    "        return ''\n",
    "    \n",
    "#print(get_wd_label(\"P725\"))\n",
    "#get_wd_label(\"Q20789322\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "# Building colors from graph\n",
    "def get_color(node_type):\n",
    "    if node_type == \"entity\": return \"violet\"#\"cornflowerblue\"\n",
    "    elif node_type == \"predicate\": return \"yellow\"\n",
    "    else: return \"red\"\n",
    "\n",
    "# Building labels for graph\n",
    "def get_elements_from_graph(graph):\n",
    "    node_names = nx.get_node_attributes(graph,\"name\")\n",
    "    node_types = nx.get_node_attributes(graph,\"type\")\n",
    "    colors = [get_color(node_types[n]) for n in node_names]\n",
    "    return node_names, colors\n",
    "\n",
    "# Plotting the graph\n",
    "def plot_graph(graph, name, title=\"Graph\"):\n",
    "    fig = plt.figure(figsize=(14,14))\n",
    "    ax = plt.subplot(111)\n",
    "    ax.set_title(str(\"answer: \"+title), fontsize=10)\n",
    "    #pos = nx.spring_layout(graph)\n",
    "    labels, colors = get_elements_from_graph(graph)\n",
    "    nx.draw(graph, node_size=30, node_color=colors, font_size=10, font_weight='bold', with_labels=True, labels=labels)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"tmqa1_graphs_imgs/\"+str(name)+\".png\", format=\"PNG\", dpi = 300)\n",
    "    plt.show()\n",
    "    \n",
    "#plot_graph(graph, \"file_name_graph\", \"Graph_title\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_statements_graph(statements, indexing_predicates=True):\n",
    "    graph = nx.Graph()\n",
    "    turn=0\n",
    "    predicate_nodes = {}\n",
    "\n",
    "    for statement in statements:\n",
    "        #print(statement)\n",
    "        if not statement['entity']['id'] in graph:\n",
    "            graph.add_node(statement['entity']['id'], name=get_wd_label(statement['entity']['id']), type='entity', turn=turn)\n",
    "        if not statement['object']['id'] in graph:\n",
    "            graph.add_node(statement['object']['id'], name=get_wd_label(statement['object']['id']), type='entity', turn=turn)\n",
    "\n",
    "        # increment index of predicate or set it at 0\n",
    "        if not statement['predicate']['id'] in predicate_nodes or not indexing_predicates:\n",
    "            predicate_nodes_index = 1\n",
    "            predicate_nodes[statement['predicate']['id']] = 1\n",
    "        else:\n",
    "            predicate_nodes[statement['predicate']['id']] += 1\n",
    "            predicate_nodes_index = predicate_nodes[statement['predicate']['id']]\n",
    "\n",
    "        # add the predicate node\n",
    "        predicate_node_id = (statement['predicate']['id'])\n",
    "        if indexing_predicates: predicate_node_id += \"-\" + str(predicate_nodes_index)\n",
    "        \n",
    "        graph.add_node(predicate_node_id, name=get_wd_label(statement['predicate']['id']), type='predicate', turn=turn)\n",
    "\n",
    "        # add the two edges (entity->predicate->object)\n",
    "        graph.add_edge(statement['entity']['id'], predicate_node_id)\n",
    "        graph.add_edge(predicate_node_id, statement['object']['id'])\n",
    "    \n",
    "    return graph, predicate_nodes\n",
    "\n",
    "#test_graph = make_statements_graph(test_unduplicate_statements, indexing_predicates=False)\n",
    "#print(test_graph[1])\n",
    "#plot_graph(test_graph[0],\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_lists(list_1, list_2):\n",
    "    if len(list_1) == len(list_2):\n",
    "        return [(list_1[i], list_2[i]) for i in range(0, len(list_1))]\n",
    "    else:\n",
    "        return \"Error: lists are not the same lenght\"\n",
    "\n",
    "#print(merge_lists([\"author\"],['P50']))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_themes(nlp_question, top_k=3):\n",
    "    # PART1: finding themes as the user typed it\n",
    "    filter_list = [\"PART\", \"PRON\"]\n",
    "    nlp_list = list(nlp_question)\n",
    "    for i,w in enumerate(nlp_question):\n",
    "        #print(w.pos_)\n",
    "        if w.pos_ in filter_list:\n",
    "            #print(w.pos_)\n",
    "            del nlp_list[i]\n",
    "    nlp_question = get_nlp(\" \".join([e.text for e in nlp_list]))\n",
    "    \n",
    "    themes = [(ent, [ent.kb_id_]) for ent in get_kb_ents(nlp_question.text) if ent.kb_id_ != \"NIL\"]\n",
    "    theme_complements = []\n",
    "    \n",
    "    noun_chunks = [chunk for chunk in nlp_question.noun_chunks]\n",
    "    theme_ids = [get_wd_ids(chunk.text, top_k=top_k) for chunk in noun_chunks]\n",
    "\n",
    "    for i, chunk in enumerate(theme_ids):\n",
    "        if chunk: themes.append((noun_chunks[i], chunk))\n",
    "        else: theme_complements.append(noun_chunks[i])\n",
    "    \n",
    "    # PART2: finding themes with the question capitalized\n",
    "    #print(nlp_question)\n",
    "    nlp_list_cap = []\n",
    "    nlp_list_low = []\n",
    "    nlp_list_lemma = []\n",
    "    nlp_list_no_det = []\n",
    "    w_filter = [\"WDT\",\"WP\",\"WP$\",\"WRB\"]\n",
    "    for w in nlp_question:\n",
    "        if w.tag_ not in w_filter:\n",
    "            nlp_list_cap.append(w.text.capitalize())\n",
    "            nlp_list_low.append(w.text.lower())\n",
    "            nlp_list_lemma.append(w.lemma_)\n",
    "        if w.pos_ == \"DET\":\n",
    "            nlp_list_no_det.append(w.text)\n",
    "            \n",
    "    nlp_question_cap = get_nlp(\" \".join([e for e in nlp_list_cap]))\n",
    "    nlp_question_low = get_nlp(\" \".join([e for e in nlp_list_low]))\n",
    "    nlp_question_lemma = get_nlp(\" \".join([e for e in nlp_list_lemma]))\n",
    "    nlp_question_no_det = get_nlp(\" \".join([e for e in nlp_list_no_det]))\n",
    "\n",
    "    themes += [(ent, [ent.kb_id_]) for ent in get_kb_ents(nlp_question_cap.text) if ent.kb_id_ != \"NIL\" and (ent, [ent.kb_id_]) not in themes]\n",
    "    themes += [(ent, [ent.kb_id_]) for ent in get_kb_ents(nlp_question_low.text) if ent.kb_id_ != \"NIL\" and (ent, [ent.kb_id_]) not in themes]\n",
    "    themes += [(ent, [ent.kb_id_]) for ent in get_kb_ents(nlp_question_lemma.text) if ent.kb_id_ != \"NIL\" and (ent, [ent.kb_id_]) not in themes]\n",
    "    themes += [(ent, [ent.kb_id_]) for ent in get_kb_ents(nlp_question_no_det.text) if ent.kb_id_ != \"NIL\" and (ent, [ent.kb_id_]) not in themes]\n",
    "    \n",
    "    #print(themes)\n",
    "    \n",
    "    noun_chunks = [chunk for chunk in nlp_question_cap.noun_chunks]\n",
    "    theme_ids = [get_wd_ids(chunk.text, top_k=top_k) for chunk in noun_chunks]\n",
    "\n",
    "    for i, chunk in enumerate(theme_ids):\n",
    "        if chunk: themes.append((noun_chunks[i], chunk))\n",
    "        else: theme_complements.append(noun_chunks[i])\n",
    "    \n",
    "    noun_chunks = [chunk for chunk in nlp_question_low.noun_chunks]\n",
    "    theme_ids = [get_wd_ids(chunk.text, top_k=top_k) for chunk in noun_chunks]\n",
    "\n",
    "    for i, chunk in enumerate(theme_ids):\n",
    "        if chunk: themes.append((noun_chunks[i], chunk))\n",
    "        else: theme_complements.append(noun_chunks[i])\n",
    "            \n",
    "    noun_chunks = [chunk for chunk in nlp_question_lemma.noun_chunks]\n",
    "    theme_ids = [get_wd_ids(chunk.text, top_k=top_k) for chunk in noun_chunks]\n",
    "\n",
    "    for i, chunk in enumerate(theme_ids):\n",
    "        if chunk: themes.append((noun_chunks[i], chunk))\n",
    "        else: theme_complements.append(noun_chunks[i])\n",
    "            \n",
    "    noun_chunks = [chunk for chunk in nlp_question_no_det.noun_chunks]\n",
    "    theme_ids = [get_wd_ids(chunk.text, top_k=top_k) for chunk in noun_chunks]\n",
    "\n",
    "    for i, chunk in enumerate(theme_ids):\n",
    "        if chunk: themes.append((noun_chunks[i], chunk))\n",
    "        else: theme_complements.append(noun_chunks[i])\n",
    "    \n",
    "    themes_filtered = []\n",
    "    for t in themes:\n",
    "        if t[0].text in [tf[0].text for tf in themes_filtered]:\n",
    "            index = [tf[0].text for tf in themes_filtered].index(t[0].text)\n",
    "            tmp = t[1]+[i for j in [tf[1] for index, tf in enumerate(themes_filtered) if tf[0].text == t[0].text] for i in j]\n",
    "            themes_filtered[index] = (t[0],tmp)\n",
    "\n",
    "        else:\n",
    "            themes_filtered.append(t)\n",
    "    #print(themes_filtered)\n",
    "    return themes_filtered, theme_complements\n",
    "\n",
    "#q0_themes = get_themes(q0_nlp, top_k=3)\n",
    "#q0_themes_test = get_themes(q0_nlp_test)\n",
    "#q0_themes_test_2 = get_themes(q0_nlp_test_2)\n",
    "#print(q0_themes)\n",
    "\n",
    "#q_test_3 = get_nlp(\"the unicorn and the raccoons love obama barack's tacos\")\n",
    "#q_test_3_themes = get_themes(q_test_3, top_k=3)\n",
    "#print(get_enhanced_themes(q_test_3_themes))\n",
    "#print(q_test_3_themes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_theme_tuples(theme_list, top_k=3):\n",
    "    return [(t, get_wd_ids(t, top_k=top_k)) for t in theme_list]\n",
    "\n",
    "def get_theme_no_stopwords(theme_list):\n",
    "    return [s for s in theme_list if not s.is_stop]\n",
    "\n",
    "def get_theme_lemmatized(theme_list):\n",
    "    return [s.lemma_ for s in theme_list]\n",
    "\n",
    "def get_permutation_tuples(theme_list, start=2):\n",
    "    permutations = []\n",
    "    for i in range(start, len(theme_list)+1):\n",
    "        permutations += itertools.permutations(theme_list,i)\n",
    "    return permutations\n",
    "\n",
    "def get_lemma_permutation_tuples(theme_list, start=2):\n",
    "    return get_permutation_tuples(get_theme_lemmatized(theme_list), start=2)\n",
    "\n",
    "def get_non_token_tuples(theme_list):\n",
    "    return [\" \".join([e for e in list(l)]) for l in theme_list]\n",
    "\n",
    "def get_non_token_lower_tuples(theme_list):\n",
    "    return [\" \".join([e.lower() for e in list(l)]) for l in theme_list]\n",
    "\n",
    "def get_non_token_capitalize_tuples(theme_list):\n",
    "    return [\" \".join([c.capitalize() for c in [e for e in list(l)]]) for l in theme_list] \n",
    "\n",
    "def get_text_tuples(theme_list):\n",
    "    return [\" \".join([e.text for e in list(l)]) for l in theme_list]\n",
    "\n",
    "def get_lower_tuples(theme_list):\n",
    "    return [\" \".join([e.lower_ for e in list(l)]) for l in theme_list]\n",
    "\n",
    "def get_capitalized_tuples(theme_list):\n",
    "    return [\" \".join([c.capitalize() for c in [e.text for e in list(l)]]) for l in theme_list]\n",
    "\n",
    "def get_enhanced_themes(themes, top_k=3, aggressive=False):\n",
    "    enhanced_themes = []\n",
    "    # permute, capitalize, lowering of the words in the complements\n",
    "    for c in themes[1]:\n",
    "\n",
    "        per_lemma = get_theme_tuples(get_non_token_tuples([n for n in get_permutation_tuples(get_theme_lemmatized(c))]),top_k)\n",
    "        per_nostop = get_theme_tuples(get_text_tuples(get_permutation_tuples(get_theme_no_stopwords(c),start=1)),top_k)\n",
    "        per_lemma_nostop = get_theme_tuples(get_non_token_tuples([get_theme_lemmatized(s) for s in get_permutation_tuples(get_theme_no_stopwords(c),start=1)]),top_k)\n",
    "    \n",
    "        per_lemma_lower = get_theme_tuples(get_non_token_lower_tuples([n for n in get_permutation_tuples(get_theme_lemmatized(c))]),top_k)\n",
    "        per_nostop_lower = get_theme_tuples(get_lower_tuples(get_permutation_tuples(get_theme_no_stopwords(c),start=1)),top_k)\n",
    "        per_lemma_nostop_lower = get_theme_tuples(get_non_token_lower_tuples([get_theme_lemmatized(s) for s in get_permutation_tuples(get_theme_no_stopwords(c),start=1)]),top_k)\n",
    "        \n",
    "        per_lemma_capitalize = get_theme_tuples(get_non_token_capitalize_tuples([n for n in get_permutation_tuples(get_theme_lemmatized(c))]),top_k)\n",
    "        per_nostop_capitalize = get_theme_tuples(get_capitalized_tuples(get_permutation_tuples(get_theme_no_stopwords(c),start=1)),top_k)\n",
    "        per_lemma_nostop_capitalize = get_theme_tuples(get_non_token_capitalize_tuples([get_theme_lemmatized(s) for s in get_permutation_tuples(get_theme_no_stopwords(c),start=1)]),top_k)\n",
    "\n",
    "        per = get_theme_tuples(get_text_tuples(get_permutation_tuples(c)),top_k)\n",
    "        per_lower = get_theme_tuples(get_lower_tuples(get_permutation_tuples(c)),top_k)\n",
    "        per_capitalize = get_theme_tuples(get_capitalized_tuples(get_permutation_tuples(c)),top_k)\n",
    "        \n",
    "        for p in (per + per_lower + per_capitalize +\n",
    "                 per_lemma + per_lemma_lower + per_lemma_capitalize +\n",
    "                 per_nostop + per_nostop_lower + per_nostop_capitalize +\n",
    "                 per_lemma_nostop + per_lemma_nostop_lower + per_lemma_nostop_capitalize):\n",
    "            if p[1] and p not in enhanced_themes: enhanced_themes.append(p)\n",
    "    \n",
    "    if aggressive:\n",
    "        predicates = []\n",
    "        [predicates.append(get_wd_label(pred)) for pred in sum([p[1] for p in themes[0]],[]) if get_wd_label(pred) not in predicates]\n",
    "        predicates_ids = [get_wd_ids_online(p, is_predicate=True, top_k=top_k) for p in predicates]\n",
    "        predicated_themes = merge_lists(predicates, predicates_ids)\n",
    "        predicated_themes = [pt for pt in predicated_themes if pt[1] != '']\n",
    "        if predicates: enhanced_themes += predicated_themes\n",
    "    \n",
    "    return enhanced_themes\n",
    "\n",
    "#q_test_3 = get_nlp(\"the unicorn and the raccoons love obama barack's tacos\")\n",
    "#q_test_3 = get_nlp(\"what was the cause of death of yves klein\")\n",
    "#q_test_3_themes = get_themes(q_test_3, top_k=3)\n",
    "#print(q_test_3_themes[0])\n",
    "#print(get_enhanced_themes(q_test_3_themes, aggressive=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predicates_online(nlp_sentence, top_k=3, aggressive=False):\n",
    "    if aggressive: predicates = [p for p in nlp_sentence if p.pos_ == \"VERB\" or p.pos_ == \"AUX\" or p.tag_ == \"IN\"]\n",
    "    else: predicates = [p for p in nlp_sentence if p.pos_ == \"VERB\" or p.pos_ == \"AUX\"]\n",
    "    predicates_ids = [get_wd_ids_online(p.text, is_predicate=True, top_k=top_k) for p in predicates]\n",
    "    return merge_lists(predicates, predicates_ids)\n",
    "\n",
    "#q0_predicates = get_predicates_online(q0_nlp, top_k=3)\n",
    "#q0_predicates_test_2 = get_predicates_online(q0_nlp_test_2, top_k=3, aggressive=True)\n",
    "#print(q0_predicates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predicates(nlp_sentence, top_k=3):\n",
    "    predicates = [p for p in nlp_sentence if p.pos_ == \"VERB\" or p.pos_ == \"AUX\"]\n",
    "    predicates_ids = [get_wd_ids(p.lemma_, is_predicate=True, top_k=0, limit=0) for p in predicates]\n",
    "    return merge_lists(predicates, predicates_ids)\n",
    "\n",
    "#q0_nlp_test_0 = get_nlp(\"Voiced\")\n",
    "#q0_predicates = get_predicates(q0_nlp, top_k=3)\n",
    "#q0_predicates_test_2 = get_predicates(q0_nlp_test_2, top_k=3)\n",
    "#print(q0_predicates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_ids(to_extract):\n",
    "    return [i for i in itertools.chain.from_iterable([id[1] for id in to_extract])]\n",
    "#extract_ids([('name', ['id'])]) #q0_themes[0] #q0_focused_parts #q0_predicates\n",
    "#print(extract_ids([(\"The Last Unicorn\", ['Q16614390']),(\"Second Theme\", ['Q12345'])]))\n",
    "#extract_ids(q0_focused_parts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_similarity_by_words(nlp_word_from, nlp_word_to):\n",
    "    if not nlp_word_from or not nlp_word_to:\n",
    "        return 0\n",
    "    elif not nlp_word_from.vector_norm or not nlp_word_to.vector_norm:\n",
    "        return 0\n",
    "    else:\n",
    "        return nlp_word_from.similarity(nlp_word_to)\n",
    "\n",
    "#print(get_similarity_by_words(get_nlp(\"character role\"), get_nlp(\"voice actor\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_similarity_by_ids(word_id_from, word_id_to):\n",
    "    nlp_word_from = get_nlp(get_wd_label(word_id_from))\n",
    "    nlp_word_to = get_nlp(get_wd_label(word_id_to))\n",
    "    return get_similarity_by_words(nlp_word_from, nlp_word_to)\n",
    "\n",
    "#print(get_similarity_by_ids(\"P453\", \"P725\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_similar_statements(statements, from_token_id, similar_to_name, top_k=3, qualifier=False, statement_type=\"object\"):\n",
    "    highest_matching_similarity = -1\n",
    "    top_statements = []\n",
    "    nlp_name = get_nlp(similar_to_name)\n",
    "    \n",
    "    if get_wd_label(from_token_id):\n",
    "        for statement in statements:\n",
    "            if qualifier:\n",
    "                if statement.get('qualifiers'):\n",
    "                    for qualifier in statement['qualifiers']:\n",
    "                        nlp_word_to = get_nlp(get_wd_label(qualifier[statement_type]['id']))\n",
    "                        matching_similarity = get_similarity_by_words(nlp_name, nlp_word_to)\n",
    "                        if highest_matching_similarity == -1 or matching_similarity > highest_matching_similarity:\n",
    "                            highest_matching_similarity = matching_similarity\n",
    "                            best_statement = statement\n",
    "                            top_statements.append((highest_matching_similarity, best_statement))\n",
    "            else:\n",
    "                nlp_word_to = get_nlp(get_wd_label(statement[statement_type]['id']))\n",
    "                matching_similarity = get_similarity_by_words(nlp_name, nlp_word_to)\n",
    "                if highest_matching_similarity == -1 or matching_similarity > highest_matching_similarity:\n",
    "                    highest_matching_similarity = matching_similarity\n",
    "                    best_statement = statement\n",
    "                    top_statements.append((highest_matching_similarity, best_statement))\n",
    "            \n",
    "    return sorted(top_statements, key=lambda x: x[0], reverse=True)[:top_k]\n",
    "\n",
    "#statements = get_all_statements_of_entity('Q176198')\n",
    "#top_similar_statements = get_top_similar_statements(statements, 'Q176198', 'voiced')\n",
    "#print(top_similar_statements[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_similar_statements_by_word(from_token_ids, similar_to_name, top_k=3, qualifier=False, statement_type=\"object\"):\n",
    "    best_statements = []\n",
    "    for token in from_token_ids:\n",
    "        statements = get_all_statements_of_entity(token)\n",
    "        if statements: best_statements += get_top_similar_statements(statements, token, similar_to_name, top_k=top_k, qualifier=qualifier, statement_type=statement_type)\n",
    "\n",
    "    return sorted(best_statements, key=lambda x: x[0], reverse=True)[:top_k]\n",
    "\n",
    "#best_similar_statements = get_best_similar_statements_by_word(extract_ids(q0_themes[0]), 'voiced', top_k=3, qualifier=True, statement_type=\"qualifier_object\")\n",
    "#print(best_similar_statements[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_statements_subjects_labels(statements):\n",
    "    return [get_wd_label(t[1]['entity']['id']) for t in statements]\n",
    "#print(get_statements_subjects_labels(best_similar_statements))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_statements_predicates_labels(statements):\n",
    "    return [get_wd_label(t[1]['predicate']['id']) for t in statements]\n",
    "#print(get_statements_predicates_labels(best_similar_statements))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_statements_objects_labels(statements):\n",
    "    return [get_wd_label(t[1]['object']['id']) for t in statements]\n",
    "#print(get_statements_objects_labels(best_similar_statements))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_statements_qualifier_predicates_labels(statements):\n",
    "    return [get_wd_label(t[1]['qualifiers'][0]['qualifier_predicate']['id']) for t in statements]\n",
    "#print(get_statements_qualifier_predicates_labels(best_similar_statements))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_statements_qualifier_objects_labels(statements):\n",
    "    return [get_wd_label(t[1]['qualifiers'][0]['qualifier_object']['id']) for t in statements]\n",
    "#print(get_statements_qualifier_objects_labels(best_similar_statements))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_extend_by_words(cluster_root_ids, extending_words, top_k=3):\n",
    "    cluster = []\n",
    "    #start_time = time.time()\n",
    "    \n",
    "    for name in extending_words:\n",
    "        #start_cluster_time = time.time()\n",
    "        cluster += get_best_similar_statements_by_word(cluster_root_ids, name, top_k=top_k, qualifier=True, statement_type=\"qualifier_predicate\")\n",
    "        cluster += get_best_similar_statements_by_word(cluster_root_ids, name, top_k=top_k, qualifier=True, statement_type=\"qualifier_object\")\n",
    "        cluster += get_best_similar_statements_by_word(cluster_root_ids, name, top_k=top_k, qualifier=False, statement_type=\"predicate\")\n",
    "        cluster += get_best_similar_statements_by_word(cluster_root_ids, name, top_k=top_k, qualifier=False, statement_type=\"object\")\n",
    "        #end_time = time.time()\n",
    "        #print(\"EXTENDING Cluster with:\", name,\" ->\\tRunning time is {}s\".format(round(end_time-start_cluster_time,2)))\n",
    "    #end_time = time.time()\n",
    "    #print(\"EXTENDING Clusters ->\\tRunning time is {}s\".format(round(end_time-start_time,2)))\n",
    "    return cluster\n",
    "    \n",
    "#test_cluster = cluster_extend_by_words(extract_ids(q0_themes[0]), ['voiced'], top_k=2)\n",
    "#test_cluster_test_2 = cluster_extend_by_words(extract_ids(q0_themes_test_2[0]), ['birth'], top_k=2)\n",
    "#print(test_cluster[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sorts by the similarity value of statements[0]\n",
    "def sort_statements_by_similarity(statements):\n",
    "    return [s for s in sorted(statements, key=lambda x: x[0], reverse=True)]\n",
    "\n",
    "#test_sorted_statements = sort_statements_by_similarity(test_cluster)\n",
    "#test_sorted_statements_test_2 = sort_statements_by_similarity(test_cluster_test_2)\n",
    "#print(test_sorted_statements[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# appends spo from qualifiers, removes qualifier tags, and removes similarity scores\n",
    "def statements_flatter(statements):\n",
    "    best_statements_to_graph = []\n",
    "    for statement in statements:\n",
    "        tmp_statement = copy(statement)\n",
    "        if tmp_statement.get('qualifiers'):\n",
    "            #print(\"statement\", statement)\n",
    "            for q in tmp_statement['qualifiers']:\n",
    "                qualifier_statement = {'entity': {'id': tmp_statement['entity']['id']}}\n",
    "                qualifier_statement['predicate'] = {'id': q['qualifier_predicate']['id']}\n",
    "                qualifier_statement['object'] = {'id': q['qualifier_object']['id']}\n",
    "                best_statements_to_graph.append(qualifier_statement)\n",
    "            del(tmp_statement['qualifiers'])\n",
    "        else: \n",
    "            #print(\"tmp_statement\", tmp_statement)\n",
    "            if ('qualifiers' in tmp_statement): del(tmp_statement['qualifiers'])\n",
    "        if tmp_statement not in best_statements_to_graph:\n",
    "            #print(\"best_statements_to_graph\", tmp_statement)\n",
    "            best_statements_to_graph.append(tmp_statement)\n",
    "    return best_statements_to_graph\n",
    "\n",
    "#test_flatten_statements = statements_flatter([s[1] for s in test_sorted_statements])\n",
    "#test_flatten_statements_test_2 = statements_flatter([s[1] for s in test_sorted_statements_test_2])\n",
    "#print(test_flatten_statements[0])\n",
    "#test_flatten_statements_test_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove duplicates from statements\n",
    "def unduplicate_statements(statements):\n",
    "    filtered_statements = []\n",
    "    [filtered_statements.append(s) for s in statements if s not in [e for e in filtered_statements]]\n",
    "    return filtered_statements\n",
    "\n",
    "#test_unduplicate_statements = unduplicate_statements(test_flatten_statements)\n",
    "#print(len(test_flatten_statements))\n",
    "#print(len(test_unduplicate_statements))\n",
    "#print(test_unduplicate_statements[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#test_graph = make_statements_graph(test_unduplicate_statements)\n",
    "#print(test_graph[1])\n",
    "#plot_graph(test_graph[0], \"file_name_graph\", \"Graph_title\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_statements_by_id(statements, from_token_id, to_id, qualifier=False, statement_type=\"predicate\"):\n",
    "    id_statements = []\n",
    "    if not statements:\n",
    "        return id_statements\n",
    "    if get_wd_label(from_token_id):\n",
    "        for statement in statements:\n",
    "            if qualifier:\n",
    "                if statement.get('qualifiers'):\n",
    "                    for s in statement['qualifiers']:\n",
    "                        if to_id == s[statement_type]['id']:\n",
    "                            id_statements.append(statement)\n",
    "            else:\n",
    "                if to_id == statement[statement_type]['id']:\n",
    "                    id_statements.append(statement)\n",
    "    \n",
    "    return id_statements\n",
    "\n",
    "#statements_test = get_all_statements_of_entity('Q176198')\n",
    "#id_statements_test = get_statements_by_id(statements_test, 'Q176198', 'P725')\n",
    "#print(id_statements_test[0])\n",
    "\n",
    "#get_statements_by_id(root_statements, cluster_root_id, predicate_id, qualifier=False, statement_type=\"predicate\")\n",
    "#statements_test = get_all_statements_of_entity('Q176198')\n",
    "#id_statements_test = get_statements_by_id(statements_test, 'Q176198', 'P725')\n",
    "#id_statements_test[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters\n",
    "# cluster_root_ids: ['Qcode']\n",
    "# predicates_ids: ['Pcode']\n",
    "def cluster_extend_by_predicates_ids(cluster_root_ids, predicates_ids):\n",
    "    cluster = []\n",
    "    \n",
    "    for cluster_root_id in cluster_root_ids:\n",
    "        root_statements = get_all_statements_of_entity(cluster_root_id)\n",
    "        #print(\"root_statements\", root_statements)\n",
    "        for predicate_id in predicates_ids:\n",
    "            cluster += get_statements_by_id(root_statements, cluster_root_id, predicate_id, qualifier=True, statement_type=\"qualifier_predicate\")\n",
    "            cluster += get_statements_by_id(root_statements, cluster_root_id, predicate_id, qualifier=False, statement_type=\"predicate\")\n",
    "\n",
    "    return cluster\n",
    "    \n",
    "#test_predicate_clusters = cluster_extend_by_predicates_ids(extract_ids(q0_themes[0]), extract_ids(q0_predicates))\n",
    "#print(len(test_predicate_clusters))\n",
    "#test_predicate_clusters[0]\n",
    "\n",
    "#test_predicate_clusters_test_2 = cluster_extend_by_predicates_ids(extract_ids(q0_themes_test_2[0]), extract_ids(q0_predicates_test_2))\n",
    "#print(len(test_predicate_clusters_test_2))\n",
    "#print(test_predicate_clusters_test_2[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_extractor_from_complements(complements):\n",
    "    for c in complements:\n",
    "        [print(t.pos_) for t in c]\n",
    "    return complements\n",
    "\n",
    "#print(cluster_extractor_from_complements(q0_themes[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#TODO: add cache\n",
    "#TODO: Check if extending with predicate_ids is useful\n",
    "# parameter\n",
    "# question: nlp_string\n",
    "#limits=plt.axis('off')\n",
    "def build_graph(nlp, themes, themes_enhanced, predicates, deep_k=10):\n",
    "    #start_time = time.time()\n",
    "    theme_ids = extract_ids(themes[0])\n",
    "    theme_enhanced_ids = extract_ids(themes_enhanced)\n",
    "    predicates_ids = extract_ids(predicates)\n",
    "    predicates_enhanced_ids = [p for p in theme_enhanced_ids if is_wd_predicate(p)]\n",
    "    predicates_enhanced = merge_lists([get_nlp(get_wd_label(p)) for p in predicates_enhanced_ids], predicates_enhanced_ids)\n",
    "    \n",
    "    #print(theme_ids)\n",
    "    #print(theme_enhanced_ids)\n",
    "    for i, tei in enumerate(theme_enhanced_ids):\n",
    "        if tei in theme_ids:\n",
    "            tmp = theme_enhanced_ids.pop(i)\n",
    "    \n",
    "    init_clusters = cluster_extend_by_words(theme_ids, [p[0].text for p in predicates+predicates_enhanced], top_k=deep_k)\n",
    "    #print(\"init_clusters\",len(init_clusters))\n",
    "    init_clusters_enhanced = cluster_extend_by_words(theme_enhanced_ids, [p[0].text for p in predicates+predicates_enhanced], top_k=deep_k)\n",
    "    #print(\"init_clusters_enhanced\",len(init_clusters_enhanced))\n",
    "    init_sorted_statements = sort_statements_by_similarity(init_clusters + init_clusters_enhanced)\n",
    "    #print(\"init_sorted_statements\",len(init_sorted_statements))\n",
    "    init_flatten_statements = statements_flatter([s[1] for s in init_sorted_statements])\n",
    "    #print(\"init_flatten_statements\",len(init_flatten_statements))\n",
    "    \n",
    "    predicate_ids_clusters = cluster_extend_by_predicates_ids(theme_ids, predicates_ids+predicates_enhanced_ids)\n",
    "    #print(\"predicate_ids_clusters\",len(predicate_ids_clusters))\n",
    "    predicate_ids_enhanced_clusters = cluster_extend_by_predicates_ids(theme_enhanced_ids, predicates_ids+predicates_enhanced_ids)\n",
    "    #print(\"predicate_ids_enhanced_clusters\",len(predicate_ids_enhanced_clusters))\n",
    "    predicate_ids_flatten_statements = statements_flatter(predicate_ids_clusters+predicate_ids_enhanced_clusters)\n",
    "    #print(\"predicate_ids_flatten_statements\",len(predicate_ids_flatten_statements))\n",
    "    \n",
    "    clusters = init_flatten_statements+predicate_ids_flatten_statements\n",
    "    filtered_statements = unduplicate_statements(clusters)\n",
    "    #print(predicate_ids_enhanced_clusters)\n",
    "    graph = make_statements_graph(filtered_statements)\n",
    "\n",
    "    ##print(\"clusters:\", len(clusters))\n",
    "    ##print(\"filtered_statements:\", len(filtered_statements))\n",
    "    #end_time = time.time()\n",
    "    #print(\"->\\tRunning time is {}s\".format(round(end_time-start_time,2)))\n",
    "    \n",
    "    return graph\n",
    "\n",
    "#q0_test = questions[0]\n",
    "#q0_test = \"Which actor voiced the Unicorn in The Last Unicorn?\"\n",
    "#q0_test = \"what was the cause of death of yves klein\"\n",
    "#q0_test = \"Who is the wife of Barack Obama?\"\n",
    "#q0_test = \"Who is the author of Le Petit Prince?\"\n",
    "#q0_nlp_test = get_nlp(q0_test)\n",
    "#q0_themes_test = get_themes(q0_nlp_test, top_k=3)\n",
    "#q0_themes_enhanced_test = get_enhanced_themes(q0_themes_test, top_k=3)\n",
    "#q0_predicates_test = get_predicates_online(q0_nlp_test, top_k=3)\n",
    "#q0_focused_parts_test = []\n",
    "#graph, predicates_dict = build_graph(q0_nlp_test, q0_themes_test, q0_themes_enhanced_test, q0_predicates_test, deep_k=3)\n",
    "#print(predicates_dict)\n",
    "#plot_graph(graph, \"file_name_graph\", \"Graph_title\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the graph for complements\n",
    "# parameters\n",
    "# name: string\n",
    "def find_name_in_graph(graph, name):\n",
    "    return [x for x,y in graph.nodes(data=True) if y['name'].lower() == name.lower()]\n",
    "\n",
    "#[find_name_in_graph(c.text) for c in q0_themes[1]]\n",
    "#print(find_name_in_graph(graph, \"the unicorn\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: clean the complements by removing stopwords etc.\n",
    "def find_theme_complement(graph, themes):\n",
    "    return [i for i in itertools.chain.from_iterable(\n",
    "        [id for id in [c for c in [find_name_in_graph(graph, t.text) for t in themes[1]] if c]])]\n",
    "\n",
    "#print(find_theme_complement(graph, q0_themes_test))\n",
    "#[i for i in itertools.chain.from_iterable([id for id in check_theme_complement(graph, q0_themes)])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_paths_in_graph(graph, node_start, node_end):\n",
    "    return [p for p in nx.all_simple_paths(graph, source=node_start, target=node_end)]\n",
    "        \n",
    "#test_paths = find_paths_in_graph(graph, \"Q16205566\", \"Q7774795\")\n",
    "#print(test_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_id_in_graph(graph, node_id):\n",
    "    return graph.has_node(node_id)\n",
    "#print(is_id_in_graph(graph, \"Q24039104\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_name_in_graph(graph, node_name):\n",
    "    return find_name_in_graph(graph, node_name) != []\n",
    "#print(is_name_in_graph(graph, \"the Unicorn\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_paths_for_themes(graph, themes):\n",
    "    themes_ids = [t for t in  extract_ids(themes[0])]\n",
    "    complements_ids = find_theme_complement(graph, themes)\n",
    "    paths = []\n",
    "    for t_id in themes_ids:\n",
    "        if is_id_in_graph(graph, t_id):\n",
    "            for c_id in complements_ids:\n",
    "                if is_id_in_graph(graph, c_id):\n",
    "                    path = find_paths_in_graph(graph, t_id, c_id)\n",
    "                    if path:\n",
    "                        paths.append(path)\n",
    "    paths = [i for i in itertools.chain.from_iterable(\n",
    "        [id for id in paths])]\n",
    "    \n",
    "    return paths\n",
    "#print(find_paths_for_themes(graph, q0_themes_test))\n",
    "#print(find_paths_for_themes(graph, q0_themes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_node_predicates_from_path(paths):\n",
    "    predicates = []\n",
    "    for p in paths:\n",
    "        [predicates.append(i[:i.find(\"-\")]) for i in p if is_wd_predicate(i[:i.find(\"-\")]) and i[:i.find(\"-\")] not in predicates]\n",
    "    return predicates\n",
    "\n",
    "#test_node_predicates = get_node_predicates_from_path(test_paths)\n",
    "#print(test_node_predicates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_node_predicate_similarity_from_path(paths, predicates):\n",
    "    path_predicates = get_node_predicates_from_path(paths)\n",
    "    return sorted([(pp, get_similarity_by_ids(p2, pp)) for p in predicates for p2 in p[1] for pp in path_predicates], key=lambda x: x[-1], reverse=True)\n",
    "\n",
    "#test_node_pedicate_similarities = get_node_predicate_similarity_from_path(test_paths, q0_predicates)\n",
    "#print(test_node_pedicate_similarities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_focused_parts(nlp_sentence, top_k=3):\n",
    "    focused_parts = [t.head for t in nlp_sentence if t.tag_ == \"WDT\" or t.tag_ == \"WP\" or t.tag_ == \"WP$\" or t.tag_ == \"WRB\"] \n",
    "    focused_parts_ids = [get_wd_ids(p.text, top_k=top_k) for p in focused_parts]\n",
    "    return merge_lists(focused_parts, focused_parts_ids)\n",
    "\n",
    "#print(get_focused_parts(q0_nlp_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#questions_2 = (\"what was the cause of death of yves klein\",\n",
    "#               \"Who is the wife of Barack Obama?\",\n",
    "#               \"Who is the president of the United States?\",\n",
    "#               \"When was produced the first Matrix movie?\",\n",
    "#               \"Who made the soundtrack of the The Last Unicorn movie?\",\n",
    "#               \"Who is the author of Le Petit Prince?\",\n",
    "#               \"Which actor voiced the Unicorn in The Last Unicorn?\",\n",
    "#               \"how is called the rabbit in Alice in Wonderland?\",\n",
    "#               \"what city was alex golfis born in\",\n",
    "#               \"which stadium do the wests tigers play in\",\n",
    "#               \"Which nation is Martha Mattox from\"\n",
    "#              )\n",
    "#\n",
    "#question_2 = questions_2[6] #\"what city was alex golfis born in\"#\n",
    "#q_nlp_2 = get_nlp(question_2)\n",
    "#q_themes_2 = get_themes(q_nlp_2, top_k=3)\n",
    "#q_themes_enhanced_2 = get_enhanced_themes(q_themes_2, top_k=3)\n",
    "#q_predicates_2 = get_predicates(q_nlp_2, top_k=3)\n",
    "#if q_predicates_2:\n",
    "#        if not q_predicates_2[0][1]: q_predicates_2 = get_predicates_online(q_nlp_2, top_k=3)\n",
    "#q_focused_parts_2 = get_focused_parts(q_nlp_2)\n",
    "#print(\"q_nlp:\", q_nlp_2)\n",
    "#print(\"e\\t\\te.pos_\\te.tag_\\te.dep_\\te.head\\te.children\")\n",
    "#for e in q_nlp_2:\n",
    "#    print(e.text,\"\\t\\t\", e.pos_,\"\\t\", e.tag_,\"\\t\", e.dep_,\"\\t\", e.head, \"\\t\", [child for child in e.children])\n",
    "#\n",
    "#print(\"\\nq_themes:\", q_themes_2)\n",
    "#print(\"q_themes_enhanced:\",q_themes_enhanced_2)\n",
    "#print(\"q_predicates:\", q_predicates_2)\n",
    "#print(\"q_focused_parts:\", q_focused_parts_2)\n",
    "#\n",
    "#graph_2, predicates_dict_2 = build_graph(q_nlp_2, q_themes_2, q_themes_enhanced_2, q_predicates_2, deep_k=40)\n",
    "#print(len(graph_2), \"nodes and\", graph_2.size(), \"edges\")\n",
    "#print(predicates_dict_2)\n",
    "##plot_graph(graph_2, \"main_graph\", \"Main_graph_title\")\n",
    "##answers_2 = find_anwser_from_graph_2(graph, q0_nlp, q0_themes, q_themes_enhanced_2, q_predicates_2, q_focused_parts_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot_graph(graph_2, \"test_file_name_graph\", \"Graph_title\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_compound(nlp_list, themes):\n",
    "    compounded = []\n",
    "    #if not nlp_list[0]:\n",
    "    #    return compounded\n",
    "    try:\n",
    "        for t in [e[0] for e in themes[0]] + themes[1]:\n",
    "            for l in [n[0] for n in nlp_list]:\n",
    "                if l.text.lower() in t.text.lower():\n",
    "                    compounded.append(t.text)\n",
    "        return compounded\n",
    "    except:\n",
    "        return compounded\n",
    "\n",
    "# TODO: make the predicate search go further in the path list for the !i%2\n",
    "def find_paths_keywords(graph, nlp, themes, themes_enhanced, predicates, focused_parts):\n",
    "    WH_FILTER = [\"WDT\", \"WP\", \"WP$\", \"WRB\"]\n",
    "    VERB_FILTER = [\"VERB\", \"AUX\"]\n",
    "    NOUN_FILTER = [\"NOUN\",\"PROPN\"]\n",
    "    POSITION_FILTER = [\"ADP\"]\n",
    "    \n",
    "    focused_parts_words = [t[0].text for t in focused_parts]\n",
    "    focused_parts_ids = [j for i in [t[1] for t in focused_parts] for j in i]\n",
    "    focused_parts_predicates_ids = [f for f in focused_parts_ids if is_wd_predicate(f)]\n",
    "    focused_parts_words_ids = [f for f in focused_parts_ids if is_wd_entity(f)]\n",
    "    focused_parts_words_ids_labeled = [get_wd_label(p) for p in focused_parts_words_ids]\n",
    "    #print(focused_parts_words_2)\n",
    "\n",
    "    question_anchors = [t for t in nlp if t.tag_ in WH_FILTER]\n",
    "    themes_enhanced_list = [t[0] for t in themes_enhanced]\n",
    "    focus_themes = [t[0].text for t in themes[0]]\n",
    "    focus_path_by_tails = [[c for c in t.head.children if c.pos_ in NOUN_FILTER] for t in nlp if t.pos_ == \"PRON\"]\n",
    "    focus_part_by_head = [t.head for t in question_anchors]\n",
    "    predicates_nlp = [t for t in nlp if t.pos_ in VERB_FILTER]\n",
    "    predicates_lemma = [t.lemma_ for t in predicates_nlp]\n",
    "    predicates_attention = [t for t in nlp if t.head in predicates_nlp]\n",
    "    predicates_attention_tails = [[c for c in t.children] for t in predicates_attention]\n",
    "    in_attention_heads = [t.head.text for t in nlp if t.pos_ in POSITION_FILTER]\n",
    "    in_attention_tails = add_compound([[c for c in t.children] for t in nlp if t.pos_ in POSITION_FILTER], themes)\n",
    "    focus_themes_enhanced = [t[0] for t in themes_enhanced\n",
    "                             if t[0].lower() in [a.lower() for a in in_attention_tails]\n",
    "                             or t[0].lower() in [a.lower() for a in in_attention_heads]]\n",
    "    \n",
    "    theme_enhanced_ids = extract_ids(themes_enhanced)\n",
    "    predicates_enhanced_ids = [(p) for p in theme_enhanced_ids if is_wd_predicate(p)]\n",
    "    [predicates_enhanced_ids.append(p) for p in focused_parts_predicates_ids if p not in predicates_enhanced_ids]\n",
    "    \n",
    "    alterniative_words = {}\n",
    "    for t in themes_enhanced:\n",
    "        for e in predicates_enhanced_ids:\n",
    "            if e in t[1]:\n",
    "                alterniative_words[t[0]] = [get_nlp(get_wd_label(e)),[e]]\n",
    "            else:\n",
    "                alterniative_words[get_wd_label(e)] = [get_nlp(get_wd_label(e)),[e]]\n",
    "    \n",
    "    #print(\"focused_parts_predicates_ids\",focused_parts_predicates_ids)\n",
    "    #print(\"focused_parts_words_ids\",focused_parts_words_ids)\n",
    "    #print(\"alterniative_words\",alterniative_words)\n",
    "    #print(\"predicates_enhanced_ids\",predicates_enhanced_ids)\n",
    "    ##print(\"predicates_enhanced\",predicates_enhanced)\n",
    "    #print(\"question_anchors\",question_anchors)\n",
    "    #print(\"in_attention_heads\",in_attention_heads)\n",
    "    #print(\"in_attention_tails\",in_attention_tails)\n",
    "    #print(\"focus_themes\",focus_themes)\n",
    "    #print(\"themes_enhanced_list\",themes_enhanced_list)\n",
    "    #print(\"focus_themes_enhanced\",focus_themes_enhanced)\n",
    "    #print(\"focus_path_by_tails\",focus_path_by_tails)\n",
    "    #print(\"focus_part_by_head\",focus_part_by_head)\n",
    "    #print(\"predicates_nlp\",predicates_nlp)\n",
    "    #print(\"predicates_lemma\",predicates_lemma)\n",
    "    #print(\"predicates_attention\",predicates_attention)\n",
    "    #print(\"predicates_attention_tails\",predicates_attention_tails)\n",
    "    #\n",
    "    #print(\"\\n\")\n",
    "    paths_keywords = []\n",
    "    [paths_keywords.append(e.lower()) for e in focused_parts_words + in_attention_heads + in_attention_tails + focus_themes + focus_themes_enhanced + focused_parts_words_ids_labeled if e.lower() not in paths_keywords]\n",
    "    #print(paths_keywords)\n",
    "    #paths_keywords = [p for p in itertools.permutations(paths_keywords)]\n",
    "    #print(paths_keywords)\n",
    "    return paths_keywords, alterniative_words, question_anchors\n",
    "    \n",
    "    #initial_paths = find_paths_for_themes(graph, themes)\n",
    "    #predicate_id_similarities = get_node_predicate_similarity_from_path(initial_paths, predicates)\n",
    "    #best_path = [p for p in initial_paths if predicate_id_similarities[0][0] == p[1][:p[1].find(\"-\")]]\n",
    "    #path_answer = get_wd_label(best_path[0][2]) if best_path else []\n",
    "    \n",
    "    #return (path_answer, best_path[0][2]) if path_answer else (False, False)\n",
    "\n",
    "#paths_keywords_2 = find_paths_keywords(graph_2, q_nlp_2, q_themes_2, q_themes_enhanced_2, q_predicates_2, q_focused_parts_2)\n",
    "#paths_keywords_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_paths_keywords_nodes(graph, keywords,threshold=0.9,top_performance=50):\n",
    "    keywords_nodes = []\n",
    "    for k in keywords:\n",
    "        nlp_lookup = get_nlp(k)\n",
    "        keywords_nodes.append([x for x,y in graph.nodes(data=True)\n",
    "               if get_nlp(y['name']).similarity(nlp_lookup) >= threshold])\n",
    "    \n",
    "    if keywords_nodes:\n",
    "        if len(keywords_nodes[1]) * len(keywords_nodes[0]) > top_performance:\n",
    "            if len(keywords_nodes[0]) <= int(sqrt(top_performance)):\n",
    "                keywords_nodes[1] = keywords_nodes[1][:int(top_performance/len(keywords_nodes[0]))]\n",
    "            elif len(keywords_nodes[0]) >= len(keywords_nodes[1]):\n",
    "                keywords_nodes[0] = keywords_nodes[0][:int(top_performance/len(keywords_nodes[1]))]\n",
    "            else:\n",
    "                keywords_nodes[0] = keywords_nodes[0][:int(sqrt(top_performance))]\n",
    "                keywords_nodes[1] = keywords_nodes[1][:int(sqrt(top_performance))]\n",
    "    \n",
    "    keywords_nodes_per = [p for p in itertools.permutations(keywords_nodes, 2)]\n",
    "    \n",
    "    paths_keyword_nodes = []\n",
    "    for pkn in keywords_nodes_per:\n",
    "        for pkn1 in pkn[0]:\n",
    "            for pkn2 in pkn[1]:\n",
    "                [paths_keyword_nodes.append(p) for p in nx.all_simple_paths(graph, source=pkn1, target=pkn2) if p not in paths_keyword_nodes]\n",
    "    \n",
    "    return paths_keyword_nodes\n",
    "\n",
    "def find_path_nodes_from_graph_2(graph, keywords, threshold=0.9, thres_inter=0.15, top_k=3, top_performance=50,min_paths=3000):\n",
    "    #print(\"current threshold\", str(round(threshold, 1)))\n",
    "    main_keyword_paths = get_paths_keywords_nodes(graph, keywords[0],threshold=threshold,top_performance=top_performance)\n",
    "    alternative_keyword_paths = []\n",
    "    \n",
    "    for k_1 in keywords[1]:\n",
    "        for i, k_0 in enumerate(keywords[0]):\n",
    "            if k_1==k_0:\n",
    "                tmp_keywords = keywords[0].copy()\n",
    "                tmp_keywords[i] = keywords[1][k_1][0].text\n",
    "                alternative_keyword_paths += get_paths_keywords_nodes(graph, tmp_keywords, threshold=threshold,top_performance=top_performance)\n",
    "    \n",
    "    keyword_paths = main_keyword_paths+alternative_keyword_paths\n",
    "    \n",
    "    #print(\"len(keyword_paths)\",len(keyword_paths))\n",
    "    if len(keyword_paths) < min_paths:\n",
    "        if threshold == 0: return keyword_paths\n",
    "        threshold -= thres_inter\n",
    "        if threshold < 0: threshold = 0\n",
    "        keyword_paths = find_path_nodes_from_graph_2(graph, keywords, threshold, thres_inter, top_k,top_performance,min_paths)\n",
    "    \n",
    "    return keyword_paths\n",
    "\n",
    "#start_time = time.time()\n",
    "#path_nodes_2 = find_path_nodes_from_graph_2(graph_2, paths_keywords_2, threshold=0.9, thres_inter=0.15, top_performance=50, min_paths=3000)\n",
    "#end_time = time.time()\n",
    "#print(\"Finding path nodes ->\\tRunning time is {}s\".format(round(end_time-start_time,2))) \n",
    "#print(path_nodes_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "#node_predicates_names_2 = get_node_predicates_from_path(path_nodes_2)\n",
    "\n",
    "def is_sublist(a, b):\n",
    "    if not a: return True\n",
    "    if not b: return False\n",
    "    #if a == b: return False\n",
    "    return b[:len(a)] == a or is_sublist(a, b[1:])\n",
    "\n",
    "def paths_nodes_filter(path_nodes, graph):\n",
    "    filtered_paths = []\n",
    "    \n",
    "    for path in path_nodes:\n",
    "        filtered_row = []\n",
    "        for i,p in enumerate(path):\n",
    "            if is_wd_predicate(p[:p.find(\"-\")]):\n",
    "                if i == 0:\n",
    "                    #if p[:p.find(\"-\")] == \"P725\":\n",
    "                    #    print(p)\n",
    "                    neighbor = [k for k in graph[p].keys() if k != path[i+1]]\n",
    "                    if neighbor:\n",
    "                        filtered_row.append(neighbor[0])\n",
    "                        filtered_row.append(p[:p.find(\"-\")])\n",
    "                    else:\n",
    "                        continue\n",
    "                    #print(filtered_row)\n",
    "                elif i > 0 and i < len(path)-1:\n",
    "                    filtered_row.append(p[:p.find(\"-\")])\n",
    "                else:\n",
    "                    neighbor = [k for k in graph[p].keys() if k != path[i-1]]\n",
    "                    if neighbor:\n",
    "                        filtered_row.append(p[:p.find(\"-\")])\n",
    "                        filtered_row.append(neighbor[0])\n",
    "                    else:\n",
    "                        continue\n",
    "            else: filtered_row.append(p)\n",
    "        \n",
    "        #print(\"filtered_paths\",filtered_paths)\n",
    "        \n",
    "        if len(filtered_row) > 1 and filtered_row not in filtered_paths: \n",
    "            filtered_paths.append(filtered_row)\n",
    "    \n",
    "    unique_paths = filtered_paths.copy()\n",
    "    for i,fp in enumerate(filtered_paths):\n",
    "        for fp_2 in filtered_paths:\n",
    "            if (is_sublist(fp, fp_2) and fp!=fp_2):\n",
    "                unique_paths[i] = []\n",
    "                break\n",
    "    \n",
    "    unique_paths = [p for p in unique_paths if p]\n",
    "    \n",
    "    #print(\"unique_paths\",len(unique_paths))\n",
    "    \n",
    "    #for i, up in enumerate(unique_paths):\n",
    "    #    for up_2 in unique_paths:\n",
    "    #        if (list(reversed(up)) == up_2):\n",
    "    #            unique_paths[i] = []\n",
    "    #            break\n",
    "    \n",
    "    \n",
    "    #cleaned_paths = []\n",
    "    #unique_paths = [up for up in unique_paths if up]\n",
    "    \n",
    "    #for up in unique_paths:\n",
    "    #    for i,e in enumerate(up):\n",
    "    #        if not is_wd_predicate(e):\n",
    "    #            for j,r in enumerate(list(reversed(up))): \n",
    "    #                if not is_wd_predicate(r):\n",
    "    #                    cleaned_paths.append(up[i:-j])\n",
    "    #            break\n",
    "                \n",
    "    #print(\"cleaned_paths\",len(cleaned_paths))\n",
    "                \n",
    "    #cleaned_paths = [c for c in cleaned_paths if len(c) > 2]\n",
    "    \n",
    "    #unique_paths = cleaned_paths.copy()\n",
    "    #for i,fp in enumerate(cleaned_paths):\n",
    "    #    for fp_2 in cleaned_paths:\n",
    "    #        if (is_sublist(fp, fp_2) and fp!=fp_2):\n",
    "    #            unique_paths[i] = []\n",
    "    #            break\n",
    "    \n",
    "    #unique_paths = [p for p in unique_paths if len(p) > 2]       \n",
    "    \n",
    "    #for i, up in enumerate(unique_paths):\n",
    "    #    for up_2 in unique_paths:\n",
    "    #        if (list(reversed(up)) == up_2):\n",
    "    #            unique_paths[i] = []\n",
    "    #            break\n",
    "        \n",
    "        #print(up)\n",
    "    #[up for up in unique_paths if up and not is_wd_predicate(up[-1]) and not is_wd_predicate(up[0])]\n",
    "    #print()\n",
    "    #for up in unique_paths:\n",
    "    #    print(up)\n",
    "    #    break\n",
    "    #    return []\n",
    "    \n",
    "    return [p for p in unique_paths if len(p) > 2] #False#[up for up in unique_paths if up and not is_wd_predicate(up[-1]) and not is_wd_predicate(up[0])]#False# [p for p in unique_paths if p]\n",
    "                \n",
    "#paths_nodes_filtered_2 = paths_nodes_filter(path_nodes_2, graph_2)\n",
    "#print(\"unique_paths\", len(paths_nodes_filtered_2))\n",
    "#for p in paths_nodes_filtered_2:\n",
    "#    print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hypothesises(nlp, paths_keywords, filtered_paths):#, themes, themes_enhanced):\n",
    "    #print(paths_keywords)\n",
    "    #print([p for p in paths_keywords[1].values])\n",
    "    complementary_predicates = [p[0] for p in list(paths_keywords[1].values())]\n",
    "    \n",
    "    #print(complementary_predicates)\n",
    "    #locate positions   \n",
    "    anchors_positions = []\n",
    "    anchors_focuses = []\n",
    "    #keywords_positions = []\n",
    "    #predicates_positions = []\n",
    "    \n",
    "    [anchors_positions.append(i) for i, w in enumerate(nlp) if w in paths_keywords[2]]\n",
    "    #print(\"anchors_positions 1\",anchors_positions)\n",
    "    \n",
    "    #anchors_childrens\n",
    "    for p in anchors_positions:\n",
    "        children = [c for c in nlp[p].children]\n",
    "        if children == []: \n",
    "            children = [c for c in nlp[p].head.children]\n",
    "        else: children += nlp[p].head\n",
    "        anchors_focuses += ([c for c in children\n",
    "               if c not in [nlp[a] for a in anchors_positions]\n",
    "               and c.pos_ != \"PUNCT\"])\n",
    "        if not anchors_focuses:\n",
    "            anchors_focuses = [nlp[p].head]\n",
    "        \n",
    "        #print(anchors_focuses)\n",
    "        anchors_focuses += complementary_predicates\n",
    "    \n",
    "    #[anchors_focuses_filtered.append(af) for af in anchors_focuses if af not in anchors_focuses_filtered]\n",
    "    #print(\"anchors_focuses\",anchors_focuses_filtered)\n",
    "    \n",
    "    \n",
    "    #find anchor position in paths\n",
    "    anchors_predicates = []\n",
    "    #print(\"filtered_paths\",filtered_paths)\n",
    "    for af in anchors_focuses:\n",
    "        for p in filtered_paths:\n",
    "            #print(af, p)\n",
    "            for e in p:\n",
    "                #print(af,get_wd_label(e))\n",
    "                if is_wd_predicate(e) and e not in [ap[0] for ap in anchors_predicates]:\n",
    "                    #print(af,get_wd_label(e))\n",
    "                    anchors_predicates.append([e, get_similarity_by_words(get_nlp(get_wd_label(e)),af)])\n",
    "                \n",
    "    #print(\"anchors_predicates\",anchors_predicates)\n",
    "    #for p in filtered_paths:\n",
    "    #    for af in anchors_focuses:\n",
    "    #        for e in p:\n",
    "    #            #print(af,get_wd_label(e))\n",
    "    #            if is_wd_predicate(e) and e not in [ap[0] for ap in anchors_predicates]:\n",
    "    #                #print(af,get_wd_label(e))\n",
    "    #                anchors_predicates.append([e, get_similarity_by_words(get_nlp(get_wd_label(e)),af)])\n",
    "                \n",
    "    anchors_predicates = [a for a in sorted(anchors_predicates, key=lambda x: x[-1], reverse=True) if a[1] > 0.5]\n",
    "    #print(\"anchors_predicates\",anchors_predicates)\n",
    "    \n",
    "    anchors_predicates_filtered = []\n",
    "    for ap in anchors_predicates:\n",
    "        for af in anchors_focuses:\n",
    "            anchors_predicates_filtered.append([ap[0],get_similarity_by_words(get_nlp(get_wd_label(ap[0])),af)])\n",
    "    \n",
    "    if not anchors_predicates_filtered:\n",
    "        anchors_predicates_filtered = anchors_predicates\n",
    "    \n",
    "    anchors_predicates_filtered = [a for a in sorted(anchors_predicates_filtered, key=lambda x: x[-1], reverse=True) if a[1] > 0.5]\n",
    "    #print(\"anchors_predicates_filtered\",anchors_predicates_filtered)\n",
    "    \n",
    "    anchors_predicates=[]\n",
    "    [anchors_predicates.append(apf) for apf in anchors_predicates_filtered if apf not in anchors_predicates]\n",
    "    #print(\"anchors_predicates\",anchors_predicates)\n",
    "    \n",
    "    hypothesises_tuples = []\n",
    "    for ap in anchors_predicates:\n",
    "        for fp in filtered_paths:\n",
    "            for i, e in enumerate(fp):\n",
    "                #print(e)\n",
    "                if e == ap[0] and i>1 and i<len(fp)-1:\n",
    "                    #print(i, [fp[i-1], fp[i], fp[i+1]])\n",
    "                    hypothesis_tuple = [fp[i-1], fp[i], fp[i+1]]\n",
    "                    if hypothesis_tuple not in hypothesises_tuples:\n",
    "                        hypothesises_tuples.append(hypothesis_tuple)\n",
    "                        \n",
    "    #print(\"hypothesises_tuples\",hypothesises_tuples)\n",
    "    #print(\"hypothesises_tuples\",hypothesises_tuples)\n",
    "    #print([a[0] for a in anchors_predicates])\n",
    "    keywords_names = [k for k in paths_keywords[0]]\n",
    "    #print(\"keywords_names\",keywords_names)\n",
    "    #keywords_ids = [i for j in [get_wd_ids(k) for k in keywords_names if get_wd_ids(k)] for i in j]\n",
    "    #print(\"keywords_names\",keywords_ids)\n",
    "    #print(extract_ids(themes[0]))\n",
    "    #print(extract_ids(themes_enhanced))\n",
    "    #keywords_ids = []\n",
    "    #[keywords_ids.append(i) for i in extract_ids(themes[0]) + extract_ids(themes_enhanced) if i not in keywords_ids]\n",
    "    #print(\"keywords_ids\",keywords_ids)\n",
    "    \n",
    "    #print(\"anchors_predicates\",anchors_predicates)\n",
    "    \n",
    "    hypothesises = []\n",
    "    for ht in hypothesises_tuples:\n",
    "        if ht[1] in [a[0] for a in anchors_predicates]:\n",
    "            #rint(ht)\n",
    "        #if ht[1] == anchors_predicates[0][0]:\n",
    "            for k in paths_keywords[0]:\n",
    "                hypo_sum = 0\n",
    "                nlp_k = get_nlp(k)\n",
    "                nlp_ht2 = get_nlp(get_wd_label(ht[2]))\n",
    "                if not nlp_ht2:\n",
    "                    break\n",
    "\n",
    "                k_lemma = ' '.join([e.lower_ for e in nlp_k if e.pos_ != \"DET\"])\n",
    "                ht2_lemma = ' '.join([e.lower_ for e in nlp_ht2 if e.pos_ != \"DET\"])\n",
    "                \n",
    "                if (nlp_k.text.lower() != nlp_ht2.text.lower() \n",
    "                    and k_lemma != nlp_ht2[0].text.lower()\n",
    "                    and nlp_k.text.lower() != ht2_lemma\n",
    "                    and k_lemma != ht2_lemma\n",
    "                   ):\n",
    "                    hypo_sum += get_similarity_by_words(nlp_ht2, nlp_k)\n",
    "                    for ap in anchors_predicates:\n",
    "                        #print(\"ap\",ap, \"ht\",ht, \"hypo_sum\",hypo_sum)\n",
    "                        if ap[0] == ht[1]:\n",
    "                            hypo_sum = abs(hypo_sum)\n",
    "                            if get_wd_label(ht[0]).lower() in keywords_names:\n",
    "                                hypo_sum += abs(ap[1])\n",
    "                            if get_wd_label(ht[2]).lower() in keywords_names:\n",
    "                                hypo_sum += abs(ap[1])\n",
    "                            #else: hypo_sum = ap[1]\n",
    "                            #hypo_sum *= abs(ap[1])\n",
    "                                                        \n",
    "                            \n",
    "                            #break\n",
    "                            #print(\"ap\",ap, \"ht\",ht, \"hypo_sum\",hypo_sum)\n",
    "                            #print(ht)\n",
    "                            #break\n",
    "                            #hypo_sum = abs(hypo_sum)\n",
    "                            #hypo_sum += abs(ap[1])\n",
    "                            #hypo_sum += abs(ap[1])\n",
    "                            #hypo_sum += ap[1]\n",
    "                            #hypo_sum += abs(hypo_sum)\n",
    "                            #hypo_sum *= abs(ap[1])\n",
    "                            \n",
    "                            \n",
    "                            #hypo_sum = abs(hypo_sum)\n",
    "                            #hypo_sum /= ap[1]\n",
    "                            #hypo_sum -= ap[1]\n",
    "                            #hypo_sum += hypo_sum/ap[1]\n",
    "                        \n",
    "                            \n",
    "                    #print(paths_keywords[1])\n",
    "                    if get_wd_label(ht[0]).lower() in paths_keywords[0]: \n",
    "                        if get_wd_label(ht[2]).lower() in paths_keywords[0]:\n",
    "                            continue\n",
    "                        else:\n",
    "                            hypo = ht[2]\n",
    "                    else: hypo = ht[0]\n",
    "\n",
    "                    if not hypothesises: hypothesises.append([hypo, hypo_sum])\n",
    "                    else: \n",
    "                        if hypo in [h[0] for h in hypothesises]:\n",
    "                            for i, h in enumerate(hypothesises):\n",
    "                                if hypo == h[0]: hypothesises[i] = [hypo, hypo_sum*hypothesises[i][1]]\n",
    "                        else: hypothesises.append([hypo, hypo_sum])\n",
    "    \n",
    "    return sorted(hypothesises, key=lambda x: x[-1], reverse=True)\n",
    "    \n",
    "#hypothesises_2 = get_hypothesises(q_nlp_2, paths_keywords_2, paths_nodes_filtered_2)#, q_themes_2, q_themes_enhanced_2)\n",
    "#print(\"q_nlp_2\", q_nlp_2)\n",
    "#print(\"paths_keywords_2\", paths_keywords_2)\n",
    "#print(\"paths_nodes_filtered\", paths_nodes_filtered)\n",
    "#print(\"hypothesises_2\", hypothesises_2)\n",
    "#print(hypothesises_2)\n",
    "#print([(get_wd_label(h[0]),h[1]) for i,h in enumerate(hypothesises_2)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_by_n(l, i):\n",
    "    list_n = []\n",
    "    for j in range(0, len(l)+1, 1):\n",
    "        tmp = l[j-i:i+j-i]\n",
    "        if tmp:\n",
    "            list_n.append(tmp)\n",
    "    return list_n\n",
    "\n",
    "def match_hypothesises(graph, question, themes, predicates, hypothesises, paths):\n",
    "    meaningful_paths = []\n",
    "    \n",
    "    theme_ids = sum([t[1] for t in themes[0]],[])\n",
    "    for p in paths:\n",
    "        counter = 0\n",
    "        for ti in theme_ids:\n",
    "            if ti in p and p not in meaningful_paths:\n",
    "                counter += 1\n",
    "        for pred in [p[1] for p in predicates]:\n",
    "            for e in pred:\n",
    "                if e in p:\n",
    "                    counter += 1\n",
    "                else:\n",
    "                    counter = 0\n",
    "        for hypo in hypothesises:\n",
    "            if hypo[0] in p:\n",
    "                counter += 1\n",
    "            if hypo[0] == p[0]:\n",
    "                counter += 1\n",
    "            if hypo[0] == p[-1]:\n",
    "                counter += 1\n",
    "        \n",
    "        if counter > 0: meaningful_paths.append((counter, p))\n",
    "\n",
    "    meaningful_paths = sorted(meaningful_paths, key=lambda x: x[0], reverse=True)\n",
    "    #print(\"meaningful_paths:\",len(meaningful_paths))\n",
    "    #print(\"\\n\")\n",
    "    \n",
    "    looped_paths = []\n",
    "    for hypo in hypothesises:\n",
    "        for mp in meaningful_paths:\n",
    "            if mp[1][0] == hypo[0] or mp[1][-1] == hypo[0]:\n",
    "                if graph.has_node(mp[1][0]) and graph.has_node(mp[1][-1]):\n",
    "                    path_tmp = list(nx.all_simple_paths(graph, mp[1][0],mp[1][-1]))\n",
    "                    if len(path_tmp)>1:\n",
    "                        for p in path_tmp:\n",
    "                            if p not in [lp[1] for lp in looped_paths]:\n",
    "                                looped_paths.append((mp[0],p))\n",
    "                #else:\n",
    "                #    if not graph.has_node(mp[1][0]):\n",
    "                #        print(\"MISSING NODE:\", mp[1][0], get_wd_label(mp[1][0]))\n",
    "                #    if not graph.has_node(mp[1][-1]):\n",
    "                #        print(\"MISSING NODE:\", mp[1][-1], get_wd_label(mp[1][-1]))\n",
    "                \n",
    "    #print(\"looped_paths\", len(looped_paths))\n",
    "    \n",
    "    looped_paths_untagged = []\n",
    "    for lp in looped_paths:\n",
    "        row_tmp = []\n",
    "        for w in lp[1]:\n",
    "            if w.find(\"-\") > 0:\n",
    "                row_tmp.append(w[:w.find(\"-\")])\n",
    "            else:\n",
    "                row_tmp.append(w)\n",
    "        looped_paths_untagged.append((lp[0],row_tmp))\n",
    "        \n",
    "    #print(\"looped_paths_untagged\",looped_paths_untagged)\n",
    "    \n",
    "    mp_similarities_untagged = []\n",
    "    mp_similarities_tagged = []\n",
    "    for i_lp, lp in enumerate(looped_paths_untagged):\n",
    "        #print(lp)\n",
    "        sentence = get_nlp(\" \".join([get_wd_label(w) for w in lp[1]]))\n",
    "        similarity = get_similarity_by_words(sentence, question)\n",
    "        #print(similarity, sentence)\n",
    "        mp_similarities_untagged.append((similarity,lp[1]))\n",
    "        mp_similarities_tagged.append((similarity,looped_paths[i_lp][1]))\n",
    "        \n",
    "    #print(\"mp_similarities_untagged\",len(mp_similarities_untagged))\n",
    "    #print(\"mp_similarities_untagged\",mp_similarities_untagged)\n",
    "    \n",
    "    mp_similarities_tagged = sorted(mp_similarities_tagged, key=lambda x: x[0], reverse=True)\n",
    "    mp_similarities_tagged = [mp for mp in mp_similarities_tagged if mp[0] > 0.9]\n",
    "    \n",
    "    mp_similarities_untagged = sorted(mp_similarities_untagged, key=lambda x: x[0], reverse=True)\n",
    "    mp_similarities_untagged = [mp for mp in mp_similarities_untagged if mp[0] > 0.9]\n",
    "    \n",
    "    #print(\"mp_similarities_untagged\",len(mp_similarities_untagged))\n",
    "    #print(\"mp_similarities_tagged\",len(mp_similarities_tagged))\n",
    "    \n",
    "    WH_FILTER = [\"WDT\", \"WP\", \"WP$\", \"WRB\"]\n",
    "    wh_position = [w.i for w in question if w.tag_ in WH_FILTER][0]\n",
    "    question_list = [w.lower_ for w in question if not w.is_punct]\n",
    "    question_list_filtered = [w.lower_ for w in question if not w.is_punct and w.tag_ not in WH_FILTER]\n",
    "    \n",
    "    golden_paths = []\n",
    "    for mp in mp_similarities_tagged:\n",
    "        #print(\"mp[1]\",mp[1])\n",
    "        for i_e, e in enumerate(mp[1]):\n",
    "            if i_e <= 1 or i_e >= len(mp[1])-2:\n",
    "                continue\n",
    "            if not is_wd_entity(e):\n",
    "                continue\n",
    "\n",
    "            mp_e_statements = get_all_statements_of_entity(e)\n",
    "            extended_paths = get_statements_by_id(mp_e_statements, e, mp[1][i_e+1][:mp[1][i_e+1].find(\"-\")], qualifier=False, statement_type=\"predicate\")\n",
    "            extended_paths_qualifier = get_statements_by_id(mp_e_statements, e, mp[1][i_e+1][:mp[1][i_e+1].find(\"-\")], qualifier=True, statement_type=\"qualifier_predicate\")\n",
    "\n",
    "            for ep in extended_paths_qualifier:\n",
    "                if (ep['entity']['id'] == mp[1][i_e] and \n",
    "                    ep['predicate']['id'] == mp[1][i_e-1][:mp[1][i_e-1].find(\"-\")] and\n",
    "                    ep['object']['id'] == mp[1][i_e-2] and\n",
    "                    ep['qualifiers']):\n",
    "                    for q in ep['qualifiers']:\n",
    "                        if(q['qualifier_predicate'][\"id\"] == mp[1][i_e+1][:mp[1][i_e+1].find(\"-\")] and\n",
    "                          q['qualifier_object'][\"id\"] == mp[1][i_e+2]):\n",
    "                            if mp[1] not in golden_paths:\n",
    "                                golden_paths.append(mp[1])\n",
    "\n",
    "                if (ep['entity']['id'] == mp[1][i_e+2] and \n",
    "                    ep['predicate']['id'] == mp[1][i_e+1][:mp[1][i_e+1].find(\"-\")] and\n",
    "                    ep['object']['id'] == mp[1][i_e] and\n",
    "                    ep['qualifiers']):\n",
    "                    for q in ep['qualifiers']:\n",
    "                        if(q['qualifier_predicate'][\"id\"] == mp[1][i_e-1][:mp[1][i_e-1].find(\"-\")] and\n",
    "                          q['qualifier_object'][\"id\"] == mp[1][i_e-2]):\n",
    "                            if mp[1] not in golden_paths:\n",
    "                                golden_paths.append(mp[1])\n",
    "\n",
    "            for ep in extended_paths:\n",
    "                if (ep['entity']['id'] == mp[1][i_e] and \n",
    "                    ep['predicate']['id'] == mp[1][i_e-1][:mp[1][i_e-1].find(\"-\")] and\n",
    "                    ep['object']['id'] == mp[1][i_e-2] and\n",
    "                    ep['qualifiers']):\n",
    "                    for q in ep['qualifiers']:\n",
    "                        if(q['qualifier_predicate'][\"id\"] == mp[1][i_e+1][:mp[1][i_e+1].find(\"-\")] and\n",
    "                          q['qualifier_object'][\"id\"] == mp[1][i_e+2]):\n",
    "                            if mp[1] not in golden_paths:\n",
    "                                golden_paths.append(mp[1])\n",
    "\n",
    "                if (ep['entity']['id'] == mp[1][i_e+2] and \n",
    "                    ep['predicate']['id'] == mp[1][i_e+1][:mp[1][i_e+1].find(\"-\")] and\n",
    "                    ep['object']['id'] == mp[1][i_e] and\n",
    "                    ep['qualifiers']):\n",
    "                    for q in ep['qualifiers']:\n",
    "                        if(q['qualifier_predicate'][\"id\"] == mp[1][i_e-1][:mp[1][i_e-1].find(\"-\")] and\n",
    "                          q['qualifier_object'][\"id\"] == mp[1][i_e-2]):\n",
    "                            if mp[1] not in golden_paths:\n",
    "                                golden_paths.append(mp[1])    \n",
    "\n",
    "    sorted_golden_paths = []\n",
    "    for gp in golden_paths:\n",
    "        tmp_gp = []\n",
    "        for e in gp:\n",
    "            if is_wd_entity(e):\n",
    "                tmp_gp.append(get_wd_label(e))\n",
    "            else:\n",
    "                tmp_gp.append(get_wd_label(e[:e.find(\"-\")]))\n",
    "        nlp_gp = get_nlp(\" \".join(tmp_gp))\n",
    "        sorted_golden_paths.append((get_similarity_by_words(question,nlp_gp), gp))\n",
    "\n",
    "    sorted_golden_paths = sorted(sorted_golden_paths, key=lambda x: x[0], reverse=True)\n",
    "    #print(\"len(sorted_golden_paths) BEFORE\",len(sorted_golden_paths))\n",
    "    \n",
    "    sorted_golden_paths = [sgp[1] for sgp in sorted_golden_paths]\n",
    "    \n",
    "    if not sorted_golden_paths: \n",
    "        for lp in [lp[1] for lp in looped_paths]:\n",
    "            if lp[0] == hypothesises[0][0]:\n",
    "                if lp not in sorted_golden_paths:\n",
    "                    sorted_golden_paths.append(lp)\n",
    "            if lp[-1] == hypothesises[0][0]:\n",
    "                lp = list(reversed(lp))\n",
    "                if lp not in sorted_golden_paths:\n",
    "                    sorted_golden_paths.append(lp)\n",
    "    \n",
    "    #print(\"len(sorted_golden_paths) AFTER\",len(sorted_golden_paths))\n",
    "        \n",
    "    if not sorted_golden_paths: \n",
    "        for p in paths:\n",
    "            #print(p)\n",
    "            if p[0] == hypothesises[0][0]:\n",
    "                #print(p)\n",
    "                if p not in sorted_golden_paths:\n",
    "                    sorted_golden_paths.append(p)\n",
    "            if p[-1] == hypothesises[0][0]:\n",
    "                p = list(reversed(p))\n",
    "                if p not in sorted_golden_paths:\n",
    "                    sorted_golden_paths.append(p)\n",
    "                    \n",
    "    #print(\"len(sorted_golden_paths) AFTER AFTER\",len(sorted_golden_paths))\n",
    "    \n",
    "    if not sorted_golden_paths:\n",
    "        for p in paths:\n",
    "            if hypothesises[0][0] in p:\n",
    "                if p not in sorted_golden_paths:\n",
    "                    sorted_golden_paths.append(p)\n",
    "                    \n",
    "    #print(\"len(sorted_golden_paths) AFTER AFTER AFTER\",len(sorted_golden_paths))\n",
    "    \n",
    "    \n",
    "    golden_paths_filtered = []\n",
    "    for gp in sorted_golden_paths:\n",
    "        tmp_path = []\n",
    "        for i_e, e in enumerate(gp):\n",
    "            if i_e < len(gp)-2 and not is_wd_entity(e):\n",
    "                if e == gp[i_e+2]:\n",
    "                    golden_paths_filtered.append(gp[:gp.index(e)+2])\n",
    "                    break\n",
    "                else:\n",
    "                    tmp_path.append(e)\n",
    "            else:\n",
    "                tmp_path.append(e)\n",
    "        \n",
    "        if tmp_path:\n",
    "            for i_e, e in enumerate(tmp_path):\n",
    "                if is_wd_entity(e):\n",
    "                    if tmp_path.count(e) > 1:\n",
    "                        pass\n",
    "                    else:\n",
    "                        if tmp_path not in golden_paths_filtered:\n",
    "                            golden_paths_filtered.append(tmp_path)\n",
    "                            \n",
    "    #print(\"len(golden_paths_filtered)\",len(golden_paths_filtered))\n",
    "    \n",
    "    golden_unique_paths = golden_paths_filtered.copy()\n",
    "    for i_sgp, sgp in enumerate(golden_paths_filtered):\n",
    "        for sgp_2 in golden_paths_filtered:\n",
    "            if (is_sublist(sgp, sgp_2) and sgp!=sgp_2):\n",
    "                golden_unique_paths[i_sgp] = []\n",
    "                break\n",
    "    \n",
    "    #print()\n",
    "    golden_unique_paths = [gup for gup in golden_unique_paths if gup]\n",
    "    hypothesises_names = [h[0] for h in hypothesises]\n",
    "    \n",
    "    #print(\"golden_unique_paths[0][0]\",golden_unique_paths[0][0])\n",
    "    #print(\"hypothesises_names\",hypothesises_names)\n",
    "    if hypothesises_names[0] != golden_unique_paths[0][0]:\n",
    "        if golden_unique_paths[0][0] in hypothesises_names:\n",
    "            #print(\"hypothesises_names.index(golden_unique_paths[0][0])\",hypothesises_names.index(golden_unique_paths[0][0]))\n",
    "            hypothesises_names.pop(hypothesises_names.index(golden_unique_paths[0][0]))\n",
    "        hypothesises_names.insert(0,golden_unique_paths[0][0])\n",
    "    \n",
    "    golden_unique_paths = [hypothesises_names]+golden_unique_paths\n",
    "    #print(\"len(golden_unique_paths)\",len(golden_unique_paths))\n",
    "    \n",
    "    return golden_unique_paths\n",
    "\n",
    "\n",
    "#start_time = time.time()\n",
    "#golden_paths_2 = match_hypothesises(graph_2, q_nlp_2, q_themes_2, q_predicates_2, hypothesises_2, paths_nodes_filtered_2)\n",
    "#end_time = time.time()\n",
    "#print(\"Golden paths ->\\tRunning time is {}s\".format(round(end_time-start_time,2)))\n",
    "#print(golden_paths_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "## questions = (\"what was the cause of death of yves klein\",\n",
    "#               \"Who is the wife of Barack Obama?\",\n",
    "#               \"Who is the president of the United States?\",\n",
    "#               \"When was produced the first Matrix movie?\",\n",
    "#               \"Who made the soundtrack of the The Last Unicorn movie?\",\n",
    "#               \"Who is the author of Le Petit Prince?\",\n",
    "#               \"Which actor voiced the Unicorn in The Last Unicorn?\",\n",
    "#               \"how is called the rabbit in Alice in Wonderland?\"\n",
    "#              )\n",
    "\n",
    "def answer_question(question, verbose=False, aggressive=False, looped=False, deep_k=50):\n",
    "    if verbose: start_time = time.time()\n",
    "    q_nlp = get_nlp(question)\n",
    "    if verbose: print(\"-> q_nlp:\",q_nlp)\n",
    "    q_themes = get_themes(q_nlp, top_k=3)\n",
    "    if verbose: print(\"-> q_themes:\",q_themes)\n",
    "    q_themes_enhanced = get_enhanced_themes(q_themes, top_k=3, aggressive=aggressive)\n",
    "    if verbose: print(\"-> q_themes_enhanced:\",q_themes_enhanced)\n",
    "    q_predicates = get_predicates(q_nlp, top_k=3)\n",
    "    if q_predicates:\n",
    "        if not q_predicates[0][1]: q_predicates = get_predicates_online(q_nlp, top_k=3, aggressive=aggressive)\n",
    "    if verbose: print(\"-> q_predicates:\",q_predicates)\n",
    "    q_focused_parts = get_focused_parts(q_nlp, top_k=3)\n",
    "    if verbose: print(\"-> q_focused_parts:\",q_focused_parts)\n",
    "    if verbose: print(\"-> Building the graph with k_deep\",str(deep_k),\"... (could be long)\")\n",
    "    for k in range(10, deep_k, 10):\n",
    "        graph, predicates_dict = build_graph(q_nlp, q_themes, q_themes_enhanced, q_predicates, deep_k=deep_k)\n",
    "        if graph.size() > 500 or len(graph) > 500:\n",
    "            deep_k -= 10\n",
    "            if verbose: print(\"---> Rebuilding the graph with k_deep\",str(deep_k), \"... Previously:\",len(graph), \"nodes or\", graph.size(), \"edges was above the limit...\")\n",
    "        else: break\n",
    "    if verbose: print(\"--> \",len(graph), \"nodes and\", graph.size(), \"edges\")\n",
    "    if verbose: print(\"-> predicates_dict:\",predicates_dict)\n",
    "    paths_keywords = find_paths_keywords(graph, q_nlp, q_themes, q_themes_enhanced, q_predicates, q_focused_parts)\n",
    "    if verbose: print(\"-> paths_keywords:\",paths_keywords)\n",
    "    if verbose: print(\"-> Computing possible paths... (could be long)\")\n",
    "    path_nodes = find_path_nodes_from_graph_2(graph, paths_keywords, threshold=0.9, thres_inter=0.05, top_k=3, top_performance=50,min_paths=3000)\n",
    "    if verbose: print(\"--> len(path_nodes):\",len(path_nodes))\n",
    "    if len(path_nodes) < 20000:\n",
    "        if verbose: print(\"-> Filtering paths... (could be long)\")\n",
    "        paths_nodes_filtered = paths_nodes_filter(path_nodes, graph)\n",
    "        if verbose: \n",
    "            print(\"--> len(paths_nodes_filtered):\",len(paths_nodes_filtered))\n",
    "            #print(\"paths_nodes_filtered:\",paths_nodes_filtered)\n",
    "    else: \n",
    "        if verbose: print(\"--> Skipping paths filtering... (too much paths)\")\n",
    "        paths_nodes_filtered = path_nodes\n",
    "    if verbose: print(\"-> Computing hypothesises...\")\n",
    "    hypothesises = get_hypothesises(q_nlp, paths_keywords, paths_nodes_filtered)\n",
    "    if verbose: print(\"--> hypothesises:\",hypothesises)\n",
    "    if hypothesises:\n",
    "        if verbose: print(\"-> Computing golden paths...\")\n",
    "        golden_paths = match_hypothesises(graph, q_nlp, q_themes, q_predicates, hypothesises, paths_nodes_filtered)\n",
    "        if verbose: print(\"--> len(golden_paths):\",len(golden_paths))\n",
    "    else:\n",
    "        if not looped:\n",
    "            if verbose: print(\"-> Looping on aggressive mode...\")\n",
    "            golden_paths = answer_question(question, verbose=verbose, aggressive=True, looped=True, deep_k=deep_k)\n",
    "        else: \n",
    "            if verbose: print(\"--> End of loop\")\n",
    "            golden_paths=[]\n",
    "\n",
    "    save_cache_data()\n",
    "    \n",
    "    if golden_paths:\n",
    "        cleared_golden_paths = [golden_paths[0].copy()]\n",
    "        for p in golden_paths[1:]:\n",
    "            tmp_translation = []\n",
    "            for e in p:\n",
    "                if is_wd_entity(e) or is_wd_predicate(e):\n",
    "                    tmp_translation.append(e)\n",
    "                else:\n",
    "                    if e[0] == \"P\": tmp_translation.append(e[:e.find(\"-\")])\n",
    "                    else: tmp_translation.append(e)\n",
    "            if tmp_translation not in cleared_golden_paths:\n",
    "                cleared_golden_paths.append(tmp_translation)\n",
    "        \n",
    "        if verbose: print(\"--> len(cleared_golden_paths):\",len(cleared_golden_paths))\n",
    "            \n",
    "    if verbose: \n",
    "        end_time = time.time()\n",
    "        print(\"->\\tRunning time is {}s\".format(round(end_time-start_time,2)))\n",
    "                    \n",
    "    if golden_paths:\n",
    "        return cleared_golden_paths\n",
    "    else: return False\n",
    "\n",
    "\n",
    "#answer = answer_question(\"what film is by the writer phil hay?\", verbose=True)\n",
    "#answer = answer_question(\"When was produced the first Matrix movie?\", verbose=True) \n",
    "#answer = answer_question(\"Which actor voiced the Unicorn in The Last Unicorn?\", verbose=False) #works \n",
    "#answer = answer_question(\"Which is the nation of Martha Mattox\", verbose=True)\n",
    "#answer = answer_question(\"Who made the soundtrack of the The Last Unicorn movie?\", verbose=True)\n",
    "#answer = answer_question(\"Who is the author of Le Petit Prince?\", verbose=True)\n",
    "\n",
    "#answer = answer_question(\"When was produced the first Matrix movie?\", verbose=True)\n",
    "#answer = answer_question(\"Who is the president of the United States?\", verbose=True) #node Q76 not in graph\n",
    "#answer = answer_question(\"Who is the wife of Barack Obama?\", verbose=False) #works 308.15s\n",
    "#answer = answer_question(\"what was the cause of death of yves klein\", verbose=True)\n",
    "#answer = answer_question(\"what city was alex golfis born in\", verbose=True)\n",
    "#answer = answer_question(\"which stadium do the wests tigers play in\", verbose=True) #462.47s\n",
    "#answer = answer_question(\"lol\", verbose=True)\n",
    "\n",
    "#if answer: \n",
    "#    print(\"Answer:\",get_wd_label(answer[0][0]), \"(\"+str(answer[0][0])+\")\")\n",
    "#    print(\"Paths:\",[[get_wd_label(e) for e in row] for row in answer[1:]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print([get_wd_label(answer[0][0])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to_translate = ['Q13133', 'P26', 'Q76', 'P31', 'Q5', 'P31', 'Q24039104', 'P21', 'Q6581072', 'P1552', 'Q188830', 'P26', 'Q18531596']\n",
    "#to_translate = ['Q202725', 'P725', 'Q176198', 'P453', 'Q30060419', 'P31', 'Q30167264', 'P1889', 'Q7246', 'P138', 'Q18356448']\n",
    "#\n",
    "#masked = []\n",
    "#for tt in to_translate:\n",
    "#    masked.append(get_wd_label(tt))\n",
    "#    masked.append(\"[MASK]\")\n",
    "#print(\"marked\",masked)\n",
    "#\n",
    "#print(\"->\",[get_wd_label(e) for e in to_translate])\n",
    "#print(\"-->\",\" \".join([get_wd_label(e) for e in to_translate]))\n",
    "#\n",
    "#print(\"-->\",\" [MASK] \".join([get_wd_label(e) for e in to_translate]))\n",
    "#print(\"-->\",\"[\" +\" , [MASK] , \".join([get_wd_label(e) for e in to_translate])+\"]\")\n",
    "#\n",
    "#FILTER_ELEMENTS = ['P31']\n",
    "#filtered_by_elements = [\"[MASK]\" if x in FILTER_ELEMENTS else get_wd_label(x) for x in to_translate]\n",
    "#\n",
    "#print(\"-->\",\" [MASK] \".join([e for e in filtered_by_elements]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#graph_2.has_node(\"Q13133\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get_similarity_by_words(get_nlp(\"mia farrow\"), get_nlp(\"farrow mia\")) #1.000000077374981\n",
    "#get_similarity_by_words(get_nlp(\"actor voiced\"), get_nlp(\"voice actor\")) #0.8541489425987572 \n",
    "#get_similarity_by_words(get_nlp(\"actor voiced the unicorn in the last unicorn\"), \n",
    "#                        get_nlp(\"the unicorn last unicorn actor voiced\")) #0.9573255410217848\n",
    "#get_similarity_by_words(get_nlp(\"voice actor\"),get_nlp(\"instance of\")) #0.30931508860569823\n",
    "#get_similarity_by_words(get_nlp(\"voice actor\"),get_nlp(\"present in work\")) #0.34966764303274056\n",
    "#get_similarity_by_words(get_nlp(\"voice actor\"),get_nlp(\"subject has role\")) #0.5026860362728758\n",
    "\n",
    "#get_similarity_by_words(get_nlp(\"voice actor\"),get_nlp(\"protagonist\")) #0.4688377364169893\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#subgraphs = [graph.subgraph(c) for c in nx.connected_components(graph)]\n",
    "#print(len(subgraphs))\n",
    "#[len(s.nodes) for s in subgraphs]\n",
    "#len(subgraphs[0].nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for path in nx.all_simple_paths(graph, source=\"Q176198\", target=\"Q202725\"):\n",
    "#    print(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nx.shortest_path(graph, source=\"Q176198\", target=\"Q202725\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nlp_lookup_test = get_nlp(\"klein yves\")\n",
    "#[y['name'] for x,y in graph.nodes(data=True) if get_nlp(y['name']).similarity(nlp_lookup_test) >= 0.9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#list(nx.dfs_labeled_edges(graph, source=get_themes(q0_nlp, top_k=3)[0][0][1][0], depth_limit=4))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot_graph(graph_2, \"test_file_name_graph\", \"Graph_title\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nlp_lookup_test = get_nlp(\"klein yves\")\n",
    "#[y['name'] for x,y in graph.nodes(data=True) if get_nlp(y['name']).similarity(nlp_lookup_test) >= 0.9]\n",
    "#[y['name'] for x,y in graph_2.nodes(data=True) if y['type'] == 'predicate']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:qa]",
   "language": "python",
   "name": "conda-env-qa-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
