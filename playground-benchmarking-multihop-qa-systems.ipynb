{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "IPython.notebook.set_autosave_interval(180000)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autosaving every 180 seconds\n",
      "WARNING:tensorflow:From /data/users/romain.claret/miniconda3/envs/qa/lib/python3.7/site-packages/txt2txt/txt2txt.py:25: The name tf.GPUOptions is deprecated. Please use tf.compat.v1.GPUOptions instead.\n",
      "\n",
      "WARNING:tensorflow:From /data/users/romain.claret/miniconda3/envs/qa/lib/python3.7/site-packages/txt2txt/txt2txt.py:27: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "WARNING:tensorflow:From /data/users/romain.claret/miniconda3/envs/qa/lib/python3.7/site-packages/txt2txt/txt2txt.py:27: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the params file\n",
      "Input encoding {'o': 2, '{': 3, '.': 4, 'J': 5, '0': 6, '1': 7, '<': 8, 'B': 9, 'd': 10, '£': 11, 'e': 12, '6': 13, '!': 14, 'O': 15, 'M': 16, 'X': 17, 'f': 18, 't': 19, 'C': 20, 'V': 21, 'z': 22, 'K': 23, '\\\\': 24, '9': 25, 'P': 26, 'S': 27, '/': 28, '₹': 29, 'F': 30, 'G': 31, '=': 32, '8': 33, ')': 34, '+': 35, ']': 36, 'U': 37, \"'\": 38, '\"': 39, 'g': 40, 'N': 41, 'r': 42, 'u': 43, '&': 44, '$': 45, 'x': 46, '%': 47, ':': 48, '@': 49, '^': 50, 'I': 51, 'L': 52, 'Z': 53, 'h': 54, 'W': 55, 'A': 56, 'v': 57, '?': 58, '2': 59, '~': 60, 's': 61, 'T': 62, 'R': 63, ',': 64, '|': 65, '4': 66, '>': 67, 'y': 68, '(': 69, '[': 70, 'k': 71, 'H': 72, 'l': 73, 'j': 74, '7': 75, 'n': 76, 'i': 77, 'D': 78, 'Q': 79, ' ': 80, 'm': 81, 'Y': 82, '*': 83, '}': 84, '#': 85, 'p': 86, 'q': 87, '5': 88, 'c': 89, '`': 90, 'a': 91, 'b': 92, 'w': 93, '3': 94, 'E': 95, ';': 96, '-': 97}\n",
      "Input decoding {2: 'o', 3: '{', 4: '.', 5: 'J', 6: '0', 7: '1', 8: '<', 9: 'B', 10: 'd', 11: '£', 12: 'e', 13: '6', 14: '!', 15: 'O', 16: 'M', 17: 'X', 18: 'f', 19: 't', 20: 'C', 21: 'V', 22: 'z', 23: 'K', 24: '\\\\', 25: '9', 26: 'P', 27: 'S', 28: '/', 29: '₹', 30: 'F', 31: 'G', 32: '=', 33: '8', 34: ')', 35: '+', 36: ']', 37: 'U', 38: \"'\", 39: '\"', 40: 'g', 41: 'N', 42: 'r', 43: 'u', 44: '&', 45: '$', 46: 'x', 47: '%', 48: ':', 49: '@', 50: '^', 51: 'I', 52: 'L', 53: 'Z', 54: 'h', 55: 'W', 56: 'A', 57: 'v', 58: '?', 59: '2', 60: '~', 61: 's', 62: 'T', 63: 'R', 64: ',', 65: '|', 66: '4', 67: '>', 68: 'y', 69: '(', 70: '[', 71: 'k', 72: 'H', 73: 'l', 74: 'j', 75: '7', 76: 'n', 77: 'i', 78: 'D', 79: 'Q', 80: ' ', 81: 'm', 82: 'Y', 83: '*', 84: '}', 85: '#', 86: 'p', 87: 'q', 88: '5', 89: 'c', 90: '`', 91: 'a', 92: 'b', 93: 'w', 94: '3', 95: 'E', 96: ';', 97: '-'}\n",
      "Output encoding {'o': 2, '{': 3, '.': 4, 'J': 5, '0': 6, '1': 7, '<': 8, 'B': 9, 'd': 10, '£': 11, 'e': 12, '6': 13, '!': 14, 'O': 15, 'M': 16, 'X': 17, 'f': 18, 't': 19, 'C': 20, 'V': 21, 'z': 22, 'K': 23, '\\\\': 24, '9': 25, 'P': 26, 'S': 27, '/': 28, '₹': 29, 'F': 30, 'G': 31, '=': 32, '8': 33, ')': 34, '+': 35, ']': 36, 'U': 37, \"'\": 38, '\"': 39, 'g': 40, 'N': 41, 'r': 42, 'u': 43, '&': 44, '$': 45, 'x': 46, '%': 47, ':': 48, '@': 49, '^': 50, 'I': 51, 'L': 52, 'Z': 53, 'h': 54, 'W': 55, 'A': 56, 'v': 57, '?': 58, '2': 59, '~': 60, 's': 61, 'T': 62, 'R': 63, ',': 64, '|': 65, '4': 66, '>': 67, 'y': 68, '(': 69, '[': 70, 'k': 71, 'H': 72, 'l': 73, 'j': 74, '7': 75, 'n': 76, 'i': 77, 'D': 78, 'Q': 79, ' ': 80, 'm': 81, 'Y': 82, '*': 83, '}': 84, '#': 85, 'p': 86, 'q': 87, '5': 88, 'c': 89, '`': 90, 'a': 91, 'b': 92, 'w': 93, '3': 94, 'E': 95, ';': 96, '-': 97}\n",
      "Output decoding {2: 'o', 3: '{', 4: '.', 5: 'J', 6: '0', 7: '1', 8: '<', 9: 'B', 10: 'd', 11: '£', 12: 'e', 13: '6', 14: '!', 15: 'O', 16: 'M', 17: 'X', 18: 'f', 19: 't', 20: 'C', 21: 'V', 22: 'z', 23: 'K', 24: '\\\\', 25: '9', 26: 'P', 27: 'S', 28: '/', 29: '₹', 30: 'F', 31: 'G', 32: '=', 33: '8', 34: ')', 35: '+', 36: ']', 37: 'U', 38: \"'\", 39: '\"', 40: 'g', 41: 'N', 42: 'r', 43: 'u', 44: '&', 45: '$', 46: 'x', 47: '%', 48: ':', 49: '@', 50: '^', 51: 'I', 52: 'L', 53: 'Z', 54: 'h', 55: 'W', 56: 'A', 57: 'v', 58: '?', 59: '2', 60: '~', 61: 's', 62: 'T', 63: 'R', 64: ',', 65: '|', 66: '4', 67: '>', 68: 'y', 69: '(', 70: '[', 71: 'k', 72: 'H', 73: 'l', 74: 'j', 75: '7', 76: 'n', 77: 'i', 78: 'D', 79: 'Q', 80: ' ', 81: 'm', 82: 'Y', 83: '*', 84: '}', 85: '#', 86: 'p', 87: 'q', 88: '5', 89: 'c', 90: '`', 91: 'a', 92: 'b', 93: 'w', 94: '3', 95: 'E', 96: ';', 97: '-'}\n",
      "WARNING:tensorflow:From /data/users/romain.claret/miniconda3/envs/qa/lib/python3.7/site-packages/tensorflow/python/keras/backend.py:3673: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            (None, 202)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_1 (InputLayer)            (None, 202)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)         (None, 202, 256)     25088       input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 202, 128)     12544       input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_2 (LSTM)                   (None, 202, 256)     525312      embedding_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional) (None, 202, 256)     263168      embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dot_1 (Dot)                     (None, 202, 202)     0           lstm_2[0][0]                     \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "attention (Activation)          (None, 202, 202)     0           dot_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dot_2 (Dot)                     (None, 202, 256)     0           attention[0][0]                  \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 202, 512)     0           dot_2[0][0]                      \n",
      "                                                                 lstm_2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_1 (TimeDistrib (None, 202, 128)     65664       concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_2 (TimeDistrib (None, 202, 98)      12642       time_distributed_1[0][0]         \n",
      "==================================================================================================\n",
      "Total params: 904,418\n",
      "Trainable params: 904,418\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "import pandas as pd\n",
    "import time\n",
    "import convex as cx\n",
    "import tmqa3 as tmqa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversations_path = \"/data/users/romain.claret/tm/mse.tm.chatbot.base/data/convex/test_set/test_set_ALL.json\"\n",
    "with open(conversations_path, \"r\") as data:\n",
    "    conversations = json.load(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pickle_data(df, filename):\n",
    "    pd.set_option('display.max_columns', None)\n",
    "    filename = \"/data/users/romain.claret/tm/mse.tm.chatbot.base/benchmark_pickles/convex/\"+filename+'.pickle.bz2'\n",
    "    #df.summary = df.summary.map(sanitize_str)\n",
    "    print(\"Saving Dataframe Done!\",filename)\n",
    "    return df.to_pickle(filename, compression='bz2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lang: en, fr, de, it, es, zh\n",
    "#kb: dbpedia, wikidata, dblp, freebase\n",
    "def ask_qanswer(question):\n",
    "    data = {'query': question,'lang': 'en','kb': 'wikidata'}\n",
    "    query = requests.post('http://qanswer-core1.univ-st-etienne.fr/api/gerbil', data=data)\n",
    "    if not query:\n",
    "        return False\n",
    "    if (query.json()['questions'][0]['question']['answers']) == None:\n",
    "        return False\n",
    "    #if (query.json()['questions'][0]['question']['answers'].replace('\\n', '')) == None:\n",
    "    #    return False\n",
    "    #print(query.json()['questions'][0]['question']['answers'].replace('\\n', '').get(\"results\"))\n",
    "    try:\n",
    "        response = (json.loads(query.json()\n",
    "                .get(\"questions\")[0]\n",
    "                .get(\"question\")\n",
    "                .get(\"answers\")\n",
    "                .replace('\\n', ''))\n",
    "         .get(\"results\").get(\"bindings\"))\n",
    "    except:\n",
    "        return False\n",
    "    \n",
    "    if response:\n",
    "        return response[0].get(\"o1\").get(\"value\")[len(\"http://www.wikidata.org/entity/\"):] if response[0].get(\"o1\") is not None else False\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "#ask_qanswer(\"Who is the wife of Barack Obama\")\n",
    "#ask_qanwser(\"Which equestrian was born in dublin?\")\n",
    "#ask_qanswer(\"what is the main language spoken in a ghentar si muore facile\")\n",
    "#ask_qanswer(\"was the film helpmates in color or black-and-white?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_platypus(question):\n",
    "    headers = {'Accept': 'application/json','Accept-Language': 'en',}\n",
    "    params = (('q', question),('lang', 'en'))\n",
    "\n",
    "    response = requests.get('https://qa.askplatyp.us/v0/ask', headers=headers, params=params)\n",
    "    if response:\n",
    "        if type(response.json()['member']) is list:\n",
    "            #print(response.json()['member'][0]['result'])\n",
    "            if response.json()['member'] != []:\n",
    "                if '@id' in (json.dumps(response.json()['member'][0]['result'])):\n",
    "                    try:\n",
    "                        ps_result = (json.dumps(response.json()['member'][0]['result']['@id']))\n",
    "                    except:\n",
    "                        return False\n",
    "                else: return False\n",
    "            else: return False\n",
    "        else:\n",
    "            try:\n",
    "                if '@id' in (json.dumps(response.json()['member']['result'])):\n",
    "                    ps_result = (json.dumps(response.json()[\"member\"]['result']['@id']))\n",
    "                else: return False\n",
    "            except:\n",
    "                return False\n",
    "    else: return False\n",
    "    ps_result = ps_result[4:-1]\n",
    "    #print(result[:1])\n",
    "    if ps_result[:1] != 'P' and ps_result[:1] != 'Q':\n",
    "        return False\n",
    "    return ps_result\n",
    "#ask_platypus(\"Which genre of album is harder.....faster?\")\n",
    "#ask_platypus(\"how does engelbert zaschka identify\")\n",
    "#ask_platypus(\"Which Swiss conductor's cause of death is myoc...\")\n",
    "#ask_platypus(\"where was padraic mcguinness's place of death\")\n",
    "#ask_platypus(\"was the film helpmates in color or black-and-white?\")\n",
    "#ask_platypus(\"Who created the show life on earth\")\n",
    "#ask_platypus(\"Who is the wife of Barack Obama\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_convex(question):\n",
    "    cx_result = cx.answer_complete_question(question, cx.tagmeToken)['answers'][0]['answer']\n",
    "    #print(cx_result)\n",
    "    #answer = str(cx.wd.wikidata_id_to_label(result['answers'][0]['answer']))\n",
    "    try:\n",
    "        if not cx_result:\n",
    "            return cx_result\n",
    "        if cx_result[:1] != 'P' and cx_result[:1] != 'Q':\n",
    "            return False\n",
    "        return cx_result\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "#ask_convex(\"Which actor voiced the Unicorn in The Last Unicorn?\")\n",
    "#ask_convex(\"Which genre of album is harder.....faster?\")\n",
    "#ask_convex(\"Which label is somevelvetsidewalk signed to ttle of fort fisher \")\n",
    "#ask_convex(\"Who is the wife of Barack Obama\")\n",
    "#ask_convex(\"100% senorita is a television show in what language?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_tmqa(question, verbose=False, timer=False, show_graph=False, cores=tmqa.mp.cpu_count(), banning_str=False):\n",
    "    tmqa_result = tmqa.answer_question(question, verbose=verbose, timer=timer, show_graph=show_graph, cores=cores, banning_str=banning_str)\n",
    "    #print(tmqa_result)\n",
    "    if not tmqa_result:\n",
    "        return tmqa_result\n",
    "    #if tmqa_result[]\n",
    "    #if tmqa_result[0][0][:1] != 'P' and tmqa_result[0][0][:1] != 'Q':\n",
    "    #    return False\n",
    "    return tmqa_result[0]\n",
    "\n",
    "#answer = ask_tmqa_1(\"Which actor voiced the Unicorn in The Last Unicorn?\", verbose=True)\n",
    "#answer = ask_tmqa_1(\"what's akbar tandjung's ethnicity\", verbose=True)\n",
    "#ask_tmqa_1(\"Which genre of album is harder.....faster?\")\n",
    "#ask_tmqa_1(\"Which label is somevelvetsidewalk signed to ttle of fort fisher \")\n",
    "#ask_tmqa(\"Who is the wife of Barack Obama\")\n",
    "#print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#HEADERS = ['question', 'answer', 'qanswer', 'platypus', 'convex',\n",
    "#           'tm', \"tm_time\", \"tm_top2\", \"tm_top3\", \"tm_top4\", \"tm_top5\", \"tm_topall\"]\n",
    "#df = pd.DataFrame(columns=HEADERS)\n",
    "#LOADING\n",
    "df = pd.read_pickle(\"/data/users/romain.claret/tm/mse.tm.chatbot.base/benchmark_pickles/convex/benchmarking-qanswer-platypus-convex-qagraph-from-0-to-332.pickle.bz2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['What is the name of the writer of The Secret Garden?', 'Where does the story take place?', 'When was the book published?', 'What kind of book is it?', 'What magazine first published the book?']\n",
      "['Q276028', 'Q163', '1910-01-01T00:00:00Z', 'Q1436734', 'Q3985643']\n"
     ]
    }
   ],
   "source": [
    "for conversation in conversations:\n",
    "    questions = [turn['question'] for turn in conversation['questions']]\n",
    "    print(questions)\n",
    "    golden_answers = [tmqa.wikidata_url_to_wikidata_id(turn['answer']) for turn in conversation['questions']]\n",
    "    print(golden_answers)\n",
    "    \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t>>> Processing 332/2240: (Q169889) On which network did Lost originally air?                                  Asking Platypus\n",
      "Asking Convex\n",
      "\n",
      " 332 -> Convex Q169889\n",
      "Asking GraphQA\n",
      "User input: On which network did Lost originally air?\n",
      "--> Auto correcting question in progress...\n",
      "WARNING:tensorflow:From /data/users/romain.claret/miniconda3/envs/qa/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "-> Auto corrected q_nlp: On which network did Lost originally air \n",
      "-> q_themes: ([(air, ['Q7391292', 'Q318452']), (Air, ['Q11189065', 'Q11190093']), (Network, ['Q7000691', 'Q572165']), (network, ['Q15993745', 'Q1900326'])], [which network, originally air, On which network did Lost, Originally Air])\n",
      "-> q_themes_enhanced: [('originally', ['Q53737447']), ('Originally', ['Q7102610']), ('Lost', ['Q223446']), ('lost', ['P1356']), ('Lose', ['Q33120323'])]\n",
      "--> Calculating predicates... (could be long.. depends on uncached unpure predicates)\n",
      "behold: get_most_similar started with: network\n",
      "behold: get_most_similar started with: air\n",
      "-> q_predicates: [(did, ['P248']), (Lost, ['P1356']), (network, []), (air, [])]\n",
      "-> q_predicates \tRunning time is 86.31s\n",
      "-> q_focused_parts: [(network, ['Q15993745', 'Q1900326'])]\n",
      "-> Building the graph with k_deep 3 ... (could be long)\n",
      "->New graph \tRunning time is 37.99s\n",
      "-->  217 nodes and 218 edges\n",
      "--> Removing meaningless subgraphs\n",
      "--> New graph of: 217 nodes and 218 edges\n",
      "-> predicates_dict: {'P1160': 1, 'P1433': 1, 'P279': 3, 'P31': 17, 'P1545': 1, 'P155': 4, 'P156': 3, 'P179': 1, 'P361': 8, 'P3744': 1, 'P2002': 1, 'P679': 1, 'P1113': 1, 'P1013': 2, 'P642': 3, 'P2453': 5, 'P805': 1, 'P1411': 6, 'P495': 2, 'P571': 2, 'P1686': 1, 'P585': 2, 'P166': 3, 'P518': 1, 'P407': 1, 'P527': 6, 'P580': 1, 'P582': 1, 'P364': 1, 'P449': 1, 'P1552': 1, 'P138': 3, 'P1346': 2, 'P734': 3, 'P437': 1, 'P825': 1, 'P1680': 1, 'P282': 1, 'P1479': 1, 'P910': 1, 'P175': 1, 'P588': 3, 'P840': 1, 'P452': 1, 'P373': 1, 'P935': 1, 'P1240': 1, 'P136': 2, 'P2747': 1}\n",
      "-> paths_keywords: (['network', 'lost', 'air'], {'stated in': [stated in, ['P248']], 'number of losses': [number of losses, ['P1356']], 'lost': [number of losses, ['P1356']]}, [which])\n",
      "-> Computing possible paths... (could be long)\n",
      "--> len(path_nodes): 270\n",
      "->Computing possible paths \tRunning time is 22.98s\n",
      "-> Filtering paths... (could be long)\n",
      "--> len(paths_nodes_filtered): 246\n",
      "->\tRunning time is 3.17s\n",
      "-> Computing hypothesises...\n",
      "--> hypothesises: [['Q30', 3.5060333269722403], ['Q151885', 1.3655080899200562], ['Q7102610', 1.3186948899293747], ['Q79529', 1.3165934499884895], ['Q1035238', 1.2442912464200708], ['Q5633421', 1.143299446773693], ['Q11424', 1.1286286836948984], ['Q169336', 1.104148050810688], ['Q1276307', 1.0189093965776284], ['Q19184926', 0.998569453623279], ['Q611050', 0.9054137215418372], ['Q19798642', 0.7486703459630526], ['Q53737447', 0.740573431897714], ['Q1969448', 0.7324765178323754], ['Q3464665', 0.6722495805457316], ['Q1860', 0.6510440917183327], ['Q482994', 0.6470531512493383], ['Q223446', 0.49656110143828436], ['2000-01-01T00:00:00Z', 0.47704698557481795], ['14', 0.473706869531026], ['2004-01-01T00:00:00Z', 0.38907447809859497], ['11213', 0.23122901182111755]]\n",
      "->Computing hypothesises \tRunning time is 75.84s\n",
      "-> Computing golden paths...\n",
      "--> len(golden_paths): 0\n",
      "->\tRunning time is 14.96s\n",
      "--> len(cleared_golden_paths): 0\n",
      "->\tTotal Running time is 244.18s\n",
      "\n",
      "                                      question   answer qanswer platypus  \\\n",
      "332  On which network did Lost originally air?  Q169889   False    False   \n",
      "\n",
      "      convex                                                 tm  tm_time  \\\n",
      "332  Q169889  [Q30, Q151885, Q7102610, Q79529, Q1035238, Q56...   244.39   \n",
      "\n",
      "    tm_top2 tm_top3 tm_top4 tm_top5 tm_topall  \n",
      "332   False   False   False   False     False  \n",
      "Saving Dataframe Done! /data/users/romain.claret/tm/mse.tm.chatbot.base/benchmark_pickles/convex/benchmarking-qanswer-platypus-convex-qagraph-from-0-to-333.pickle.bz2\n",
      "\n",
      "\n",
      "\t>>> Processing 333/2240: (Q268181) Who is the author of the book that this movie was adapted from?                                  Asking Platypus\n",
      "Asking Convex\n",
      "Asking GraphQA\n",
      "User input: Who is the author of the book that this movie was adapted from?\n",
      "--> Auto correcting question in progress...\n",
      "-> Auto corrected q_nlp: Who is the author of the book that this movie was adapted from \n",
      "-> q_themes: ([(the author, ['Q21451533', 'Q51159453']), (the book, ['Q3794440', 'Q7719152']), (The Book, ['Q15865293', 'Q11250715']), (author, ['Q482980', 'P50'])], [book movie])\n",
      "-> q_themes_enhanced: [('movie', ['Q11424']), ('Movie', ['Q2512663']), ('Author', ['P50'])]\n",
      "--> Calculating predicates... (could be long.. depends on uncached unpure predicates)\n",
      "behold: get_most_similar started with: book\n",
      "behold: get_most_similar started with: movie\n",
      "-> q_predicates: [(be, ['P31']), (adapted, ['P5202']), (author, ['P50']), (book, ['P50']), (movie, ['P57'])]\n",
      "-> q_predicates \tRunning time is 10.55s\n",
      "-> q_focused_parts: [(the author, ['Q21451533', 'Q51159453']), (book, ['Q571', 'Q4942925', 'Q997698']), (movie, ['Q11424'])]\n",
      "-> Building the graph with k_deep 3 ... (could be long)\n",
      "->New graph \tRunning time is 17.37s\n",
      "-->  69 nodes and 70 edges\n",
      "--> Removing meaningless subgraphs\n",
      "--> New graph of: 38 nodes and 36 edges\n",
      "---> Rebuilding the graph with k_deep 5 ... Previously: 38 nodes or 36 edges was below the limit of 100\n",
      "->New graph \tRunning time is 17.25s\n",
      "-->  73 nodes and 74 edges\n",
      "--> Removing meaningless subgraphs\n",
      "--> New graph of: 42 nodes and 40 edges\n",
      "---> Rebuilding the graph with k_deep 7 ... Previously: 42 nodes or 40 edges was below the limit of 100\n",
      "->New graph \tRunning time is 17.29s\n",
      "-->  75 nodes and 76 edges\n",
      "--> Removing meaningless subgraphs\n",
      "--> New graph of: 44 nodes and 42 edges\n",
      "---> Rebuilding the graph with k_deep 9 ... Previously: 44 nodes or 42 edges was below the limit of 100\n",
      "->New graph \tRunning time is 17.26s\n",
      "-->  79 nodes and 80 edges\n",
      "--> Removing meaningless subgraphs\n",
      "--> New graph of: 48 nodes and 46 edges\n",
      "---> Rebuilding the graph with k_deep 11 ... Previously: 48 nodes or 46 edges was below the limit of 100\n",
      "->New graph \tRunning time is 16.45s\n",
      "-->  81 nodes and 82 edges\n",
      "--> Removing meaningless subgraphs\n",
      "--> New graph of: 50 nodes and 48 edges\n",
      "---> Rebuilding the graph with k_deep 13 ... Previously: 50 nodes or 48 edges was below the limit of 100\n",
      "->New graph \tRunning time is 16.25s\n",
      "-->  81 nodes and 82 edges\n",
      "--> Removing meaningless subgraphs\n",
      "--> New graph of: 50 nodes and 48 edges\n",
      "---> Loop detected, returning the graph in the current state\n",
      "-> predicates_dict: {'P31': 5, 'P50': 2, 'P39': 2, 'P155': 2, 'P156': 2, 'P136': 3, 'P793': 1, 'P518': 1, 'P364': 1, 'P407': 2, 'P582': 1, 'P1308': 3, 'P580': 1, 'P577': 3, 'P495': 2, 'P1104': 1, 'P1545': 2, 'P179': 1, 'P110': 1, 'P462': 1, 'P57': 1, 'P4908': 1, 'P272': 1, 'P2061': 1}\n",
      "-> paths_keywords: (['the author', 'book', 'movie', 'author', 'adapted', 'the book', 'book movie', 'book graph', 'film'], {'instance of': [instance of, ['P31']], 'adapted by': [adapted by, ['P5202']], 'author': [author, ['P50']], 'director': [director, ['P57']], 'Author': [author, ['P50']]}, [Who, that])\n",
      "-> Computing possible paths... (could be long)\n",
      "--> len(path_nodes): 114\n",
      "->Computing possible paths \tRunning time is 20.59s\n",
      "-> Filtering paths... (could be long)\n",
      "--> len(paths_nodes_filtered): 80\n",
      "->\tRunning time is 2.96s\n",
      "-> Computing hypothesises...\n",
      "--> hypothesises: [['Q169566', 21.157645078392537], ['Q906814', 20.944049439919873], ['Q15864910', 20.79634992953991]]\n",
      "->Computing hypothesises \tRunning time is 27.87s\n",
      "-> Computing golden paths...\n",
      "--> len(golden_paths): 6\n",
      "->\tRunning time is 3.9s\n",
      "--> len(cleared_golden_paths): 3\n",
      "---> First path: ['Q169566', 'P50', 'Q3794440', 'P407', 'Q1860']\n",
      "->\tTotal Running time is 170.9s\n",
      "\n",
      "                                              question   answer qanswer  \\\n",
      "333  Who is the author of the book that this movie ...  Q268181   False   \n",
      "\n",
      "    platypus   convex                             tm  tm_time tm_top2 tm_top3  \\\n",
      "333    False  Q590870  [Q169566, Q906814, Q15864910]   171.12   False   False   \n",
      "\n",
      "    tm_top4 tm_top5 tm_topall  \n",
      "333   False   False     False  \n",
      "Saving Dataframe Done! /data/users/romain.claret/tm/mse.tm.chatbot.base/benchmark_pickles/convex/benchmarking-qanswer-platypus-convex-qagraph-from-0-to-334.pickle.bz2\n",
      "\n",
      "\n",
      "\t>>> Processing 334/2240: (Q51511) Who directed Night of the Living Dead?                                  Asking Platypus\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Asking Convex\n",
      "\n",
      " 334 -> Convex Q51511\n",
      "Asking GraphQA\n",
      "User input: Who directed Night of the Living Dead?\n",
      "--> Auto correcting question in progress...\n",
      "-> Auto corrected q_nlp: Who directed Night of the Living Dead \n",
      "-> q_themes: ([(Night of the Living Dead, ['Q623051']), (Night, ['Q575', 'Q592503']), (the Living Dead, ['Q7747842', 'Q27877445']), (Night of the Living, ['Q545417', 'Q2652860']), (Living Dead, ['Q3461339', 'Q7747841']), (the living, ['Q20504012', 'Q7747823'])], [directed Night])\n",
      "-> q_themes_enhanced: [('direct', ['Q15304504']), ('night', ['Q575']), ('Direct', ['Q1187369'])]\n",
      "--> Calculating predicates... (could be long.. depends on uncached unpure predicates)\n",
      "-> q_predicates: [(directed, ['P57'])]\n",
      "-> q_predicates \tRunning time is 9.01s\n",
      "-> q_focused_parts: [(Night, ['Q575', 'Q592503']), (Dead, ['Q1180781', 'Q15874429', 'Q2410746', 'Q504434']), (the living, ['Q20504012', 'Q7747823'])]\n",
      "-> Building the graph with k_deep 3 ... (could be long)\n",
      "->New graph \tRunning time is 30.58s\n",
      "-->  109 nodes and 104 edges\n",
      "--> Removing meaningless subgraphs\n",
      "--> New graph of: 61 nodes and 60 edges\n",
      "---> Rebuilding the graph with k_deep 5 ... Previously: 61 nodes or 60 edges was below the limit of 100\n",
      "->New graph \tRunning time is 32.13s\n",
      "-->  141 nodes and 142 edges\n",
      "--> Removing meaningless subgraphs\n",
      "--> New graph of: 103 nodes and 108 edges\n",
      "-> predicates_dict: {'P57': 5, 'P31': 11, 'P136': 9, 'P1040': 1, 'P155': 5, 'P156': 11, 'P344': 1, 'P58': 2, 'P1431': 1, 'P910': 2, 'P364': 1, 'P407': 2, 'P1113': 1, 'P1476': 1, 'P361': 1, 'P279': 5, 'P1001': 1, 'P3893': 1, 'P50': 1, 'P585': 1, 'P166': 1, 'P921': 1, 'P1545': 1, 'P4908': 1, 'P179': 1, 'P4224': 1, 'P17': 1, 'P750': 1}\n",
      "-> paths_keywords: (['night', 'dead', 'the living', 'night of the living dead', 'living dead', 'the dead', 'per dead ohlin'], {'director': [director, ['P57']]}, [Who])\n",
      "-> Computing possible paths... (could be long)\n",
      "--> len(path_nodes): 714\n",
      "->Computing possible paths \tRunning time is 12.04s\n",
      "-> Filtering paths... (could be long)\n",
      "--> len(paths_nodes_filtered): 668\n",
      "->\tRunning time is 3.09s\n",
      "-> Computing hypothesises...\n",
      "--> hypothesises: [['Q51511', 36.32611327123271], ['Q152309', 35.62810958972185], ['Q2460466', 6.275428094730671], ['Q27908843', 1.4286114153713816]]\n",
      "->Computing hypothesises \tRunning time is 58.33s\n",
      "-> Computing golden paths...\n",
      "--> len(golden_paths): 19\n",
      "->\tRunning time is 9.73s\n",
      "--> len(cleared_golden_paths): 11\n",
      "---> First path: ['Q51511', 'P57', 'Q623051', 'P1040', 'Q2460466', 'P58', 'Q545417', 'P136', 'Q3072049']\n",
      "->\tTotal Running time is 157.87s\n",
      "\n",
      "                                   question  answer qanswer platypus  convex  \\\n",
      "334  Who directed Night of the Living Dead?  Q51511   False    False  Q51511   \n",
      "\n",
      "                                         tm  tm_time tm_top2 tm_top3 tm_top4  \\\n",
      "334  [Q51511, Q152309, Q2460466, Q27908843]   158.08   False   False   False   \n",
      "\n",
      "    tm_top5 tm_topall  \n",
      "334   False     False  \n",
      "Saving Dataframe Done! /data/users/romain.claret/tm/mse.tm.chatbot.base/benchmark_pickles/convex/benchmarking-qanswer-platypus-convex-qagraph-from-0-to-335.pickle.bz2\n",
      "\n",
      "\n",
      "\t>>> Processing 335/2240: (Q201330) What position does soccer player Tim Howard play?                                  Asking Platypus\n",
      "Asking Convex\n",
      "Asking GraphQA\n",
      "User input: What position does soccer player Tim Howard play?\n",
      "--> Auto correcting question in progress...\n",
      "-> Auto corrected q_nlp: What position does soccer player Tim Howard play \n",
      "-> q_themes: ([(Tim Howard, ['Q200785', 'Q7803679']), (Position, ['Q4164871', 'Q24834761']), (position, ['Q192388', 'Q1781513'])], [soccer player Tim Howard, What position does soccer player Tim, Soccer Player Tim Howard, soccer player tim howard])\n",
      "-> q_themes_enhanced: [('soccer player', ['Q937857']), ('soccer', ['Q2736']), ('player', ['Q4197743']), ('tim', ['Q29727711']), ('Soccer', ['Q2338283']), ('Player', ['Q1551573'])]\n",
      "--> Calculating predicates... (could be long.. depends on uncached unpure predicates)\n",
      "behold: get_most_similar started with: do\n",
      "behold: get_most_similar started with: soccer\n",
      "behold: get_most_similar started with: player\n",
      "behold: get_most_similar started with: play\n",
      "-> q_predicates: [(does, []), (play, ['P741']), (position, ['P625']), (soccer, ['P2685']), (player, ['P1873', 'P1872'])]\n",
      "-> q_predicates \tRunning time is 71.52s\n",
      "-> q_focused_parts: [(position, ['Q192388', 'Q1781513'])]\n",
      "-> Building the graph with k_deep 3 ... (could be long)\n",
      "->New graph \tRunning time is 29.5s\n",
      "-->  191 nodes and 192 edges\n",
      "--> Removing meaningless subgraphs\n",
      "--> New graph of: 191 nodes and 192 edges\n",
      "-> predicates_dict: {'P31': 10, 'P279': 5, 'P106': 3, 'P404': 2, 'P413': 1, 'P361': 1, 'P425': 1, 'P641': 2, 'P136': 1, 'P155': 1, 'P1480': 1, 'P642': 1, 'P373': 1, 'P2453': 3, 'P805': 1, 'P1411': 3, 'P1350': 6, 'P1351': 1, 'P1642': 1, 'P580': 7, 'P582': 4, 'P54': 8, 'P1269': 1, 'P585': 2, 'P1545': 2, 'P735': 3, 'P1449': 1, 'P364': 1, 'P1552': 1, 'P958': 1, 'P1343': 1, 'P840': 1, 'P19': 1, 'P27': 2, 'P69': 2, 'P2548': 1, 'P121': 1, 'P400': 1, 'P21': 1, 'P495': 1, 'P734': 1, 'P178': 1, 'P1057': 1, 'P644': 1, 'P702': 1, 'P688': 1, 'P645': 1, 'P4196': 1}\n",
      "-> paths_keywords: (['position', 'tim howard'], {'playing hand': [playing hand, ['P741']], 'coordinate location': [coordinate location, ['P625']], 'Basketball-Reference.com NBA player ID': [Basketball Reference.com NBA player ID, ['P2685']], 'maximum number of players': [maximum number of players, ['P1873']], 'minimum number of players': [minimum number of players, ['P1872']]}, [What])\n",
      "-> Computing possible paths... (could be long)\n",
      "--> len(path_nodes): 132\n",
      "->Computing possible paths \tRunning time is 11.91s\n",
      "-> Filtering paths... (could be long)\n",
      "--> len(paths_nodes_filtered): 130\n",
      "->\tRunning time is 3.25s\n",
      "-> Computing hypothesises...\n",
      "--> hypothesises: [['25', 0.4625109280830788], ['3', 0.3996735216127079], ['45', 0.39928230163137], ['16', 0.39176221072055556], ['4', 0.37341145046726654], ['1', 0.35098315431431754], ['Q201330', 0.2955605341842084]]\n",
      "->Computing hypothesises \tRunning time is 28.16s\n",
      "-> Computing golden paths...\n",
      "--> len(golden_paths): 5\n",
      "->\tRunning time is 5.0s\n",
      "--> len(cleared_golden_paths): 2\n",
      "---> First path: ['25', 'P1350', 'Q200785', 'P413', 'Q201330']\n",
      "->\tTotal Running time is 152.3s\n",
      "\n",
      "                                              question   answer qanswer  \\\n",
      "335  What position does soccer player Tim Howard play?  Q201330   False   \n",
      "\n",
      "    platypus convex                              tm  tm_time tm_top2 tm_top3  \\\n",
      "335    False  False  [25, 3, 45, 16, 4, 1, Q201330]   152.52   False   False   \n",
      "\n",
      "    tm_top4 tm_top5 tm_topall  \n",
      "335   False   False     False  \n",
      "Saving Dataframe Done! /data/users/romain.claret/tm/mse.tm.chatbot.base/benchmark_pickles/convex/benchmarking-qanswer-platypus-convex-qagraph-from-0-to-336.pickle.bz2\n",
      "\n",
      "\n",
      "\t>>> Processing 336/2240: (Q13417189) Who directed Interstellar?                                  Asking Platypus\n",
      "Asking Convex\n",
      "Asking GraphQA\n",
      "User input: Who directed Interstellar?\n",
      "--> Auto correcting question in progress...\n",
      "-> Auto corrected q_nlp: Who directed Interstellar \n",
      "-> q_themes: ([(Interstellar, ['Q13417189', 'Q3153615'])], [directed Interstellar, direct Interstellar])\n",
      "-> q_themes_enhanced: [('direct', ['Q15304504']), ('Direct', ['Q1187369'])]\n",
      "--> Calculating predicates... (could be long.. depends on uncached unpure predicates)\n",
      "-> q_predicates: [(directed, ['P57'])]\n",
      "-> q_predicates \tRunning time is 4.67s\n",
      "-> q_focused_parts: [(Interstellar, ['Q13417189', 'Q3153615'])]\n",
      "-> Building the graph with k_deep 3 ... (could be long)\n",
      "->New graph \tRunning time is 19.11s\n",
      "-->  48 nodes and 46 edges\n",
      "--> Removing meaningless subgraphs\n",
      "--> New graph of: 41 nodes and 40 edges\n",
      "---> Rebuilding the graph with k_deep 5 ... Previously: 41 nodes or 40 edges was below the limit of 100\n",
      "->New graph \tRunning time is 20.39s\n",
      "-->  70 nodes and 70 edges\n",
      "--> Removing meaningless subgraphs\n",
      "--> New graph of: 61 nodes and 62 edges\n",
      "---> Rebuilding the graph with k_deep 7 ... Previously: 61 nodes or 62 edges was below the limit of 100\n",
      "->New graph \tRunning time is 20.54s\n",
      "-->  83 nodes and 88 edges\n",
      "--> Removing meaningless subgraphs\n",
      "--> New graph of: 74 nodes and 80 edges\n",
      "---> Rebuilding the graph with k_deep 9 ... Previously: 74 nodes or 80 edges was below the limit of 100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "->New graph \tRunning time is 20.3s\n",
      "-->  89 nodes and 94 edges\n",
      "--> Removing meaningless subgraphs\n",
      "--> New graph of: 76 nodes and 82 edges\n",
      "---> Rebuilding the graph with k_deep 10 ... Previously: 76 nodes or 82 edges was below the limit of 100\n",
      "->New graph \tRunning time is 20.78s\n",
      "-->  89 nodes and 94 edges\n",
      "--> Removing meaningless subgraphs\n",
      "--> New graph of: 76 nodes and 82 edges\n",
      "---> Loop detected, returning the graph in the current state\n",
      "-> predicates_dict: {'P57': 1, 'P31': 2, 'P3831': 1, 'P3092': 1, 'P1040': 1, 'P136': 3, 'P156': 2, 'P344': 1, 'P2363': 1, 'P155': 2, 'P2453': 11, 'P805': 1, 'P1411': 5, 'P1346': 4, 'P166': 1, 'P279': 4, 'P291': 2, 'P577': 2, 'P1995': 2}\n",
      "-> paths_keywords: (['interstellar'], {'director': [director, ['P57']]}, [Who])\n",
      "-> Computing possible paths... (could be long)\n",
      "--> len(path_nodes): 6\n",
      "->Computing possible paths \tRunning time is 19.65s\n",
      "-> Filtering paths... (could be long)\n",
      "--> len(paths_nodes_filtered): 4\n",
      "->\tRunning time is 3.17s\n",
      "-> Computing hypothesises...\n",
      "--> hypothesises: [['Q25191', 13.629145394013529], ['Q514003', 2.684197732585129]]\n",
      "->Computing hypothesises \tRunning time is 1.44s\n",
      "-> Computing golden paths...\n",
      "--> len(golden_paths): 3\n",
      "->\tRunning time is 3.2s\n",
      "--> len(cleared_golden_paths): 1\n",
      "---> First path: ['Q25191', 'P57', 'Q13417189', 'P344', 'Q514003']\n",
      "->\tTotal Running time is 135.94s\n",
      "\n",
      "                       question     answer qanswer platypus  convex  \\\n",
      "336  Who directed Interstellar?  Q13417189   False    False  Q25191   \n",
      "\n",
      "                    tm  tm_time tm_top2 tm_top3 tm_top4 tm_top5 tm_topall  \n",
      "336  [Q25191, Q514003]   136.15   False   False   False   False     False  \n",
      "Saving Dataframe Done! /data/users/romain.claret/tm/mse.tm.chatbot.base/benchmark_pickles/convex/benchmarking-qanswer-platypus-convex-qagraph-from-0-to-337.pickle.bz2\n",
      "\n",
      "\n",
      "\t>>> Processing 337/2240: (1989-07-05T00:00:00Z) When did the show Seinfeld come out?                                  Asking Platypus\n",
      "Asking Convex\n",
      "Asking GraphQA\n",
      "User input: When did the show Seinfeld come out?\n",
      "--> Auto correcting question in progress...\n",
      "-> Auto corrected q_nlp: When did the show Seinfeld come out \n",
      "-> q_themes: ([(Seinfeld, ['Q23733', 'Q36978598']), (the show, ['Q18527399', 'Q1524084']), (The Show, ['Q11249057', 'Q10852882'])], [When did the show Seinfeld])\n",
      "-> q_themes_enhanced: [('The Do', ['Q7730457'])]\n",
      "--> Calculating predicates... (could be long.. depends on uncached unpure predicates)\n",
      "behold: get_most_similar started with: show\n",
      "behold: get_most_similar started with: come\n",
      "-> q_predicates: [(did, ['P248']), (come, ['P2171']), (show, [])]\n",
      "-> q_predicates \tRunning time is 36.33s\n",
      "-> q_focused_parts: [(Seinfeld, ['Q23733', 'Q36978598']), (the show, ['Q18527399', 'Q1524084'])]\n",
      "-> Building the graph with k_deep 3 ... (could be long)\n",
      "->New graph \tRunning time is 26.77s\n",
      "-->  74 nodes and 70 edges\n",
      "--> Removing meaningless subgraphs\n",
      "--> New graph of: 74 nodes and 70 edges\n",
      "---> Rebuilding the graph with k_deep 5 ... Previously: 74 nodes or 70 edges was below the limit of 100\n",
      "->New graph \tRunning time is 26.46s\n",
      "-->  120 nodes and 120 edges\n",
      "--> Removing meaningless subgraphs\n",
      "--> New graph of: 120 nodes and 120 edges\n",
      "-> predicates_dict: {'P580': 2, 'P582': 1, 'P577': 3, 'P463': 1, 'P155': 2, 'P156': 2, 'P1441': 3, 'P1013': 1, 'P1011': 1, 'P407': 1, 'P1552': 1, 'P1545': 6, 'P527': 6, 'P518': 1, 'P364': 1, 'P1113': 1, 'P1811': 1, 'P449': 1, 'P2437': 1, 'P734': 3, 'P31': 4, 'P495': 2, 'P361': 1, 'P453': 3, 'P161': 3, 'P3744': 2, 'P2002': 2, 'P175': 1, 'P264': 1, 'P282': 1, 'P1705': 1}\n",
      "-> paths_keywords: (['seinfeld', 'the show', 'come'], {'stated in': [stated in, ['P248']], 'They Work for You ID': [They Work for You ID, ['P2171']]}, [When])\n",
      "-> Computing possible paths... (could be long)\n",
      "--> len(path_nodes): 294\n",
      "->Computing possible paths \tRunning time is 11.05s\n",
      "-> Filtering paths... (could be long)\n",
      "--> len(paths_nodes_filtered): 258\n",
      "->\tRunning time is 2.97s\n",
      "-> Computing hypothesises...\n",
      "--> hypothesises: [['1998-05-14T00:00:00Z', 5.5894572771449855], ['1989-07-05T00:00:00Z', 5.4284398917864305], ['1927-01-01T00:00:00Z', 4.86628118665534], ['Q30', 0.7515190092726352], ['Q2777013', 0.1113358841897013], ['Q2916792', -0.085331665025063], ['Q20426643', -0.10537978116869777]]\n",
      "->Computing hypothesises \tRunning time is 51.13s\n",
      "-> Computing golden paths...\n",
      "--> len(golden_paths): 15\n",
      "->\tRunning time is 4.82s\n",
      "--> len(cleared_golden_paths): 7\n",
      "---> First path: ['1998-05-14T00:00:00Z', 'P582', 'Q23733', 'P580', '1989-07-05T00:00:00Z']\n",
      "->\tTotal Running time is 162.27s\n",
      "\n",
      "                                 question                answer qanswer  \\\n",
      "337  When did the show Seinfeld come out?  1989-07-05T00:00:00Z   False   \n",
      "\n",
      "    platypus     convex                                                 tm  \\\n",
      "337    False  Q20426643  [1998-05-14T00:00:00Z, 1989-07-05T00:00:00Z, 1...   \n",
      "\n",
      "     tm_time tm_top2 tm_top3 tm_top4 tm_top5 tm_topall  \n",
      "337   162.48   False   False   False   False     False  \n",
      "Saving Dataframe Done! /data/users/romain.claret/tm/mse.tm.chatbot.base/benchmark_pickles/convex/benchmarking-qanswer-platypus-convex-qagraph-from-0-to-338.pickle.bz2\n",
      "\n",
      "\n",
      "\t>>> Processing 338/2240: (Q171048) What was the first Pixar movie?                                  Asking Platypus\n",
      "Asking Convex\n",
      "Asking GraphQA\n",
      "User input: What was the first Pixar movie?\n",
      "--> Auto correcting question in progress...\n",
      "-> Auto corrected q_nlp: What was the first Pixar movie \n",
      "-> q_themes: ([(first, ['Q19269277']), (Pixar, ['Q127552', 'Q3389458']), (Movie, ['Q43263082', 'Q2512663'])], [the first Pixar movie, was the first Pixar, The First Pixar Movie, the first pixar movie, first Pixar movie])\n",
      "-> q_themes_enhanced: [('movie', ['Q11424']), ('The First', ['Q18689797']), ('The Movie', ['Q7752539'])]\n",
      "--> Calculating predicates... (could be long.. depends on uncached unpure predicates)\n",
      "behold: get_most_similar started with: movie\n",
      "-> q_predicates: [(be, ['P31']), (first, ['P577']), (movie, ['P57'])]\n",
      "-> q_predicates \tRunning time is 8.9s\n",
      "-> q_focused_parts: [(movie, ['Q11424']), (Pixar, ['Q127552', 'Q3389458'])]\n",
      "-> Building the graph with k_deep 3 ... (could be long)\n",
      "->New graph \tRunning time is 19.38s\n",
      "-->  70 nodes and 68 edges\n",
      "--> Removing meaningless subgraphs\n",
      "--> New graph of: 70 nodes and 68 edges\n",
      "---> Rebuilding the graph with k_deep 5 ... Previously: 70 nodes or 68 edges was below the limit of 100\n",
      "->New graph \tRunning time is 19.4s\n",
      "-->  78 nodes and 76 edges\n",
      "--> Removing meaningless subgraphs\n",
      "--> New graph of: 78 nodes and 76 edges\n",
      "---> Rebuilding the graph with k_deep 6 ... Previously: 78 nodes or 76 edges was below the limit of 100\n",
      "->New graph \tRunning time is 19.27s\n",
      "-->  82 nodes and 80 edges\n",
      "--> Removing meaningless subgraphs\n",
      "--> New graph of: 82 nodes and 80 edges\n",
      "---> Rebuilding the graph with k_deep 7 ... Previously: 82 nodes or 80 edges was below the limit of 100\n",
      "->New graph \tRunning time is 20.12s\n",
      "-->  92 nodes and 92 edges\n",
      "--> Removing meaningless subgraphs\n",
      "--> New graph of: 92 nodes and 92 edges\n",
      "---> Rebuilding the graph with k_deep 8 ... Previously: 92 nodes or 92 edges was below the limit of 100\n",
      "->New graph \tRunning time is 21.02s\n",
      "-->  94 nodes and 94 edges\n",
      "--> Removing meaningless subgraphs\n",
      "--> New graph of: 94 nodes and 94 edges\n",
      "---> Rebuilding the graph with k_deep 9 ... Previously: 94 nodes or 94 edges was below the limit of 100\n",
      "->New graph \tRunning time is 20.71s\n",
      "-->  100 nodes and 100 edges\n",
      "--> Removing meaningless subgraphs\n",
      "--> New graph of: 100 nodes and 100 edges\n",
      "---> Rebuilding the graph with k_deep 10 ... Previously: 100 nodes or 100 edges was below the limit of 100\n",
      "->New graph \tRunning time is 19.43s\n",
      "-->  104 nodes and 104 edges\n",
      "--> Removing meaningless subgraphs\n",
      "--> New graph of: 104 nodes and 104 edges\n",
      "-> predicates_dict: {'P1557': 1, 'P138': 6, 'P175': 1, 'P31': 8, 'P1476': 1, 'P407': 1, 'P856': 1, 'P571': 1, 'P131': 1, 'P793': 1, 'P580': 1, 'P127': 1, 'P364': 1, 'P1114': 1, 'P749': 1, 'P1552': 3, 'P495': 2, 'P969': 1, 'P642': 1, 'P452': 1, 'P279': 2, 'P161': 4, 'P17': 2, 'P57': 1, 'P58': 1, 'P2096': 2, 'P625': 1, 'P159': 1, 'P1545': 1, 'P170': 1, 'P1423': 1}\n",
      "-> paths_keywords: (['movie', 'pixar', 'first', 'film'], {'instance of': [instance of, ['P31']], 'date of publication': [date of publication, ['P577']], 'director': [director, ['P57']]}, [What])\n",
      "-> Computing possible paths... (could be long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--> len(path_nodes): 242\n",
      "->Computing possible paths \tRunning time is 10.31s\n",
      "-> Filtering paths... (could be long)\n",
      "--> len(paths_nodes_filtered): 206\n",
      "->\tRunning time is 2.98s\n",
      "-> Computing hypothesises...\n",
      "--> hypothesises: [['Q1284058', 7.798615232260388], ['Q7752539', 3.614328159225882], ['Q46047837', 1.1471439007813211], ['Q19860854', 1.0573948886623195], ['Q272404', 0.825740027230818], ['Q1762059', 0.804846999627669], ['Q41253', 0.7522725649841601], ['Q4830453', 0.6134759428127], ['Q1860', 0.5971230160874398], ['Q1107679', 0.5890795085219132]]\n",
      "->Computing hypothesises \tRunning time is 43.6s\n",
      "-> Computing golden paths...\n",
      "--> len(golden_paths): 28\n",
      "->\tRunning time is 10.97s\n",
      "--> len(cleared_golden_paths): 16\n",
      "---> First path: ['Q1284058', 'P57', 'Q7752539', 'P364', 'Q1860', 'P407', 'Q127552', 'P31', 'Q1762059']\n",
      "->\tTotal Running time is 218.77s\n",
      "\n",
      "                            question   answer qanswer platypus convex  \\\n",
      "338  What was the first Pixar movie?  Q171048   False  Q387066  False   \n",
      "\n",
      "                                                    tm  tm_time tm_top2  \\\n",
      "338  [Q1284058, Q7752539, Q46047837, Q19860854, Q27...    219.0   False   \n",
      "\n",
      "    tm_top3 tm_top4 tm_top5 tm_topall  \n",
      "338   False   False   False     False  \n",
      "Saving Dataframe Done! /data/users/romain.claret/tm/mse.tm.chatbot.base/benchmark_pickles/convex/benchmarking-qanswer-platypus-convex-qagraph-from-0-to-339.pickle.bz2\n",
      "\n",
      "\n",
      "\t>>> Processing 339/2240: (Q483810) What is the creation date of The Cranberries?                                  Asking Platypus\n",
      "Asking Convex\n",
      "Asking GraphQA\n",
      "User input: What is the creation date of The Cranberries?\n",
      "--> Auto correcting question in progress...\n",
      "-> Auto corrected q_nlp: What is the creation date of The Cranberries \n",
      "-> q_themes: ([(The Cranberries, ['Q483810']), (Cranberries, ['Q5181883']), (the cranberry, ['Q51460792'])], [the creation date, is the creation date of The, The Creation Date])\n",
      "-> q_themes_enhanced: [('creation date', ['P571']), ('creation', ['Q11398090']), ('date', ['Q1652093']), ('The Creation', ['Q1456447']), ('Creation', ['Q17001317']), ('Date', ['Q36603893'])]\n",
      "--> Calculating predicates... (could be long.. depends on uncached unpure predicates)\n",
      "behold: get_most_similar started with: cranberry\n",
      "-> q_predicates: [(be, ['P31']), (creation, ['P571']), (date, ['P837']), (Cranberries, [])]\n",
      "-> q_predicates \tRunning time is 39.54s\n",
      "-> q_focused_parts: [(date, ['Q1652093', 'Q3016931', 'P837']), (creation, ['Q386724', 'Q11398090', 'Q16686448', 'Q20004056']), (The Cranberries, ['Q483810'])]\n",
      "-> Building the graph with k_deep 3 ... (could be long)\n",
      "->New graph \tRunning time is 19.91s\n",
      "-->  136 nodes and 132 edges\n",
      "--> Removing meaningless subgraphs\n",
      "--> New graph of: 115 nodes and 112 edges\n",
      "-> predicates_dict: {'P571': 2, 'P101': 1, 'P1672': 1, 'P180': 1, 'P186': 1, 'P428': 1, 'P156': 1, 'P175': 3, 'P373': 1, 'P3712': 1, 'P407': 2, 'P443': 1, 'P3245': 1, 'P3250': 2, 'P577': 2, 'P1686': 2, 'P585': 2, 'P166': 2, 'P31': 5, 'P642': 2, 'P279': 6, 'P1013': 3, 'P1932': 2, 'P291': 1, 'P1104': 1, 'P1476': 1, 'P527': 3, 'P1557': 1, 'P264': 2, 'P155': 1, 'P2031': 1, 'P136': 2, 'P856': 1, 'P3744': 1, 'P2002': 1, 'P2093': 1, 'P576': 1, 'P366': 1, 'P1582': 1, 'P828': 1}\n",
      "-> paths_keywords: (['date', 'creation', 'the cranberries', 'cranberries', 'the cranberry', 'creative work', 'artificial entity'], {'instance of': [instance of, ['P31']], 'creation date': [date of foundation or creation, ['P571']], 'day in year for periodic occurrence': [day in date for periodic occurrence, ['P837']], 'date of foundation or creation': [date of foundation or creation, ['P571']]}, [What])\n",
      "-> Computing possible paths... (could be long)\n",
      "--> len(path_nodes): 254\n",
      "->Computing possible paths \tRunning time is 22.83s\n",
      "-> Filtering paths... (could be long)\n",
      "--> len(paths_nodes_filtered): 250\n",
      "->\tRunning time is 3.28s\n",
      "-> Computing hypothesises...\n",
      "--> hypothesises: [['1966-01-01T00:00:00Z', 1.501047974649864], ['Q629995', 1.2654202836852309], ['Q1456447', 1.117964383139345], ['1989-01-01T00:00:00Z', 0.9899883771822144], ['Q5741069', 0.8803490006540103], ['Q36603893', 0.7441909730929263], ['Q5', 0.7441909730929263], ['Q215380', 0.734880791628826], ['Q169298', 0.6274282359696215], ['Q836575', 0.46017243515584283], ['Q190', 0.40273034063922564], ['Q652', 0.4026569063708615], ['Q482994', 0.38626948218713075], ['1995-01-01T00:00:00Z', 0.3780762987831233], ['01.22.13.000', 0.37354784237839256], ['1996-01-01T00:00:00Z', 0.3715148946821725], ['1990-01-01T00:00:00Z', 0.35583629308922177], ['Q25292', 0.31327095240398106], ['2014-09-01T00:00:00Z', 0.3105036511617983], ['Q27685', 0.28667809793234655], ['01.13.21.111', 0.22562056216273224]]\n",
      "->Computing hypothesises \tRunning time is 129.29s\n",
      "-> Computing golden paths...\n",
      "--> len(golden_paths): 5\n",
      "->\tRunning time is 13.22s\n",
      "--> len(cleared_golden_paths): 2\n",
      "---> First path: ['1966-01-01T00:00:00Z', 'P571', 'Q1456447', 'P31', 'Q215380']\n",
      "->\tTotal Running time is 231.15s\n",
      "\n",
      "                                          question   answer qanswer platypus  \\\n",
      "339  What is the creation date of The Cranberries?  Q483810   False    False   \n",
      "\n",
      "    convex                                                 tm  tm_time  \\\n",
      "339  False  [1966-01-01T00:00:00Z, Q629995, Q1456447, 1989...   231.36   \n",
      "\n",
      "    tm_top2 tm_top3 tm_top4 tm_top5 tm_topall  \n",
      "339   False   False   False   False     False  \n",
      "Saving Dataframe Done! /data/users/romain.claret/tm/mse.tm.chatbot.base/benchmark_pickles/convex/benchmarking-qanswer-platypus-convex-qagraph-from-0-to-340.pickle.bz2\n",
      "\n",
      "\n",
      "\t>>> Processing 340/2240: (Q19927199) What is Eminem's actual name?                                  Asking Platypus\n",
      "Asking Convex\n",
      "Asking GraphQA\n",
      "User input: What is Eminem's actual name?\n",
      "--> Auto correcting question in progress...\n",
      "-> Auto corrected q_nlp: What is Eminem actual name \n",
      "-> q_themes: ([(Eminem, ['Q5608']), (name, ['Q82799', 'Q503992']), (Name, ['Q1627497', 'Q11236330'])], [Eminem actual name, Eminem Name, is Eminem, Actual Name, eminem actual name])\n",
      "-> q_themes_enhanced: [('Actual', ['Q11904132'])]\n",
      "--> Calculating predicates... (could be long.. depends on uncached unpure predicates)\n",
      "-> q_predicates: [(be, ['P31']), (actual, ['P3842']), (name, ['P735', 'P1448'])]\n",
      "-> q_predicates \tRunning time is 8.08s\n",
      "-> q_focused_parts: [(Eminem, ['Q5608'])]\n",
      "-> Building the graph with k_deep 3 ... (could be long)\n",
      "->New graph \tRunning time is 49.13s\n",
      "-->  1829 nodes and 1824 edges\n",
      "--> Removing meaningless subgraphs\n",
      "--> New graph of: 1829 nodes and 1824 edges\n",
      "---> Rebuilding the graph with k_deep 2 ... Previously: 1829 nodes or 1824 edges was above the limit of 350\n",
      "---> Too many nodes, statistically it's not worth the run. Cancelling question, it probably require reasoning.\n",
      "\n",
      "                          question     answer qanswer platypus     convex  \\\n",
      "340  What is Eminem's actual name?  Q19927199   False    False  Q19819759   \n",
      "\n",
      "        tm  tm_time tm_top2 tm_top3 tm_top4 tm_top5 tm_topall  \n",
      "340  False   106.57   False   False   False   False     False  \n",
      "Saving Dataframe Done! /data/users/romain.claret/tm/mse.tm.chatbot.base/benchmark_pickles/convex/benchmarking-qanswer-platypus-convex-qagraph-from-0-to-341.pickle.bz2\n",
      "\n",
      "\n",
      "\t>>> Processing 341/2240: (Q2264282) What is the main character in the book?                                  Asking Platypus\n",
      "Asking Convex\n",
      "Asking GraphQA\n",
      "User input: What is the main character in the book?\n",
      "--> Auto correcting question in progress...\n",
      "-> Auto corrected q_nlp: What is the main character in the book \n",
      "-> q_themes: ([(the book, ['Q3794440', 'Q7719152']), (book, ['Q571', 'Q421300']), (Book, ['Q16860229', 'Q11515178']), (The Book, ['Q15865293', 'Q11250715']), (main character, ['Q50937280'])], [])\n",
      "-> q_themes_enhanced: []\n",
      "--> Calculating predicates... (could be long.. depends on uncached unpure predicates)\n",
      "behold: get_most_similar started with: book\n",
      "-> q_predicates: [(be, ['P31']), (main, ['P921', 'P301']), (character, ['P674']), (book, ['P50'])]\n",
      "-> q_predicates \tRunning time is 7.64s\n",
      "-> q_focused_parts: [(character, ['Q3241972', 'Q1792372']), (main, ['Q3278265']), (book, ['Q571', 'Q421300'])]\n",
      "-> Building the graph with k_deep 3 ... (could be long)\n",
      "->New graph \tRunning time is 10.62s\n",
      "-->  70 nodes and 66 edges\n",
      "--> Removing meaningless subgraphs\n",
      "--> New graph of: 70 nodes and 66 edges\n",
      "---> Rebuilding the graph with k_deep 5 ... Previously: 70 nodes or 66 edges was below the limit of 100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "->New graph \tRunning time is 10.51s\n",
      "-->  86 nodes and 82 edges\n",
      "--> Removing meaningless subgraphs\n",
      "--> New graph of: 86 nodes and 82 edges\n",
      "---> Rebuilding the graph with k_deep 6 ... Previously: 86 nodes or 82 edges was below the limit of 100\n",
      "->New graph \tRunning time is 10.45s\n",
      "-->  92 nodes and 88 edges\n",
      "--> Removing meaningless subgraphs\n",
      "--> New graph of: 92 nodes and 88 edges\n",
      "---> Rebuilding the graph with k_deep 7 ... Previously: 92 nodes or 88 edges was below the limit of 100\n",
      "->New graph \tRunning time is 10.5s\n",
      "-->  96 nodes and 92 edges\n",
      "--> Removing meaningless subgraphs\n",
      "--> New graph of: 96 nodes and 92 edges\n",
      "---> Rebuilding the graph with k_deep 8 ... Previously: 96 nodes or 92 edges was below the limit of 100\n",
      "->New graph \tRunning time is 10.45s\n",
      "-->  102 nodes and 98 edges\n",
      "--> Removing meaningless subgraphs\n",
      "--> New graph of: 102 nodes and 98 edges\n",
      "---> Rebuilding the graph with k_deep 9 ... Previously: 102 nodes or 98 edges was below the limit of 100\n",
      "->New graph \tRunning time is 10.44s\n",
      "-->  106 nodes and 102 edges\n",
      "--> Removing meaningless subgraphs\n",
      "--> New graph of: 106 nodes and 102 edges\n",
      "-> predicates_dict: {'P1013': 2, 'P31': 6, 'P734': 9, 'P279': 5, 'P2868': 9, 'P50': 2, 'P518': 1, 'P364': 1, 'P407': 2, 'P136': 3, 'P793': 1, 'P577': 3, 'P495': 2, 'P1104': 1, 'P282': 1, 'P462': 1, 'P272': 1, 'P373': 1}\n",
      "-> paths_keywords: (['character', 'main', 'book', 'the book', 'main character'], {}, [What])\n",
      "-> Computing possible paths... (could be long)\n",
      "--> len(path_nodes): 92\n",
      "->Computing possible paths \tRunning time is 19.66s\n",
      "-> Filtering paths... (could be long)\n",
      "--> len(paths_nodes_filtered): 80\n",
      "->\tRunning time is 3.1s\n",
      "-> Computing hypothesises...\n",
      "--> hypothesises: [['Q906814', 17.480207726859327], ['Q636497', 0.9721696211599699]]\n",
      "->Computing hypothesises \tRunning time is 8.9s\n",
      "-> Computing golden paths...\n",
      "--> len(golden_paths): 2\n",
      "->\tRunning time is 3.41s\n",
      "--> len(cleared_golden_paths): 1\n",
      "---> First path: ['Q906814', 'P50', 'Q11250715', 'P31', 'Q571']\n",
      "->\tTotal Running time is 108.52s\n",
      "\n",
      "                                    question    answer qanswer platypus  \\\n",
      "341  What is the main character in the book?  Q2264282   False    False   \n",
      "\n",
      "    convex                  tm  tm_time tm_top2 tm_top3 tm_top4 tm_top5  \\\n",
      "341  False  [Q906814, Q636497]   108.73   False   False   False   False   \n",
      "\n",
      "    tm_topall  \n",
      "341     False  \n",
      "Saving Dataframe Done! /data/users/romain.claret/tm/mse.tm.chatbot.base/benchmark_pickles/convex/benchmarking-qanswer-platypus-convex-qagraph-from-0-to-342.pickle.bz2\n",
      "\n",
      "\n",
      "\t>>> Processing 342/2240: (Q2344912) Who is the author of The Mist?                                  Asking Platypus\n",
      "Asking Convex\n",
      "Asking GraphQA\n",
      "User input: Who is the author of The Mist?\n",
      "--> Auto correcting question in progress...\n",
      "-> Auto corrected q_nlp: Who is the author of The Mist \n",
      "-> q_themes: ([(the author, ['Q21451533', 'Q51159453']), (The Mist, ['Q695209', 'Q29479900']), (mist, ['Q192196']), (Mist, ['Q16422474', 'Q12605533']), (author, ['Q482980', 'P50'])], [is the author of The])\n",
      "-> q_themes_enhanced: [('The Author', ['Q21451533']), ('The The', ['Q1354463']), ('Author', ['Q1220968', 'P50'])]\n",
      "--> Calculating predicates... (could be long.. depends on uncached unpure predicates)\n",
      "-> q_predicates: [(be, ['P31']), (author, ['P50'])]\n",
      "-> q_predicates \tRunning time is 8.23s\n",
      "-> q_focused_parts: [(the author, ['Q21451533', 'Q51159453']), (The Mist, ['Q695209', 'Q29479900'])]\n",
      "-> Building the graph with k_deep 3 ... (could be long)\n",
      "->New graph \tRunning time is 22.7s\n",
      "-->  98 nodes and 98 edges\n",
      "--> Removing meaningless subgraphs\n",
      "--> New graph of: 98 nodes and 98 edges\n",
      "---> Rebuilding the graph with k_deep 4 ... Previously: 98 nodes or 98 edges was below the limit of 100\n",
      "->New graph \tRunning time is 23.22s\n",
      "-->  100 nodes and 100 edges\n",
      "--> Removing meaningless subgraphs\n",
      "--> New graph of: 100 nodes and 100 edges\n",
      "---> Rebuilding the graph with k_deep 5 ... Previously: 100 nodes or 100 edges was below the limit of 100\n",
      "->New graph \tRunning time is 23.29s\n",
      "-->  115 nodes and 116 edges\n",
      "--> Removing meaningless subgraphs\n",
      "--> New graph of: 115 nodes and 116 edges\n",
      "-> predicates_dict: {'P39': 2, 'P155': 2, 'P156': 2, 'P1013': 1, 'P1877': 1, 'P527': 1, 'P571': 1, 'P31': 7, 'P582': 1, 'P1308': 2, 'P364': 3, 'P580': 2, 'P921': 1, 'P1476': 2, 'P175': 3, 'P136': 5, 'P1552': 1, 'P1040': 1, 'P577': 1, 'P3744': 1, 'P2002': 1, 'P58': 2, 'P495': 1, 'P910': 1, 'P57': 1, 'P282': 1, 'P144': 1, 'P800': 1, 'P373': 1, 'P112': 1, 'P1981': 1, 'P2747': 1, 'P1705': 1, 'P1545': 2, 'P179': 1, 'P4908': 1}\n",
      "-> paths_keywords: (['the author', 'the mist', 'author', 'mist'], {'instance of': [instance of, ['P31']], 'author': [author, ['P50']], 'Author': [author, ['P50']]}, [Who])\n",
      "-> Computing possible paths... (could be long)\n",
      "--> len(path_nodes): 260\n",
      "->Computing possible paths \tRunning time is 25.24s\n",
      "-> Filtering paths... (could be long)\n",
      "--> len(paths_nodes_filtered): 198\n",
      "->\tRunning time is 3.4s\n",
      "-> Computing hypothesises...\n",
      "--> hypothesises: [['Q9176', 4.43899549326927], ['Q11879590', 1.5916071199977098], ['Q11424', 0.6396359268644466], ['Q1860', 0.5161037078564268], ['Q215380', 0.4867909257748099], ['Q5398426', 0.44913305204517906], ['Q1354463', 0.39704441264691837], ['1979-01-01T00:00:00Z', 0.3072978995190267]]\n",
      "->Computing hypothesises \tRunning time is 32.17s\n",
      "-> Computing golden paths...\n",
      "--> len(golden_paths): 1\n",
      "->\tRunning time is 5.97s\n",
      "--> len(cleared_golden_paths): 1\n",
      "---> First path: ['Q9176', 'P364', 'Q12605533', 'P31', 'Q11424', 'P31', 'Q695209', 'P1476', 'The Mist@en']\n",
      "->\tTotal Running time is 146.39s\n",
      "\n",
      "                           question    answer qanswer platypus  convex  \\\n",
      "342  Who is the author of The Mist?  Q2344912   False   Q39829  Q18594   \n",
      "\n",
      "                                                    tm  tm_time tm_top2  \\\n",
      "342  [Q9176, Q11879590, Q11424, Q1860, Q215380, Q53...    146.6   False   \n",
      "\n",
      "    tm_top3 tm_top4 tm_top5 tm_topall  \n",
      "342   False   False   False     False  \n",
      "Saving Dataframe Done! /data/users/romain.claret/tm/mse.tm.chatbot.base/benchmark_pickles/convex/benchmarking-qanswer-platypus-convex-qagraph-from-0-to-343.pickle.bz2\n",
      "\n",
      "\n",
      "\t>>> Processing 343/2240: (2001-06-22T00:00:00Z) What date was the 2001 film The Fast and the Furious released in theaters?                                  Asking Platypus\n",
      "Asking Convex\n",
      "Asking GraphQA\n",
      "User input: What date was the 2001 film The Fast and the Furious released in theaters?\n",
      "--> Auto correcting question in progress...\n",
      "-> Auto corrected q_nlp: What date was the 2001 film The Fast and the Furious released in theaters \n",
      "-> q_themes: ([(The Fast and the Furious, ['Q155476']), (the film, ['Q25302710', 'Q3520871']), (theaters, ['Q42196365']), (Fast and Furious, ['Q412525', 'Q5437012']), (Fast and the Furious, ['Q129055']), (Date, ['Q858423', 'Q36603893']), (date, ['Q3016931', 'Q902501']), (theater, ['Q11635', 'Q24354'])], [What date was the film The, the Furious release])\n",
      "-> q_themes_enhanced: [('release', ['Q2031291']), ('The Release', ['Q27050992']), ('Release', ['Q10359102'])]\n",
      "--> Calculating predicates... (could be long.. depends on uncached unpure predicates)\n",
      "behold: get_most_similar started with: release\n",
      "behold: get_most_similar started with: theater\n",
      "-> q_predicates: [(be, ['P31']), (released, []), (date, ['P837']), (film, ['P57']), (theaters, [])]\n",
      "-> q_predicates \tRunning time is 72.6s\n",
      "-> q_focused_parts: [(date, ['Q3016931', 'Q902501'])]\n",
      "-> Building the graph with k_deep 3 ... (could be long)\n",
      "->New graph \tRunning time is 56.73s\n",
      "-->  535 nodes and 528 edges\n",
      "--> Removing meaningless subgraphs\n",
      "--> New graph of: 523 nodes and 518 edges\n",
      "---> Rebuilding the graph with k_deep 2 ... Previously: 523 nodes or 518 edges was above the limit of 350\n",
      "-> predicates_dict: {'P31': 168, 'P428': 1, 'P136': 4, 'P279': 3, 'P1040': 1, 'P577': 6, 'P291': 1, 'P360': 2, 'P805': 2, 'P1343': 2, 'P2125': 1, 'P1814': 1, 'P3650': 1, 'P585': 1, 'P1082': 1, 'P123': 1, 'P1433': 1, 'P407': 1, 'P973': 1, 'P527': 2, 'P406': 2, 'P1476': 2, 'P364': 4, 'P131': 1, 'P57': 4, 'P19': 2, 'P2888': 1, 'P856': 2, 'P453': 2, 'P161': 2, 'P5125': 1, 'P2747': 1, 'P1709': 1, 'P162': 1, 'P1545': 1, 'P156': 1, 'P179': 1, 'P373': 1, 'P935': 1, 'P421': 1, 'P17': 1, 'P2093': 1, 'P750': 1, 'P175': 1, 'P1963': 2}\n",
      "-> paths_keywords: (['date', 'released', 'theaters', 'the fast and the furious', 'the film', 'fast and furious', 'theater'], {'instance of': [instance of, ['P31']], 'day in year for periodic occurrence': [day in date for periodic occurrence, ['P837']], 'director': [director, ['P57']]}, [What])\n",
      "-> Computing possible paths... (could be long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--> len(path_nodes): 18608\n",
      "->Computing possible paths \tRunning time is 26.45s\n",
      "-> Filtering paths... (could be long)\n",
      "--> len(paths_nodes_filtered): 16996\n",
      "->\tRunning time is 39.43s\n",
      "-> Computing hypothesises...\n"
     ]
    }
   ],
   "source": [
    "### Evaluate\n",
    "banning_str = False\n",
    "\n",
    "start_time = time.time()\n",
    "conversations_len = len(conversations)\n",
    "\n",
    "for i, conversation in enumerate(conversations):\n",
    "    if i >= len(df):\n",
    "        questions = [turn['question'] for turn in conversation['questions']]\n",
    "        question = questions[0]\n",
    "        answers = [tmqa.wikidata_url_to_wikidata_id(turn['answer']) for turn in conversation['questions']]\n",
    "        answer = answers[0]\n",
    "        print(\"\\r\\t>>> Processing {}/{}: ({}) {}\".format(i,conversations_len,answer,question), end='                                  ')\n",
    "        \n",
    "        #print(\"Asking qAnswer\")\n",
    "        result_qanswer = False #ask_qanswer(question)\n",
    "        print(\"Asking Platypus\")\n",
    "        result_platypus = ask_platypus(question)\n",
    "        print(\"Asking Convex\")\n",
    "        result_convex = ask_convex(question)\n",
    "        print(\"\\n\",i, \"-> qAnswer\", result_qanswer) if result_qanswer == answer else False\n",
    "        print(\"\\n\",i, \"-> Platypus\", result_platypus) if result_platypus == answer else False\n",
    "        print(\"\\n\",i, \"-> Convex\", result_convex) if result_convex == answer else False\n",
    "\n",
    "        df_tm = False\n",
    "        df_tm_top2 = False\n",
    "        df_tm_top3 = False\n",
    "        df_tm_top4 = False\n",
    "        df_tm_top5 = False\n",
    "        df_tm_topall = False\n",
    "        \n",
    "        print(\"Asking GraphQA\")\n",
    "        start_time_tmqa = time.time()\n",
    "        result_tmqa = ask_tmqa(question, verbose=True, timer=True, banning_str=banning_str)   \n",
    "        if result_tmqa:\n",
    "            df_tm = result_tmqa[0]\n",
    "            if answer in result_tmqa[:2]:\n",
    "                df_tm_top2 = True\n",
    "            if answer in result_tmqa[:3]:\n",
    "                df_tm_top3 = True\n",
    "            if answer in result_tmqa[:4]:\n",
    "                df_tm_top4 = True\n",
    "            if answer in result_tmqa[:5]:\n",
    "                df_tm_top5 = True\n",
    "            if answer in result_tmqa:\n",
    "                df_tm_topall = True\n",
    "            \n",
    "        df_tm_time = round(time.time()-start_time_tmqa,2)\n",
    "        \n",
    "        df = df.append({\"question\":question, 'answer':answer, 'qanswer':result_qanswer, 'platypus':result_platypus, 'convex':result_convex,\n",
    "                       \"tm\":df_tm, \"tm_time\":df_tm_time, \"tm_top2\":df_tm_top2, \"tm_top3\":df_tm_top3, \"tm_top4\":df_tm_top4, \"tm_top5\":df_tm_top5, \"tm_topall\":df_tm_topall},\n",
    "                       ignore_index=True)\n",
    "\n",
    "        print(\"\\n\",i, \"-> tmqa\", df_tm) if str(df_tm) == str(answer) else False\n",
    "        print(\"\\n\",i, \"-> tmqa in answers\", result_tmqa) if df_tm_topall == True else False\n",
    "        \n",
    "        print(df.tail(1))\n",
    "        \n",
    "        pickle_data(df, \"benchmarking-qanswer-platypus-convex-qagraph-from-0-to-\"+str(len(df)))\n",
    "        \n",
    "        print(\"\\n\")\n",
    "    \n",
    "    #break\n",
    "\n",
    "print(\"->\\tRunning time is {}s\".format(round(time.time()-start_time,2)))\n",
    "df.tail(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#result_convex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SAVING\n",
    "#pickle_data(df_loaded, \"benchmarking-qanswer-platypus-convex-tm1-from-0-to-\"+str(len(df_loaded)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LOADING\n",
    "#df_loaded = pd.read_pickle(\"/data/users/romain.claret/tm/wikidata-simplequestions/benchmark_pickles/benchmarking-qanswer-platypus-convex-tm1-from-0-to-9961.pickle.bz2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = df_loaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_loaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_loaded = df_loaded.replace(\"\", False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_loaded['qanswer'][34] = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#del df_loaded['tm2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_loaded.rename({'mine':'tm1'}, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_loaded['tm1_top4'] = \"False\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_loaded = df_loaded[['question','source','qanswer','platypus','convex','tm1','tm1_time','tm1_top2','tm1_top3','tm1_top4','tm1_top5','tm1_topall']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_loaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_loaded_len = len(df_loaded)\n",
    "#for i, question in enumerate(df_loaded['question']):\n",
    "#    if i >= 0:\n",
    "#    #if i >= 497:\n",
    "#        source = str(df_loaded['source'][i])\n",
    "#        print(str(i)+\"/\"+str(df_loaded_len),question,\"-> source:\",source)\n",
    "#        \n",
    "#        start_time = time.time()\n",
    "#        result_tmqa_1 = ask_tmqa_1(question, verbose=True)\n",
    "#        \n",
    "#        if result_tmqa_1:\n",
    "#            df_loaded['tm1'][i] = result_tmqa_1[0]\n",
    "#            if source in result_tmqa_1[:2]:\n",
    "#                df_loaded['tm1_top2'][i] = True\n",
    "#            if source in result_tmqa_1[:3]:\n",
    "#                df_loaded['tm1_top3'][i] = True\n",
    "#            if source in result_tmqa_1[:4]:\n",
    "#                df_loaded['tm1_top4'][i] = True\n",
    "#            if source in result_tmqa_1[:5]:\n",
    "#                df_loaded['tm1_top5'][i] = True\n",
    "#            if source in result_tmqa_1:\n",
    "#                df_loaded['tm1_topall'][i] = True\n",
    "#        else:\n",
    "#            df_loaded['tm1'][i] = False\n",
    "#        end_time = time.time()\n",
    "#        df_loaded['tm1_time'][i] = round(end_time-start_time,2)\n",
    "#        print(\"->\\tRunning time is {}s\".format(round(end_time-start_time,2)))\n",
    "#        print(str(str(df_loaded['tm1'][i])==str(source)),\"---> result_tmqa_1:\",str(result_tmqa_1)+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.tail(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_loaded.head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = df_loaded.copy()\n",
    "#df = df.replace(\"\", False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_backup = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#max_row = 496\n",
    "#df_len = len(df)\n",
    "#df_qanswer_max = df[(df.index<=max_row) & (df.qanswer == df.source)]\n",
    "#df_qanswer_max_len = len(df_qanswer_max)\n",
    "#\n",
    "#df_platypus_max = df[(df.index<=max_row) & (df.platypus == df.source)]\n",
    "#df_platypus_max_len = len(df_platypus_max)\n",
    "#\n",
    "#df_convex_max = df[(df.index<=max_row) & (df.convex == df.source)]\n",
    "#df_convex_max_len = len(df_convex_max)\n",
    "#\n",
    "#df_tm1_max = df[(df.index<=max_row) & (df.tm1 == df.source)]\n",
    "#df_tm1_max_len = len(df_tm1_max)\n",
    "#\n",
    "#df_tm1_max_top2 = df[(df.index<=max_row) & (df.tm1_top2 == True)]\n",
    "#df_tm1_max_top2_len = len(df_tm1_max_top2)\n",
    "#\n",
    "#df_tm1_max_top3 = df[(df.index<=max_row) & (df.tm1_top3 == True)]\n",
    "#df_tm1_max_top3_len = len(df_tm1_max_top3)\n",
    "#\n",
    "#df_tm1_max_top4 = df[(df.index<=max_row) & (df.tm1_top4 == True)]\n",
    "#df_tm1_max_top4_len = len(df_tm1_max_top4)\n",
    "#\n",
    "#df_tm1_max_top5 = df[(df.index<=max_row) & (df.tm1_top5 == True)]\n",
    "#df_tm1_max_top5_len = len(df_tm1_max_top5)\n",
    "#\n",
    "#df_tm1_max_topall = df[(df.index<=max_row) & (df.tm1_topall == True)]\n",
    "#df_tm1_max_topall_len = len(df_tm1_max_topall)\n",
    "#\n",
    "#print(\"qanswer:\", df_qanswer_max_len,df_qanswer_max_len/max_row)\n",
    "#print(\"platypus:\", df_platypus_max_len, df_platypus_max_len/max_row)\n",
    "#print(\"convex:\", df_convex_max_len, df_convex_max_len/max_row)\n",
    "#print(\"tm1:\", df_tm1_max_len, df_tm1_max_len/max_row)\n",
    "#print(\"tm1_top2:\", df_tm1_max_top2_len, df_tm1_max_top2_len/max_row)\n",
    "#print(\"tm1_top3:\", df_tm1_max_top3_len, df_tm1_max_top3_len/max_row)\n",
    "#print(\"tm1_top4:\", df_tm1_max_top4_len, df_tm1_max_top4_len/max_row)\n",
    "#print(\"tm1_top5:\", df_tm1_max_top5_len, df_tm1_max_top5_len/max_row)\n",
    "#print(\"tm1_topall:\", df_tm1_max_topall_len, df_tm1_max_topall_len/max_row)\n",
    "#\n",
    "#df[ & (df.qanswer == df.source)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(\"qanswer:\",len(df[df.qanswer == df.source]),len(df[df.qanswer == df.source])/len(df))\n",
    "#print(\"platypus:\",len(df[df.platypus == df.source]),len(df[df.platypus == df.source])/len(df))\n",
    "#print(\"convex:\",len(df[df.convex == df.source]),len(df[df.convex == df.source])/len(df))\n",
    "#print(\"tm1:\",len(df[df.tm1 == df.source]),len(df[df.tm1 == df.source])/len(df))\n",
    "#print(\"tm1_top2:\",len(df[df.tm1_top2 == True]),len(df[df.tm1_top2 == True])/len(df))\n",
    "#print(\"tm1_top3:\",len(df[df.tm1_top3 == True]),len(df[df.tm1_top3 == True])/len(df))\n",
    "#print(\"tm1_top4:\",len(df[df.tm1_top4 == True]),len(df[df.tm1_top4 == True])/len(df))\n",
    "#print(\"tm1_top5:\",len(df[df.tm1_top5 == True]),len(df[df.tm1_top5 == True])/len(df))\n",
    "#print(\"tm1_topall:\",len(df[df.tm1_topall == True]),len(df[df.tm1_topall == True])/len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.tm1_top2 = df.tm1_top3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:qa]",
   "language": "python",
   "name": "conda-env-qa-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
