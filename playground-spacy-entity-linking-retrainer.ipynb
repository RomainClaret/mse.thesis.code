{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-12-05 14:46:12,951 - root - INFO - Test info\n",
      "2019-12-05 14:46:12,954 - root - DEBUG - Test debug\n",
      "2019-12-05 14:46:12,956 - root - ERROR - Test error\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "logger = logging.getLogger()\n",
    "\n",
    "formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Setup file handler\n",
    "fhandler  = logging.FileHandler('retrain-wd-entity-linking.log', mode='a')\n",
    "fhandler.setLevel(logging.DEBUG)\n",
    "fhandler.setFormatter(formatter)\n",
    "\n",
    "# Configure stream handler for the cells\n",
    "chandler = logging.StreamHandler()\n",
    "chandler.setLevel(logging.DEBUG)\n",
    "chandler.setFormatter(formatter)\n",
    "\n",
    "# Add both handlers\n",
    "logger.addHandler(fhandler)\n",
    "logger.addHandler(chandler)\n",
    "logger.setLevel(logging.DEBUG)\n",
    "\n",
    "# Show the handlers\n",
    "logger.handlers\n",
    "\n",
    "# Log Something\n",
    "logger.info(\"Test info\")\n",
    "logger.debug(\"Test debug\")\n",
    "logger.error(\"Test error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_el_toy_example(nlp):\n",
    "    text = (\n",
    "        \"In The Hitchhiker's Guide to the Galaxy, written by Douglas Adams, \"\n",
    "        \"Douglas reminds us to always bring our towel, even in China or Brazil. \"\n",
    "        \"The main character in Doug's novel is the man Arthur Dent, \"\n",
    "        \"but Dougledydoug doesn't write about George Washington or Homer Simpson.\"\n",
    "    )\n",
    "    doc = nlp(text)\n",
    "    logger.info(text)\n",
    "    for ent in doc.ents:\n",
    "        logger.info(\" \".join([\"ent\", ent.text, ent.label_, ent.kb_id_]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-12-05 14:46:14,340 - root - INFO - PRE 0: loading nlp model: /data/users/romain.claret/tm/data2/nlp_custom_3\n",
      "2019-12-05 14:47:00,499 - root - INFO - PRE 1: loading kb model: /data/users/romain.claret/tm/data2/kb\n",
      "2019-12-05 14:47:04,524 - root - INFO - PRE 2: setup entity linker\n",
      "2019-12-05 14:47:06,029 - root - INFO - STEP 0: starting with: /data/users/romain.claret/tm/data2/pieces/x04.jsonl\n",
      "2019-12-05 14:47:06,030 - root - INFO - STEP 1: loading training data\n",
      "2019-12-05 14:47:06,030 - wikipedia_processor - INFO - Reading train data with limit 1000000\n",
      " 20%|█▉        | 195625/1000000 [1:16:10<7:26:10, 30.05it/s] "
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.util import minibatch, compounding\n",
    "from pathlib import Path\n",
    "import sys\n",
    "import random\n",
    "sys.path.insert(1, '/data/users/romain.claret/tm/spaCy/bin/wiki_entity_linking')\n",
    "import kb_creator as kbc\n",
    "import wikipedia_processor as wp\n",
    "import entity_linker_evaluation as ele\n",
    "\n",
    "dir_kb = Path(\"/data/users/romain.claret/tm/data2/\")\n",
    "KB_FILE = \"kb\"\n",
    "KB_MODEL_DIR = \"nlp_kb\"\n",
    "OUTPUT_MODEL_DIR = \"nlp_custom_3\"\n",
    "nlp_dir = dir_kb / OUTPUT_MODEL_DIR\n",
    "kb_path = dir_kb / KB_FILE\n",
    "\n",
    "paths = [\"pieces/x00.jsonl\",\n",
    "        \"pieces/x01.jsonl\",\n",
    "        \"pieces/x02.jsonl\",\n",
    "        \"pieces/x03.jsonl\",\n",
    "        \"pieces/x04.jsonl\",\n",
    "        \"pieces/x05.jsonl\",\n",
    "        \"pieces/x06.jsonl\"]\n",
    "\n",
    "epochs=10\n",
    "dropout=0.5\n",
    "lr=0.005\n",
    "l2=1e-6\n",
    "train_inst=1000000\n",
    "dev_inst=159000\n",
    "labels_discard=None\n",
    "\n",
    "logger.info(\"PRE 0: loading nlp model: \"+str(nlp_dir))\n",
    "nlp = spacy.load(nlp_dir)\n",
    "logger.info(\"PRE 1: loading kb model: \"+str(kb_path))\n",
    "kb = kbc.read_kb(nlp, kb_path)\n",
    "logger.info(\"PRE 2: setup entity linker\")\n",
    "other_pipes = [pipe for pipe in nlp.pipe_names if pipe != \"entity_linker\"]\n",
    "el_pipe = nlp.get_pipe(\"entity_linker\")\n",
    "\n",
    "with nlp.disable_pipes(*other_pipes):  # only train Entity Linking\n",
    "    optimizer = nlp.begin_training()\n",
    "    optimizer.learn_rate = lr\n",
    "    optimizer.L2 = l2\n",
    "\n",
    "start_from = 4\n",
    "\n",
    "for i, path in enumerate(paths):\n",
    "    if i >= start_from:\n",
    "        training_path = dir_kb / paths[i]\n",
    "        logger.info(\"STEP 0: starting with: \"+str(training_path))\n",
    "\n",
    "        logger.info(\"STEP 1: loading training data\")\n",
    "        train_data = wp.read_training(\n",
    "                nlp=nlp,\n",
    "                entity_file_path=training_path,\n",
    "                dev=False,\n",
    "                limit=train_inst,\n",
    "                kb=kb,\n",
    "                labels_discard=labels_discard\n",
    "            )\n",
    "\n",
    "        logger.info(\"STEP 2: loading dev data\")\n",
    "        dev_data = wp.read_training(\n",
    "            nlp=nlp,\n",
    "            entity_file_path=training_path,\n",
    "            dev=True,\n",
    "            limit=dev_inst,\n",
    "            kb=None,\n",
    "            labels_discard=labels_discard\n",
    "        )\n",
    "\n",
    "        logger.info(\"STEP 3: evaluating the baseline\")\n",
    "        ele.measure_performance(dev_data, kb, el_pipe, baseline=True, context=False)\n",
    "\n",
    "        logger.info(\"STEP 4: starting training\")\n",
    "        for itn in range(epochs):\n",
    "            random.shuffle(train_data)\n",
    "            losses = {}\n",
    "            batches = minibatch(train_data, size=compounding(4.0, 128.0, 1.001))\n",
    "            batchnr = 0\n",
    "\n",
    "            with nlp.disable_pipes(*other_pipes):\n",
    "                for batch in batches:\n",
    "                    try:\n",
    "                        docs, golds = zip(*batch)\n",
    "                        nlp.update(\n",
    "                            docs=docs,\n",
    "                            golds=golds,\n",
    "                            sgd=optimizer,\n",
    "                            drop=dropout,\n",
    "                            losses=losses,\n",
    "                        )\n",
    "                        batchnr += 1\n",
    "                    except Exception as e:\n",
    "                        logger.error(\"Error updating batch:\" + str(e))\n",
    "            if batchnr > 0:\n",
    "                logging.info(\"Epoch {}, train loss {}\".format(itn, round(losses[\"entity_linker\"] / batchnr, 2)))\n",
    "                ele.measure_performance(dev_data, kb, el_pipe, baseline=False, context=True)\n",
    "\n",
    "        logger.info(\"STEP 5: evaluating training\")\n",
    "        ele.measure_performance(dev_data, kb, el_pipe)\n",
    "\n",
    "        logger.info(\"STEP 6: evaluating with example\")\n",
    "        run_el_toy_example(nlp)\n",
    "\n",
    "        logger.info(\"STEP 7: save current state of nlp model\")\n",
    "        nlp_output_dir = dir_kb / str(\"nlp_custom_\"+str(i))\n",
    "        nlp.to_disk(nlp_output_dir)\n",
    "        \n",
    "        del train_data\n",
    "        del dev_data\n",
    "\n",
    "logger.info(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import random\n",
    "#from spacy.util import minibatch, compounding\n",
    "#\n",
    "#logger.info(\"STEP 4: starting training\")\n",
    "#for itn in range(epochs):\n",
    "#    random.shuffle(train_data)\n",
    "#    losses = {}\n",
    "#    batches = minibatch(train_data, size=compounding(4.0, 128.0, 1.001))\n",
    "#    batchnr = 0\n",
    "#\n",
    "#    with nlp.disable_pipes(*other_pipes):\n",
    "#        for batch in batches:\n",
    "#            try:\n",
    "#                docs, golds = zip(*batch)\n",
    "#                nlp.update(\n",
    "#                    docs=docs,\n",
    "#                    golds=golds,\n",
    "#                    sgd=optimizer,\n",
    "#                    drop=dropout,\n",
    "#                    losses=losses,\n",
    "#                )\n",
    "#                batchnr += 1\n",
    "#            except Exception as e:\n",
    "#                logger.error(\"Error updating batch:\" + str(e))\n",
    "#    if batchnr > 0:\n",
    "#        logging.info(\"Epoch {}, train loss {}\".format(itn, round(losses[\"entity_linker\"] / batchnr, 2)))\n",
    "#        ele.measure_performance(dev_data, kb, el_pipe, baseline=False, context=True)\n",
    "#\n",
    "#logger.info(\"STEP 5: evaluating training\")\n",
    "#ele.measure_performance(dev_data, kb, el_pipe)\n",
    "#\n",
    "#logger.info(\"STEP 6: evaluating with example\")\n",
    "#run_el_toy_example(nlp)\n",
    "#\n",
    "#logger.info(\"STEP 7: save current state of nlp model\")\n",
    "#nlp_output_dir = dir_kb / str(\"nlp_custom_\"+str(i))\n",
    "#nlp.to_disk(nlp_output_dir)\n",
    "#\n",
    "#logger.info(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-12-09 13:28:34,200 - root - INFO - Done!\n"
     ]
    }
   ],
   "source": [
    "logger.info(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dev_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-258e6a8e9ef7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#del train_data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mdel\u001b[0m \u001b[0mdev_data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'dev_data' is not defined"
     ]
    }
   ],
   "source": [
    "#del train_data\n",
    "del "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:qa]",
   "language": "python",
   "name": "conda-env-qa-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
