{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-12-02 19:51:01,529 - root - INFO - Test info\n",
      "2019-12-02 19:51:01,531 - root - DEBUG - Test debug\n",
      "2019-12-02 19:51:01,532 - root - ERROR - Test error\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "logger = logging.getLogger()\n",
    "\n",
    "formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Setup file handler\n",
    "fhandler  = logging.FileHandler('retrain-wd-entity-linking.log', mode='a')\n",
    "fhandler.setLevel(logging.DEBUG)\n",
    "fhandler.setFormatter(formatter)\n",
    "\n",
    "# Configure stream handler for the cells\n",
    "chandler = logging.StreamHandler()\n",
    "chandler.setLevel(logging.DEBUG)\n",
    "chandler.setFormatter(formatter)\n",
    "\n",
    "# Add both handlers\n",
    "logger.addHandler(fhandler)\n",
    "logger.addHandler(chandler)\n",
    "logger.setLevel(logging.DEBUG)\n",
    "\n",
    "# Show the handlers\n",
    "logger.handlers\n",
    "\n",
    "# Log Something\n",
    "logger.info(\"Test info\")\n",
    "logger.debug(\"Test debug\")\n",
    "logger.error(\"Test error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_el_toy_example(nlp):\n",
    "    text = (\n",
    "        \"In The Hitchhiker's Guide to the Galaxy, written by Douglas Adams, \"\n",
    "        \"Douglas reminds us to always bring our towel, even in China or Brazil. \"\n",
    "        \"The main character in Doug's novel is the man Arthur Dent, \"\n",
    "        \"but Dougledydoug doesn't write about George Washington or Homer Simpson.\"\n",
    "    )\n",
    "    doc = nlp(text)\n",
    "    logger.info(text)\n",
    "    for ent in doc.ents:\n",
    "        logger.info(\" \".join([\"ent\", ent.text, ent.label_, ent.kb_id_]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-12-02 19:51:01,893 - root - INFO - PRE 0: loading nlp model: /data/users/romain.claret/tm/data2/nlp_custom_1\n",
      "2019-12-02 19:51:31,963 - root - INFO - PRE 1: loading kb model: /data/users/romain.claret/tm/data2/kb\n",
      "2019-12-02 19:51:35,985 - root - INFO - PRE 2: setup entity linker\n",
      "2019-12-02 19:51:36,936 - root - INFO - STEP 0: starting with: /data/users/romain.claret/tm/data2/pieces/x02.jsonl\n",
      "2019-12-02 19:51:36,937 - root - INFO - STEP 1: loading training data\n",
      "2019-12-02 19:51:36,938 - wikipedia_processor - INFO - Reading train data with limit 1000000\n",
      "2019-12-03 02:20:42,017 - wikipedia_processor - INFO - Read 1000000 entities in 153807 articles\n",
      "2019-12-03 02:20:42,017 - root - INFO - STEP 2: loading dev data\n",
      "2019-12-03 02:20:42,018 - wikipedia_processor - INFO - Reading dev data with limit 159000\n",
      "2019-12-03 03:07:44,415 - wikipedia_processor - INFO - Read 159005 entities in 20221 articles\n",
      "2019-12-03 03:07:44,416 - root - INFO - STEP 3: evaluating the baseline\n",
      "2019-12-03 03:07:48,856 - entity_linker_evaluation - INFO - Counts: {'CARDINAL': 262, 'DATE': 1660, 'EVENT': 1823, 'FAC': 4512, 'GPE': 44345, 'LANGUAGE': 420, 'LAW': 163, 'LOC': 4175, 'MONEY': 46, 'NORP': 5501, 'ORDINAL': 51, 'ORG': 30100, 'PERCENT': 3, 'PERSON': 60113, 'PRODUCT': 1358, 'QUANTITY': 49, 'TIME': 45, 'WORK_OF_ART': 4379}\n",
      "2019-12-03 03:07:48,857 - entity_linker_evaluation - INFO - Random: F-score = 0.53 | Recall = 0.485 | Precision = 0.584 | F-score by label = {'CARDINAL': 0.2012847965738758, 'DATE': 0.10178901912399753, 'EVENT': 0.3886883273164862, 'FAC': 0.613340214165914, 'GPE': 0.29010973583846134, 'LANGUAGE': 0.23076923076923078, 'LAW': 0.6950354609929078, 'LOC': 0.528395061728395, 'MONEY': 0.48484848484848486, 'NORP': 0.2555886736214605, 'ORDINAL': 0.08421052631578947, 'ORG': 0.5777179803415928, 'PERCENT': 0.0, 'PERSON': 0.7338469995248432, 'PRODUCT': 0.6170300287947348, 'QUANTITY': 0.5569620253164558, 'TIME': 0.5882352941176471, 'WORK_OF_ART': 0.5888084265964451}\n",
      "2019-12-03 03:07:48,857 - entity_linker_evaluation - INFO - Prior: F-score = 0.786 | Recall = 0.719 | Precision = 0.866 | F-score by label = {'CARDINAL': 0.28693790149892934, 'DATE': 0.13448488587291793, 'EVENT': 0.8050541516245489, 'FAC': 0.7315185137401626, 'GPE': 0.8206790051241255, 'LANGUAGE': 0.7163461538461539, 'LAW': 0.7659574468085105, 'LOC': 0.8137751786874593, 'MONEY': 0.5151515151515151, 'NORP': 0.6784649776453054, 'ORDINAL': 0.0631578947368421, 'ORG': 0.7772919236087752, 'PERCENT': 0.0, 'PERSON': 0.8053813831719975, 'PRODUCT': 0.774167009461127, 'QUANTITY': 0.6835443037974683, 'TIME': 0.7294117647058822, 'WORK_OF_ART': 0.6922975641869651}\n",
      "2019-12-03 03:07:48,858 - entity_linker_evaluation - INFO - Oracle: F-score = 0.878 | Recall = 0.783 | Precision = 1.0 | F-score by label = {'CARDINAL': 0.4899135446685879, 'DATE': 0.4242999525391552, 'EVENT': 0.864528184366241, 'FAC': 0.8091844813935075, 'GPE': 0.9357285141718866, 'LANGUAGE': 0.9743589743589743, 'LAW': 0.801470588235294, 'LOC': 0.8902179691653376, 'MONEY': 0.5396825396825397, 'NORP': 0.9538841875059427, 'ORDINAL': 0.17857142857142855, 'ORG': 0.8486621913668789, 'PERCENT': 0.5, 'PERSON': 0.8607302258147049, 'PRODUCT': 0.8559393428812131, 'QUANTITY': 0.7105263157894737, 'TIME': 0.9024390243902439, 'WORK_OF_ART': 0.8050211488606904}\n",
      "2019-12-03 03:07:48,858 - root - INFO - STEP 4: starting training\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'random' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-678ba52cd0cd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"STEP 4: starting training\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mitn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m             \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m             \u001b[0mlosses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m             \u001b[0mbatches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mminibatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompounding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m128.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.001\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'random' is not defined"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.util import minibatch, compounding\n",
    "from pathlib import Path\n",
    "import sys\n",
    "import random\n",
    "sys.path.insert(1, '/data/users/romain.claret/tm/spaCy/bin/wiki_entity_linking')\n",
    "import kb_creator as kbc\n",
    "import wikipedia_processor as wp\n",
    "import entity_linker_evaluation as ele\n",
    "\n",
    "dir_kb = Path(\"/data/users/romain.claret/tm/data2/\")\n",
    "KB_FILE = \"kb\"\n",
    "KB_MODEL_DIR = \"nlp_kb\"\n",
    "OUTPUT_MODEL_DIR = \"nlp_custom_1\"\n",
    "nlp_dir = dir_kb / OUTPUT_MODEL_DIR\n",
    "kb_path = dir_kb / KB_FILE\n",
    "\n",
    "paths = [\"pieces/x00.jsonl\",\n",
    "        \"pieces/x01.jsonl\",\n",
    "        \"pieces/x02.jsonl\",\n",
    "        \"pieces/x03.jsonl\",\n",
    "        \"pieces/x04.jsonl\",\n",
    "        \"pieces/x05.jsonl\",\n",
    "        \"pieces/x06.jsonl\"]\n",
    "\n",
    "epochs=10\n",
    "dropout=0.5\n",
    "lr=0.005\n",
    "l2=1e-6\n",
    "train_inst=1000000\n",
    "dev_inst=159000\n",
    "labels_discard=None\n",
    "\n",
    "logger.info(\"PRE 0: loading nlp model: \"+str(nlp_dir))\n",
    "nlp = spacy.load(nlp_dir)\n",
    "logger.info(\"PRE 1: loading kb model: \"+str(kb_path))\n",
    "kb = kbc.read_kb(nlp, kb_path)\n",
    "logger.info(\"PRE 2: setup entity linker\")\n",
    "other_pipes = [pipe for pipe in nlp.pipe_names if pipe != \"entity_linker\"]\n",
    "el_pipe = nlp.get_pipe(\"entity_linker\")\n",
    "\n",
    "with nlp.disable_pipes(*other_pipes):  # only train Entity Linking\n",
    "    optimizer = nlp.begin_training()\n",
    "    optimizer.learn_rate = lr\n",
    "    optimizer.L2 = l2\n",
    "\n",
    "start_from = 2\n",
    "\n",
    "for i, path in enumerate(paths):\n",
    "    if i >= start_from:\n",
    "        training_path = dir_kb / paths[start_from]\n",
    "        logger.info(\"STEP 0: starting with: \"+str(training_path))\n",
    "\n",
    "        logger.info(\"STEP 1: loading training data\")\n",
    "        train_data = wp.read_training(\n",
    "                nlp=nlp,\n",
    "                entity_file_path=training_path,\n",
    "                dev=False,\n",
    "                limit=train_inst,\n",
    "                kb=kb,\n",
    "                labels_discard=labels_discard\n",
    "            )\n",
    "\n",
    "        logger.info(\"STEP 2: loading dev data\")\n",
    "        dev_data = wp.read_training(\n",
    "            nlp=nlp,\n",
    "            entity_file_path=training_path,\n",
    "            dev=True,\n",
    "            limit=dev_inst,\n",
    "            kb=None,\n",
    "            labels_discard=labels_discard\n",
    "        )\n",
    "\n",
    "        logger.info(\"STEP 3: evaluating the baseline\")\n",
    "        ele.measure_performance(dev_data, kb, el_pipe, baseline=True, context=False)\n",
    "\n",
    "        logger.info(\"STEP 4: starting training\")\n",
    "        for itn in range(epochs):\n",
    "            random.shuffle(train_data)\n",
    "            losses = {}\n",
    "            batches = minibatch(train_data, size=compounding(4.0, 128.0, 1.001))\n",
    "            batchnr = 0\n",
    "\n",
    "            with nlp.disable_pipes(*other_pipes):\n",
    "                for batch in batches:\n",
    "                    try:\n",
    "                        docs, golds = zip(*batch)\n",
    "                        nlp.update(\n",
    "                            docs=docs,\n",
    "                            golds=golds,\n",
    "                            sgd=optimizer,\n",
    "                            drop=dropout,\n",
    "                            losses=losses,\n",
    "                        )\n",
    "                        batchnr += 1\n",
    "                    except Exception as e:\n",
    "                        logger.error(\"Error updating batch:\" + str(e))\n",
    "            if batchnr > 0:\n",
    "                logging.info(\"Epoch {}, train loss {}\".format(itn, round(losses[\"entity_linker\"] / batchnr, 2)))\n",
    "                ele.measure_performance(dev_data, kb, el_pipe, baseline=False, context=True)\n",
    "\n",
    "        logger.info(\"STEP 5: evaluating training\")\n",
    "        ele.measure_performance(dev_data, kb, el_pipe)\n",
    "\n",
    "        logger.info(\"STEP 6: evaluating with example\")\n",
    "        run_el_toy_example(nlp)\n",
    "\n",
    "        logger.info(\"STEP 7: save current state of nlp model\")\n",
    "        nlp_output_dir = dir_kb / str(\"nlp_custom_\"+str(i))\n",
    "        nlp.to_disk(nlp_output_dir)\n",
    "\n",
    "logger.info(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-12-03 11:11:53,880 - root - INFO - STEP 4: starting training\n",
      "2019-12-03 11:51:00,289 - root - INFO - Epoch 0, train loss 0.18\n",
      "2019-12-03 12:22:09,244 - entity_linker_evaluation - INFO - Context Only: F-score = 0.688 | Recall = 0.63 | Precision = 0.758 | F-score by label = {'CARDINAL': 0.22269807280513917, 'DATE': 0.10487353485502778, 'EVENT': 0.7352587244283996, 'FAC': 0.6566894594245904, 'GPE': 0.6790285219588665, 'LANGUAGE': 0.5528846153846154, 'LAW': 0.7163120567375886, 'LOC': 0.6448343079922028, 'MONEY': 0.48484848484848486, 'NORP': 0.3727645305514158, 'ORDINAL': 0.08421052631578947, 'ORG': 0.6747019471540159, 'PERCENT': 0.0, 'PERSON': 0.7646110702207148, 'PRODUCT': 0.6458247634718223, 'QUANTITY': 0.5569620253164558, 'TIME': 0.6823529411764706, 'WORK_OF_ART': 0.6146148782093482}\n",
      "2019-12-03 12:54:18,952 - entity_linker_evaluation - INFO - Context And Prior: F-score = 0.772 | Recall = 0.706 | Precision = 0.851 | F-score by label = {'CARDINAL': 0.2569593147751606, 'DATE': 0.1283158544108575, 'EVENT': 0.7966305655836341, 'FAC': 0.7196490775383823, 'GPE': 0.8053300264395518, 'LANGUAGE': 0.6730769230769231, 'LAW': 0.75177304964539, 'LOC': 0.796361273554256, 'MONEY': 0.5151515151515151, 'NORP': 0.6073025335320417, 'ORDINAL': 0.08421052631578947, 'ORG': 0.7626789251410845, 'PERCENT': 0.0, 'PERSON': 0.798114279858012, 'PRODUCT': 0.7552447552447553, 'QUANTITY': 0.6582278481012658, 'TIME': 0.7058823529411765, 'WORK_OF_ART': 0.680184331797235}\n",
      "2019-12-03 13:33:35,315 - root - INFO - Epoch 1, train loss 0.18\n",
      "2019-12-03 14:05:25,614 - entity_linker_evaluation - INFO - Context Only: F-score = 0.692 | Recall = 0.633 | Precision = 0.763 | F-score by label = {'CARDINAL': 0.22698072805139188, 'DATE': 0.10857495373226403, 'EVENT': 0.7388688327316486, 'FAC': 0.6569474906463683, 'GPE': 0.6910784061395915, 'LANGUAGE': 0.5793269230769231, 'LAW': 0.7163120567375886, 'LOC': 0.678882391163093, 'MONEY': 0.48484848484848486, 'NORP': 0.3777943368107302, 'ORDINAL': 0.08421052631578947, 'ORG': 0.6723100497066189, 'PERCENT': 0.0, 'PERSON': 0.7638470926928345, 'PRODUCT': 0.6260798025503909, 'QUANTITY': 0.5569620253164558, 'TIME': 0.6823529411764706, 'WORK_OF_ART': 0.6164581961816986}\n",
      "2019-12-03 14:36:44,175 - entity_linker_evaluation - INFO - Context And Prior: F-score = 0.772 | Recall = 0.706 | Precision = 0.851 | F-score by label = {'CARDINAL': 0.2612419700214133, 'DATE': 0.1307834669956817, 'EVENT': 0.7966305655836341, 'FAC': 0.7188749838730486, 'GPE': 0.8050258546058635, 'LANGUAGE': 0.6706730769230769, 'LAW': 0.75177304964539, 'LOC': 0.7968810916179336, 'MONEY': 0.5151515151515151, 'NORP': 0.6184798807749627, 'ORDINAL': 0.08421052631578947, 'ORG': 0.7623425645625445, 'PERCENT': 0.0, 'PERSON': 0.7973875695266134, 'PRODUCT': 0.7568901686548745, 'QUANTITY': 0.6582278481012658, 'TIME': 0.7058823529411765, 'WORK_OF_ART': 0.679921000658328}\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from spacy.util import minibatch, compounding\n",
    "\n",
    "logger.info(\"STEP 4: starting training\")\n",
    "for itn in range(epochs):\n",
    "    random.shuffle(train_data)\n",
    "    losses = {}\n",
    "    batches = minibatch(train_data, size=compounding(4.0, 128.0, 1.001))\n",
    "    batchnr = 0\n",
    "\n",
    "    with nlp.disable_pipes(*other_pipes):\n",
    "        for batch in batches:\n",
    "            try:\n",
    "                docs, golds = zip(*batch)\n",
    "                nlp.update(\n",
    "                    docs=docs,\n",
    "                    golds=golds,\n",
    "                    sgd=optimizer,\n",
    "                    drop=dropout,\n",
    "                    losses=losses,\n",
    "                )\n",
    "                batchnr += 1\n",
    "            except Exception as e:\n",
    "                logger.error(\"Error updating batch:\" + str(e))\n",
    "    if batchnr > 0:\n",
    "        logging.info(\"Epoch {}, train loss {}\".format(itn, round(losses[\"entity_linker\"] / batchnr, 2)))\n",
    "        ele.measure_performance(dev_data, kb, el_pipe, baseline=False, context=True)\n",
    "\n",
    "logger.info(\"STEP 5: evaluating training\")\n",
    "ele.measure_performance(dev_data, kb, el_pipe)\n",
    "\n",
    "logger.info(\"STEP 6: evaluating with example\")\n",
    "run_el_toy_example(nlp)\n",
    "\n",
    "logger.info(\"STEP 7: save current state of nlp model\")\n",
    "nlp_output_dir = dir_kb / str(\"nlp_custom_\"+str(i))\n",
    "nlp.to_disk(nlp_output_dir)\n",
    "\n",
    "logger.info(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:qa]",
   "language": "python",
   "name": "conda-env-qa-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
