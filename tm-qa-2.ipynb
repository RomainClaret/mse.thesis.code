{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "IPython.notebook.set_autosave_interval(180000)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autosaving every 180 seconds\n"
     ]
    }
   ],
   "source": [
    "%autosave 180"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/users/romain.claret/miniconda3/envs/qa/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/data/users/romain.claret/miniconda3/envs/qa/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/data/users/romain.claret/miniconda3/envs/qa/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/data/users/romain.claret/miniconda3/envs/qa/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/data/users/romain.claret/miniconda3/envs/qa/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/data/users/romain.claret/miniconda3/envs/qa/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /data/users/romain.claret/miniconda3/envs/qa/lib/python3.7/site-packages/txt2txt/txt2txt.py:25: The name tf.GPUOptions is deprecated. Please use tf.compat.v1.GPUOptions instead.\n",
      "\n",
      "WARNING:tensorflow:From /data/users/romain.claret/miniconda3/envs/qa/lib/python3.7/site-packages/txt2txt/txt2txt.py:27: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "WARNING:tensorflow:From /data/users/romain.claret/miniconda3/envs/qa/lib/python3.7/site-packages/txt2txt/txt2txt.py:27: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/users/romain.claret/miniconda3/envs/qa/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/data/users/romain.claret/miniconda3/envs/qa/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/data/users/romain.claret/miniconda3/envs/qa/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/data/users/romain.claret/miniconda3/envs/qa/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/data/users/romain.claret/miniconda3/envs/qa/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/data/users/romain.claret/miniconda3/envs/qa/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the params file\n",
      "Input encoding {'o': 2, '{': 3, '.': 4, 'J': 5, '0': 6, '1': 7, '<': 8, 'B': 9, 'd': 10, '£': 11, 'e': 12, '6': 13, '!': 14, 'O': 15, 'M': 16, 'X': 17, 'f': 18, 't': 19, 'C': 20, 'V': 21, 'z': 22, 'K': 23, '\\\\': 24, '9': 25, 'P': 26, 'S': 27, '/': 28, '₹': 29, 'F': 30, 'G': 31, '=': 32, '8': 33, ')': 34, '+': 35, ']': 36, 'U': 37, \"'\": 38, '\"': 39, 'g': 40, 'N': 41, 'r': 42, 'u': 43, '&': 44, '$': 45, 'x': 46, '%': 47, ':': 48, '@': 49, '^': 50, 'I': 51, 'L': 52, 'Z': 53, 'h': 54, 'W': 55, 'A': 56, 'v': 57, '?': 58, '2': 59, '~': 60, 's': 61, 'T': 62, 'R': 63, ',': 64, '|': 65, '4': 66, '>': 67, 'y': 68, '(': 69, '[': 70, 'k': 71, 'H': 72, 'l': 73, 'j': 74, '7': 75, 'n': 76, 'i': 77, 'D': 78, 'Q': 79, ' ': 80, 'm': 81, 'Y': 82, '*': 83, '}': 84, '#': 85, 'p': 86, 'q': 87, '5': 88, 'c': 89, '`': 90, 'a': 91, 'b': 92, 'w': 93, '3': 94, 'E': 95, ';': 96, '-': 97}\n",
      "Input decoding {2: 'o', 3: '{', 4: '.', 5: 'J', 6: '0', 7: '1', 8: '<', 9: 'B', 10: 'd', 11: '£', 12: 'e', 13: '6', 14: '!', 15: 'O', 16: 'M', 17: 'X', 18: 'f', 19: 't', 20: 'C', 21: 'V', 22: 'z', 23: 'K', 24: '\\\\', 25: '9', 26: 'P', 27: 'S', 28: '/', 29: '₹', 30: 'F', 31: 'G', 32: '=', 33: '8', 34: ')', 35: '+', 36: ']', 37: 'U', 38: \"'\", 39: '\"', 40: 'g', 41: 'N', 42: 'r', 43: 'u', 44: '&', 45: '$', 46: 'x', 47: '%', 48: ':', 49: '@', 50: '^', 51: 'I', 52: 'L', 53: 'Z', 54: 'h', 55: 'W', 56: 'A', 57: 'v', 58: '?', 59: '2', 60: '~', 61: 's', 62: 'T', 63: 'R', 64: ',', 65: '|', 66: '4', 67: '>', 68: 'y', 69: '(', 70: '[', 71: 'k', 72: 'H', 73: 'l', 74: 'j', 75: '7', 76: 'n', 77: 'i', 78: 'D', 79: 'Q', 80: ' ', 81: 'm', 82: 'Y', 83: '*', 84: '}', 85: '#', 86: 'p', 87: 'q', 88: '5', 89: 'c', 90: '`', 91: 'a', 92: 'b', 93: 'w', 94: '3', 95: 'E', 96: ';', 97: '-'}\n",
      "Output encoding {'o': 2, '{': 3, '.': 4, 'J': 5, '0': 6, '1': 7, '<': 8, 'B': 9, 'd': 10, '£': 11, 'e': 12, '6': 13, '!': 14, 'O': 15, 'M': 16, 'X': 17, 'f': 18, 't': 19, 'C': 20, 'V': 21, 'z': 22, 'K': 23, '\\\\': 24, '9': 25, 'P': 26, 'S': 27, '/': 28, '₹': 29, 'F': 30, 'G': 31, '=': 32, '8': 33, ')': 34, '+': 35, ']': 36, 'U': 37, \"'\": 38, '\"': 39, 'g': 40, 'N': 41, 'r': 42, 'u': 43, '&': 44, '$': 45, 'x': 46, '%': 47, ':': 48, '@': 49, '^': 50, 'I': 51, 'L': 52, 'Z': 53, 'h': 54, 'W': 55, 'A': 56, 'v': 57, '?': 58, '2': 59, '~': 60, 's': 61, 'T': 62, 'R': 63, ',': 64, '|': 65, '4': 66, '>': 67, 'y': 68, '(': 69, '[': 70, 'k': 71, 'H': 72, 'l': 73, 'j': 74, '7': 75, 'n': 76, 'i': 77, 'D': 78, 'Q': 79, ' ': 80, 'm': 81, 'Y': 82, '*': 83, '}': 84, '#': 85, 'p': 86, 'q': 87, '5': 88, 'c': 89, '`': 90, 'a': 91, 'b': 92, 'w': 93, '3': 94, 'E': 95, ';': 96, '-': 97}\n",
      "Output decoding {2: 'o', 3: '{', 4: '.', 5: 'J', 6: '0', 7: '1', 8: '<', 9: 'B', 10: 'd', 11: '£', 12: 'e', 13: '6', 14: '!', 15: 'O', 16: 'M', 17: 'X', 18: 'f', 19: 't', 20: 'C', 21: 'V', 22: 'z', 23: 'K', 24: '\\\\', 25: '9', 26: 'P', 27: 'S', 28: '/', 29: '₹', 30: 'F', 31: 'G', 32: '=', 33: '8', 34: ')', 35: '+', 36: ']', 37: 'U', 38: \"'\", 39: '\"', 40: 'g', 41: 'N', 42: 'r', 43: 'u', 44: '&', 45: '$', 46: 'x', 47: '%', 48: ':', 49: '@', 50: '^', 51: 'I', 52: 'L', 53: 'Z', 54: 'h', 55: 'W', 56: 'A', 57: 'v', 58: '?', 59: '2', 60: '~', 61: 's', 62: 'T', 63: 'R', 64: ',', 65: '|', 66: '4', 67: '>', 68: 'y', 69: '(', 70: '[', 71: 'k', 72: 'H', 73: 'l', 74: 'j', 75: '7', 76: 'n', 77: 'i', 78: 'D', 79: 'Q', 80: ' ', 81: 'm', 82: 'Y', 83: '*', 84: '}', 85: '#', 86: 'p', 87: 'q', 88: '5', 89: 'c', 90: '`', 91: 'a', 92: 'b', 93: 'w', 94: '3', 95: 'E', 96: ';', 97: '-'}\n",
      "WARNING:tensorflow:From /data/users/romain.claret/miniconda3/envs/qa/lib/python3.7/site-packages/tensorflow/python/keras/backend.py:3673: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            (None, 202)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_1 (InputLayer)            (None, 202)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)         (None, 202, 256)     25088       input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 202, 128)     12544       input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_2 (LSTM)                   (None, 202, 256)     525312      embedding_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional) (None, 202, 256)     263168      embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dot_1 (Dot)                     (None, 202, 202)     0           lstm_2[0][0]                     \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "attention (Activation)          (None, 202, 202)     0           dot_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dot_2 (Dot)                     (None, 202, 256)     0           attention[0][0]                  \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 202, 512)     0           dot_2[0][0]                      \n",
      "                                                                 lstm_2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_1 (TimeDistrib (None, 202, 128)     65664       concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_2 (TimeDistrib (None, 202, 98)      12642       time_distributed_1[0][0]         \n",
      "==================================================================================================\n",
      "Total params: 904,418\n",
      "Trainable params: 904,418\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#import convex as cx\n",
    "import requests\n",
    "import time\n",
    "import itertools\n",
    "import re\n",
    "#import numpy\n",
    "from copy import copy\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "#from pprint import pprint\n",
    "import time\n",
    "import json\n",
    "import os\n",
    "import networkx as nx\n",
    "from math import sqrt\n",
    "import spacy\n",
    "from hdt import HDTDocument\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = \"1\"\n",
    "from deepcorrect import DeepCorrect\n",
    "\n",
    "#import deepcorrect\n",
    "#print(deepcorrect.__file__)\n",
    "corrector = DeepCorrect('data/deep_punct/deeppunct_params_en', 'data/deep_punct/deeppunct_checkpoint_wikipedia')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#corrector.correct('of what nationality is ken mcgoogan')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdt_wd = HDTDocument(\"data/kb/wikidata2018_09_11.hdt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nlp = spacy.load(\"en_core_web_lg\")\n",
    "nlp = spacy.load(\"/data/users/romain.claret/tm/wiki-kb-linked-entities/nlp_custom_6\")\n",
    "#print(nlp.pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load settings\n",
    "with open( \"settings-tmqa-1.json\", \"r\") as settings_data:\n",
    "    settings = json.load(settings_data)\n",
    "    use_cache = settings['use_cache']\n",
    "    save_cache = settings['save_cache']\n",
    "    cache_path = settings['cache_path']\n",
    "#cache_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_cache_data():\n",
    "    if save_cache:\n",
    "        with open(os.path.join(cache_path,'statements_dict.json'), 'wb') as outfile:\n",
    "            outfile.write(json.dumps(statements_dict, separators=(',',':')).encode('utf8'))\n",
    "        with open(os.path.join(cache_path,'wd_labels_dict.json'), 'wb') as outfile:\n",
    "            outfile.write(json.dumps(wd_labels_dict, separators=(',',':')).encode('utf8'))\n",
    "        with open(os.path.join(cache_path,'wd_word_ids_dict.json'), 'wb') as outfile:\n",
    "            outfile.write(json.dumps(wd_word_ids_dict, separators=(',',':')).encode('utf8'))\n",
    "        with open(os.path.join(cache_path,'wd_predicate_ids_dict.json'), 'wb') as outfile:\n",
    "            outfile.write(json.dumps(wd_predicate_ids_dict, separators=(',',':')).encode('utf8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load statements cache\n",
    "use_cache = False\n",
    "if use_cache:\n",
    "    path_statements_dict = \"statements_dict.json\"\n",
    "    path_wd_labels_dict = 'wd_labels_dict.json'\n",
    "    path_wd_word_ids_dict = 'wd_word_ids_dict.json'\n",
    "    path_wd_predicate_ids_dict = 'wd_predicate_ids_dict.json'\n",
    "else:\n",
    "    path_statements_dict = \"statements_dict_empty.json\"\n",
    "    path_wd_labels_dict = 'wd_labels_dict_empty.json'\n",
    "    path_wd_word_ids_dict = 'wd_word_ids_dict_empty.json'\n",
    "    path_wd_predicate_ids_dict = 'wd_predicate_ids_dict_empty.json'\n",
    "\n",
    "with open(os.path.join(cache_path,path_statements_dict), \"rb\") as data:\n",
    "    statements_dict = json.load(data)\n",
    "with open(os.path.join(cache_path,path_wd_labels_dict), \"rb\") as data:\n",
    "    wd_labels_dict = json.load(data)\n",
    "with open(os.path.join(cache_path,path_wd_word_ids_dict), \"rb\") as data:\n",
    "    wd_word_ids_dict = json.load(data)\n",
    "with open(os.path.join(cache_path,path_wd_predicate_ids_dict), \"rb\") as data:\n",
    "    wd_predicate_ids_dict = json.load(data)\n",
    "\n",
    "#print(\"len(statements_dict)\",len(statements_dict))\n",
    "#print(\"len(wd_labels_dict)\",len(wd_labels_dict))\n",
    "#print(\"len(wd_word_ids_dict)\",len(wd_word_ids_dict))\n",
    "#print(\"len(wd_predicate_ids_dict)\",len(wd_predicate_ids_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_kb_ents(text):\n",
    "    #doc = nlp_kb(text)\n",
    "    doc = nlp(text)\n",
    "    #for ent in doc.ents:\n",
    "    #    print(\" \".join([\"ent\", ent.text, ent.label_, ent.kb_id_]))\n",
    "    return doc.ents\n",
    "        \n",
    "#ent_text_test = (\n",
    "#    \"In The Hitchhiker's Guide to the Galaxy, written by Douglas Adams, \"\n",
    "#    \"Douglas reminds us to always bring our towel, even in China or Brazil. \"\n",
    "#    \"The main character in Doug's novel is the man Arthur Dent, \"\n",
    "#    \"but Dougledydoug doesn't write about George Washington or Homer Simpson.\"\n",
    "#)\n",
    "#\n",
    "#en_text_test_2 = (\"Which actor voiced the Unicorn in The Last Unicorn?\")\n",
    "#\n",
    "#print([ent.kb_id_ for ent in get_kb_ents(ent_text_test)])\n",
    "#[ent.kb_id_ for ent in get_kb_ents(en_text_test_2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nlp(sentence, autocorrect=False):\n",
    "    #print(\"sentence\",sentence)\n",
    "    nlp_sentence = nlp(sentence)\n",
    "    nlp_sentence_list = list(nlp_sentence)\n",
    "    meaningful_punct = []\n",
    "    \n",
    "    for i_t, t in enumerate(nlp_sentence_list):\n",
    "        if t.lemma_ == \"year\":\n",
    "            nlp_sentence_list[i_t] = \"date\"\n",
    "        elif t.text == \"\\'s\":\n",
    "            if t.pos_ == \"VERB\" or t.pos_ == \"AUX\":\n",
    "                nlp_sentence_list[i_t] = \"is\"\n",
    "            else: nlp_sentence_list[i_t] = \"\"\n",
    "        elif t.text == \"\\'re\":\n",
    "            nlp_sentence_list[i_t] = \"are\"\n",
    "        elif t.pos_ == \"PUNCT\":\n",
    "            if t.text.count(\".\") > 2:\n",
    "                meaningful_punct.append((i_t,\"...\"))\n",
    "                nlp_sentence_list[i_t] = \"...\"\n",
    "            else:\n",
    "                nlp_sentence_list[i_t] = \"\"\n",
    "        else: nlp_sentence_list[i_t] = nlp_sentence_list[i_t].text\n",
    "    \n",
    "    nlp_sentence_list = [w for w in nlp_sentence_list if w]\n",
    "    #nlp_sentence = \" \".join(nlp_sentence_list)\n",
    "    \n",
    "    if autocorrect:\n",
    "        nlp_sentence = \" \".join(nlp_sentence_list)\n",
    "        nlp_sentence = (nlp_sentence.replace(\"’\", \"\\'\").replace(\"€\", \"euro\").replace(\"ç\", \"c\")\n",
    "                    .replace(\"à\", \"a\").replace(\"é\",\"e\").replace(\"ä\",\"a\").replace(\"ö\",\"o\")\n",
    "                   .replace(\"ü\",\"u\").replace(\"è\",\"e\").replace(\"¨\",\"\").replace(\"ê\",\"e\")\n",
    "                   .replace(\"â\",\"a\").replace(\"ô\",\"o\").replace(\"î\",\"i\").replace(\"û\",\"u\")\n",
    "                    .replace(\"_\",\" \").replace(\"°\",\"degree\").replace(\"§\",\"section\"))\n",
    "        nlp_sentence = corrector.correct(nlp_sentence)\n",
    "        nlp_sentence = nlp_sentence[0][\"sequence\"]\n",
    "    \n",
    "        nlp_sentence = nlp(nlp_sentence)\n",
    "        nlp_sentence_list = list(nlp_sentence)\n",
    "\n",
    "        for i_t, t in enumerate(nlp_sentence_list):\n",
    "            if t.pos_ == \"PUNCT\":\n",
    "                if i_t in [mp[0] for mp in meaningful_punct]:\n",
    "                    for mp in meaningful_punct:\n",
    "                        if i_t == mp[0]:\n",
    "                            nlp_sentence_list[mp[0]] = mp[1]\n",
    "                else: nlp_sentence_list[i_t] = ''\n",
    "\n",
    "            else:\n",
    "                nlp_sentence_list[i_t] = nlp_sentence_list[i_t].text\n",
    "\n",
    "        for mp in meaningful_punct:\n",
    "            if mp[0] < len(nlp_sentence_list):\n",
    "                if nlp_sentence_list[mp[0]] != mp[1]:\n",
    "                    nlp_sentence_list.insert(mp[0], mp[1])\n",
    "        \n",
    "    return nlp(\" \".join(nlp_sentence_list))\n",
    "\n",
    "\n",
    "#get_nlp(\"Which genre of album is harder.....faster?\", autocorrect=True)\n",
    "#get_nlp(\"Which genre of album is harder ... faster\", autocorrect=True)\n",
    "#get_nlp(\"Which home is an example of italianate architecture?\", autocorrect=True)\n",
    "#get_nlp(\"Your mom's father, were nice in the Years.!?\\'\\\":`’^!$£€\\(\\)ç*+%&/\\\\\\{\\};,àéäöüè¨êâôîû~-_<>°§...@.....\", autocorrect=True)\n",
    "#get_nlp(\"of what nationality is ken mcgoogan\", autocorrect=True)\n",
    "#get_nlp(\"you're fun\", autocorrect=True)\n",
    "#get_nlp(\"where's the fun\", autocorrect=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#questions = [ \n",
    "#    \"Which actor voiced the Unicorn in The Last Unicorn?\",\n",
    "#    \"And Alan Arkin was behind...?\",\n",
    "#    \"And Alan Arkin be behind...? Why How when which was happy make fun\",\n",
    "#    \"Who is the composer of the soundtrack?\",\n",
    "#    \"So who performed the songs?\",\n",
    "#    \"Genre of this band's music?\",\n",
    "#    \"By the way, who was the director?\"\n",
    "#            ]\n",
    "#\n",
    "#q_test = str(\"Which actor voiced the Unicorn in The Last Unicorn? \"+\n",
    "#    \"And Alan Arkin was behind...? \"+\n",
    "#    \"And Alan Arkin be behind...? Why How when which was happy make fun. \"+\n",
    "#    \"Who is the composer of the soundtrack? \"+\n",
    "#    \"So who performed songs? \"+\n",
    "#    \"Genre of this band's music? \"+\n",
    "#    \"By the way, who was the director? \")\n",
    "#\n",
    "#q_test_2 = \"Who is the wife of Barack Obama?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#q0_nlp = get_nlp(questions[0])\n",
    "#q0_nlp_test = get_nlp(q_test)\n",
    "#q0_nlp_test_2 = get_nlp(q_test_2)\n",
    "#print(q0_nlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_wd_entity(to_check):\n",
    "    pattern = re.compile('^Q[0-9]*$')\n",
    "    if pattern.match(to_check.strip()): return True\n",
    "    else: return False\n",
    "\n",
    "def is_wd_predicate(to_check):\n",
    "    pattern = re.compile('^P[0-9]*$')\n",
    "    if pattern.match(to_check.strip()): return True\n",
    "    else: return False\n",
    "    \n",
    "def is_valide_wd_id(to_check):\n",
    "    if is_wd_entity(to_check) or is_wd_predicate(to_check): return True\n",
    "    else: return False\n",
    "\n",
    "#print(is_wd_entity(\"Q155\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO redo the functions and optimize\n",
    "\n",
    "def is_entity_or_literal(wd_object):\n",
    "    if is_wd_entity(wd_object.strip()):\n",
    "        return True\n",
    "    pattern = re.compile('^[A-Za-z0-9]*$')\n",
    "    if len(wd_object) == 32 and pattern.match(wd_object.strip()):\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "# return if the given string is a literal or a date\n",
    "def is_literal_or_date (answer): \n",
    "    return not('www.wikidata.org' in answer)\n",
    "\n",
    "# return if the given string describes a year in the format YYYY\n",
    "def is_year(year):\n",
    "    pattern = re.compile('^[0-9][0-9][0-9][0-9]$')\n",
    "    if not(pattern.match(year.strip())):\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "\n",
    "# return if the given string is a date\n",
    "def is_date(date):\n",
    "    pattern = re.compile('^[0-9]+ [A-z]+ [0-9][0-9][0-9][0-9]$')\n",
    "    if not(pattern.match(date.strip())):\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "\n",
    "# return if the given string is a timestamp\n",
    "def is_timestamp(timestamp):\n",
    "    pattern = re.compile('^[0-9][0-9][0-9][0-9]-[0-9][0-9]-[0-9][0-9]T00:00:00Z')\n",
    "    if not(pattern.match(timestamp.strip())):\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "\n",
    "# convert the given month to a number\n",
    "def convert_month_to_number(month):\n",
    "    return{\n",
    "        \"january\" : \"01\",\n",
    "        \"february\" : \"02\",\n",
    "        \"march\" : \"03\",\n",
    "        \"april\" : \"04\",\n",
    "        \"may\" : \"05\",\n",
    "        \"june\" : \"06\",\n",
    "        \"july\" : \"07\",\n",
    "        \"august\" : \"08\",\n",
    "        \"september\" : \"09\", \n",
    "        \"october\" : \"10\",\n",
    "        \"november\" : \"11\",\n",
    "        \"december\" : \"12\"\n",
    "    }[month.lower()]\n",
    "\n",
    "# convert a date from the wikidata frontendstyle to timestamp style\n",
    "def convert_date_to_timestamp (date):\n",
    "    sdate = date.split(\" \")\n",
    "    # add the leading zero\n",
    "    if (len(sdate[0]) < 2):\n",
    "        sdate[0] = \"0\" + sdate[0]\n",
    "    return sdate[2] + '-' + convert_month_to_number(sdate[1]) + '-' + sdate[0] + 'T00:00:00Z'\n",
    "\n",
    "# convert a year to timestamp style\n",
    "def convert_year_to_timestamp(year):\n",
    "    return year + '-01-01T00:00:00Z'\n",
    "\n",
    "# get the wikidata id of a wikidata url\n",
    "def wikidata_url_to_wikidata_id(url):\n",
    "    if not url:\n",
    "        return False\n",
    "    if \"XMLSchema#dateTime\" in url or \"XMLSchema#decimal\" in url:\n",
    "        date = url.split(\"\\\"\", 2)[1]\n",
    "        date = date.replace(\"+\", \"\")\n",
    "        return date\n",
    "    if(is_literal_or_date(url)):\n",
    "        if is_year(url):\n",
    "            return convert_year_to_timestamp(url)\n",
    "        if is_date(url):\n",
    "            return convert_date_to_timestamp(url)\n",
    "        else:\n",
    "            url = url.replace(\"\\\"\", \"\")\n",
    "            return url\n",
    "    else:\n",
    "        url_array = url.split('/')\n",
    "        # the wikidata id is always in the last component of the id\n",
    "        return url_array[len(url_array)-1]\n",
    "    \n",
    "# fetch all statements where the given qualifier statement occurs as subject\n",
    "def get_all_statements_with_qualifier_as_subject(qualifier):\n",
    "    statements = []\n",
    "    triples, cardinality = hdt_wd.search_triples(qualifier, \"\", \"\")\n",
    "    for triple in triples:\n",
    "        sub, pre, obj = triple\n",
    "        # only consider triples with a wikidata-predicate\n",
    "        if pre.startswith(\"http://www.wikidata.org/\"):\n",
    "            statements.append({'entity': sub, 'predicate': pre, 'object': obj})\n",
    "    return statements\n",
    "\n",
    "# fetch the statement where the given qualifier statement occurs as object\n",
    "def get_statement_with_qualifier_as_object(qualifier):\n",
    "    triples, cardinality = hdt_wd.search_triples(\"\", \"\", qualifier)\n",
    "    for triple in triples:\n",
    "        sub, pre, obj = triple\n",
    "        # only consider triples with a wikidata-predicate\n",
    "        if pre.startswith(\"http://www.wikidata.org/\") and sub.startswith(\"http://www.wikidata.org/entity/Q\"):\n",
    "            return (sub, pre, obj)\n",
    "    return False\n",
    "\n",
    "# returns all statements that involve the given entity\n",
    "def get_all_statements_of_entity(entity_id):\n",
    "    # check entity pattern\n",
    "    if not is_wd_entity(entity_id.strip()):\n",
    "        return False\n",
    "    if statements_dict.get(entity_id) != None:\n",
    "        #print(\"saved statement\")\n",
    "        return statements_dict[entity_id]\n",
    "    entity = \"http://www.wikidata.org/entity/\"+entity_id\n",
    "    statements = []\n",
    "    # entity as subject\n",
    "    triples_sub, cardinality_sub = hdt_wd.search_triples(entity, \"\", \"\")\n",
    "    # entity as object\n",
    "    triples_obj, cardinality_obj = hdt_wd.search_triples(\"\", \"\", entity)\n",
    "    if cardinality_sub + cardinality_obj > 5000:\n",
    "        statements_dict[entity_id] = []\n",
    "        return []\n",
    "    # iterate through all triples in which the entity occurs as the subject\n",
    "    for triple in triples_sub:\n",
    "        sub, pre, obj = triple\n",
    "        # only consider triples with a wikidata-predicate or if it is an identifier predicate\n",
    "        if not pre.startswith(\"http://www.wikidata.org/\"):# or (wikidata_url_to_wikidata_id(pre) in identifier_predicates):\n",
    "            continue\n",
    "        # object is statement\n",
    "        if obj.startswith(\"http://www.wikidata.org/entity/statement/\"):\n",
    "            qualifier_statements = get_all_statements_with_qualifier_as_subject(obj)\n",
    "            qualifiers = []\n",
    "            for qualifier_statement in qualifier_statements:\n",
    "                if qualifier_statement['predicate'] == \"http://www.wikidata.org/prop/statement/\" + wikidata_url_to_wikidata_id(pre):\n",
    "                        obj = qualifier_statement['object']\n",
    "                elif is_entity_or_literal(wikidata_url_to_wikidata_id(qualifier_statement['object'])):\n",
    "                    qualifiers.append({\n",
    "                        \"qualifier_predicate\":{\n",
    "                            \"id\": wikidata_url_to_wikidata_id(qualifier_statement['predicate'])\n",
    "                        }, \n",
    "                        \"qualifier_object\":{\t\n",
    "                            \"id\": wikidata_url_to_wikidata_id(qualifier_statement['object'])\n",
    "                        }})\n",
    "            statements.append({'entity': {'id': wikidata_url_to_wikidata_id(sub)}, 'predicate': {'id': wikidata_url_to_wikidata_id(pre)}, 'object': {'id': wikidata_url_to_wikidata_id(obj)}, 'qualifiers': qualifiers})\n",
    "        else:\n",
    "            statements.append({'entity': {'id': wikidata_url_to_wikidata_id(sub)}, 'predicate': {'id': wikidata_url_to_wikidata_id(pre)}, 'object': {'id': wikidata_url_to_wikidata_id(obj)}, 'qualifiers': []})\n",
    "    # iterate through all triples in which the entity occurs as the object\n",
    "    for triple in triples_obj:\n",
    "        sub, pre, obj = triple\n",
    "        # only consider triples with an entity as subject and a wikidata-predicate or if it is an identifier predicate\n",
    "        if not sub.startswith(\"http://www.wikidata.org/entity/Q\"):# or not pre.startswith(\"http://www.wikidata.org/\") or wikidata_url_to_wikidata_id(pre) in identifier_predicates:\n",
    "            continue\n",
    "        if sub.startswith(\"http://www.wikidata.org/entity/statement/\"):\n",
    "            statements_with_qualifier_as_object =  get_statement_with_qualifier_as_object(sub, process)\n",
    "            # if no statement was found continue\n",
    "            if not statements_with_qualifier_as_object:\n",
    "                continue\n",
    "            main_sub, main_pred, main_obj = statements_with_qualifier_as_object\n",
    "            qualifier_statements = get_all_statements_with_qualifier_as_subject(sub)\n",
    "            qualifiers = []\n",
    "            for qualifier_statement in qualifier_statements:\n",
    "                if wikidata_url_to_wikidata_id(qualifier_statement['predicate']) == wikidata_url_to_wikidata_id(main_pred):\n",
    "                    main_obj = qualifier_statement['object']\n",
    "                elif is_entity_or_literal(wikidata_url_to_wikidata_id(qualifier_statement['object'])):\n",
    "                    qualifiers.append({\n",
    "                        \"qualifier_predicate\":{\"id\": wikidata_url_to_wikidata_id(qualifier_statement['predicate'])}, \n",
    "                        \"qualifier_object\":{\"id\": wikidata_url_to_wikidata_id(qualifier_statement['object'])}\n",
    "                    })\n",
    "            statements.append({\n",
    "                            'entity': {'id': wikidata_url_to_wikidata_id(main_sub)},\n",
    "                            'predicate': {'id': wikidata_url_to_wikidata_id(main_pred)},\n",
    "                            'object': {'id': wikidata_url_to_wikidata_id(main_obj)},\n",
    "                            'qualifiers': qualifiers\n",
    "                              })\n",
    "        else:\n",
    "            statements.append({'entity': {'id': wikidata_url_to_wikidata_id(sub)}, 'predicate': {'id': wikidata_url_to_wikidata_id(pre)}, 'object': {'id': wikidata_url_to_wikidata_id(obj)}, 'qualifiers': []})\n",
    "    # cache the data\n",
    "    statements_dict[entity_id] = statements\n",
    "    return statements\n",
    "\n",
    "#print(len(get_all_statements_of_entity(\"Q16614390\")))\n",
    "#save_cache_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wd_ids_online(name, is_predicate=False, top_k=3):\n",
    "    name = name.split('(')[0]\n",
    "    \n",
    "    if is_predicate and wd_predicate_ids_dict.get(name) != None:\n",
    "        #print(\"saved predicate online\")\n",
    "        return wd_predicate_ids_dict[name]\n",
    "    elif not is_predicate and wd_word_ids_dict.get(name) != None:\n",
    "        #print(\"saved word online\")\n",
    "        return wd_word_ids_dict[name]\n",
    "\n",
    "    request_successfull = False\n",
    "    entity_ids = \"\"\n",
    "    while not request_successfull:\n",
    "        try:\n",
    "            if is_predicate:\n",
    "                entity_ids = requests.get('https://www.wikidata.org/w/api.php?action=wbsearchentities&format=json&language=en&type=property&limit=' + str(top_k) + '&search='+name).json()\n",
    "            else:\n",
    "                entity_ids = requests.get('https://www.wikidata.org/w/api.php?action=wbsearchentities&format=json&language=en&limit=' + str(top_k) + '&search='+name).json()\n",
    "            request_successfull = True\n",
    "        except:\n",
    "            time.sleep(5)\n",
    "    results = entity_ids.get(\"search\")\n",
    "    if not results:\n",
    "        if is_predicate: wd_predicate_ids_dict[name] = \"\"\n",
    "        else: wd_word_ids_dict[name] = \"\"\n",
    "        return \"\"\n",
    "    if not len(results):\n",
    "        if is_predicate: wd_predicate_ids_dict[name] = \"\"\n",
    "        else: wd_word_ids_dict[name] = \"\"\n",
    "        return \"\"\n",
    "    res = []\n",
    "    for result in results:\n",
    "        res.append(result['id'])\n",
    "    \n",
    "    if is_predicate: wd_predicate_ids_dict[name] = res\n",
    "    else: wd_word_ids_dict[name] = res\n",
    "    \n",
    "    return res\n",
    "#print(get_wd_ids_online(\"be\", is_predicate=True, top_k=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# very computational\n",
    "def get_most_similar(word, topn=5):\n",
    "    word = nlp.vocab[str(word)]\n",
    "    queries = [w for w in word.vocab if w.is_lower == word.is_lower and w.prob >= -15]\n",
    "    by_similarity = sorted(queries, key=lambda w: word.similarity(w), reverse=True)\n",
    "    return [(w.lower_,w.similarity(word)) for w in by_similarity[:topn+1] if w.lower_ != word.lower_]\n",
    "\n",
    "#print(get_most_similar(\"voiced\", topn=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wd_ids(word, is_predicate=False, top_k=3, limit=10):\n",
    "    if is_predicate and wd_predicate_ids_dict.get(word) != None:\n",
    "        #print(\"saved predicate local\")\n",
    "        return wd_predicate_ids_dict[word]\n",
    "    elif not is_predicate and wd_word_ids_dict.get(word) != None:\n",
    "        #print(\"saved word local\")\n",
    "        return wd_word_ids_dict[word]\n",
    "    \n",
    "    language = \"en\"\n",
    "    word_formated = str(\"\\\"\"+word+\"\\\"\"+\"@\"+language)\n",
    "    to_remove = len(\"http://www.wikidata.org/entity/\")\n",
    "    t_name, card_name = hdt_wd.search_triples(\"\", \"http://schema.org/name\", word_formated, limit=top_k)\n",
    "    #print(\"names cardinality of \\\"\" + word+\"\\\": %i\" % card_name)\n",
    "    t_alt, card_alt = hdt_wd.search_triples(\"\", 'http://www.w3.org/2004/02/skos/core#altLabel', word_formated, limit=top_k)\n",
    "    #print(\"alternative names cardinality of \\\"\" + word+\"\\\": %i\" % card_alt)\n",
    "    results = list(set(\n",
    "        [t[0][to_remove:] for t in t_name if is_valide_wd_id(t[0][to_remove:])] + \n",
    "        [t[0][to_remove:] for t in t_alt if is_valide_wd_id(t[0][to_remove:])]\n",
    "           ))\n",
    "    \n",
    "    if is_predicate: results = [r for r in results if is_wd_predicate(r)]\n",
    "        \n",
    "    # cache the data\n",
    "    if is_predicate: wd_predicate_ids_dict[word] = results\n",
    "    else: wd_word_ids_dict[word] = results\n",
    "    \n",
    "    return results if limit<=0 else results[:limit]\n",
    "     \n",
    "    \n",
    "#get_wd_ids(\"The Last Unicorn\", top_k=0, limit=10)\n",
    "#print(get_wd_ids(\"wife\", is_predicate=False , top_k=0, limit=0))\n",
    "#print(get_wd_ids(\"voiced\", is_predicate=False , top_k=0, limit=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wd_label(from_id):\n",
    "    #print(\"from_id\",from_id)\n",
    "    if is_valide_wd_id(from_id):\n",
    "        if wd_labels_dict.get(from_id) != None:\n",
    "            #print(\"saved label local\")\n",
    "            return wd_labels_dict[from_id]\n",
    "        \n",
    "        language = \"en\"\n",
    "        id_url = \"http://www.wikidata.org/entity/\"+from_id\n",
    "        t_name, card_name = hdt_wd.search_triples(id_url, \"http://schema.org/name\", \"\")\n",
    "        name = [t[2].split('\\\"@en')[0].replace(\"\\\"\", \"\") for t in t_name if \"@\"+language in t[2]]\n",
    "        result = name[0] if name else ''\n",
    "        wd_labels_dict[from_id] = result #caching\n",
    "        return result\n",
    "        \n",
    "    else:\n",
    "        return ''\n",
    "    \n",
    "#print(get_wd_label(\"P725\"))\n",
    "#get_wd_label(\"Q20789322\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "# Building colors from graph\n",
    "def get_color(node_type):\n",
    "    if node_type == \"entity\": return \"violet\"#\"cornflowerblue\"\n",
    "    elif node_type == \"predicate\": return \"yellow\"\n",
    "    else: return \"red\"\n",
    "\n",
    "# Building labels for graph\n",
    "def get_elements_from_graph(graph):\n",
    "    node_names = nx.get_node_attributes(graph,\"name\")\n",
    "    node_types = nx.get_node_attributes(graph,\"type\")\n",
    "    colors = [get_color(node_types[n]) for n in node_names]\n",
    "    return node_names, colors\n",
    "\n",
    "# Plotting the graph\n",
    "def plot_graph(graph, name, title=\"Graph\"):\n",
    "    fig = plt.figure(figsize=(14,14))\n",
    "    ax = plt.subplot(111)\n",
    "    ax.set_title(str(\"answer: \"+title), fontsize=10)\n",
    "    #pos = nx.spring_layout(graph)\n",
    "    labels, colors = get_elements_from_graph(graph)\n",
    "    nx.draw(graph, node_size=30, node_color=colors, font_size=10, font_weight='bold', with_labels=True, labels=labels)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"tmqa1_graphs_imgs/\"+str(name)+\".png\", format=\"PNG\", dpi = 300)\n",
    "    plt.show()\n",
    "    \n",
    "#plot_graph(graph, \"file_name_graph\", \"Graph_title\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: handle dates and other literals, idea to start: entity != Q and objects != Q\n",
    "\n",
    "def make_statements_graph(statements, indexing_predicates=True):\n",
    "    BANNED_WD_IDS = [\n",
    "        \"Q4167410\",\"Q66087861\",\"Q65932995\",\"Q21281405\",\"Q17442446\",\"Q41770487\",\"Q29548341\",\n",
    "        \"Q29547399\",\"Q25670\"\n",
    "    ]\n",
    "    BANNED_WD_PRED_IDS = [\n",
    "        \"P1687\",\"P7087\",\"P1889\",\"P646\", \"P227\", \"P1256\", \"P1257\", \"P1258\", \"P1260\", \"P301\",\n",
    "        \"P18\",\"P1266\",\"P487\",\"P1970\",\"P2529\", \"P4390\", \"P4342\", \"P4213\", \"P487\", \"P2624\",\n",
    "        \"P4953\", \"P2241\", \"P345\", \"P703\", \"P2163\", \"P18\", \"P436\", \"P227\", \"P646\", \"P2581\",\n",
    "        \"P1006\", \"P244\", \"P214\", \"P1051\", \"P1296\", \"P461\", \"P2959\", \"P1657\", \"P3834\",\"P243\",\n",
    "        \"P3306\",\"P6932\",\"P356\",\"P1630\",\"P3303\",\"P1921\",\"P1793\",\"P1628\",\"P1184\",\"P1662\",\"P2704\",\n",
    "        \"P4793\",\"P1921\",\"P2302\"\n",
    "    ]\n",
    "    \n",
    "    graph = nx.Graph()\n",
    "    turn=0\n",
    "    predicate_nodes = {}\n",
    "\n",
    "    for statement in statements:\n",
    "        #statement['entity']['id'] in BANNED_WD_IDS \n",
    "        #statement['object']['id'] in BANNED_WD_IDS\n",
    "        #statement['predicate']['id'] in BANNED_WD_PRED_IDS\n",
    "        if (statement['entity']['id'][0] != \"Q\"\n",
    "            or statement['entity']['id'] in BANNED_WD_IDS\n",
    "            or statement['predicate']['id'][0] != \"P\"\n",
    "            or statement['predicate']['id'] in BANNED_WD_PRED_IDS\n",
    "            or statement['object']['id'][0] != \"Q\"\n",
    "            or statement['object']['id'] in BANNED_WD_IDS):\n",
    "            continue\n",
    "        \n",
    "        #print(statement)\n",
    "        if not statement['entity']['id'] in graph:\n",
    "            graph.add_node(statement['entity']['id'], name=get_wd_label(statement['entity']['id']), type='entity', turn=turn)\n",
    "        if not statement['object']['id'] in graph:\n",
    "            graph.add_node(statement['object']['id'], name=get_wd_label(statement['object']['id']), type='entity', turn=turn)\n",
    "\n",
    "        # increment index of predicate or set it at 0\n",
    "        if not statement['predicate']['id'] in predicate_nodes or not indexing_predicates:\n",
    "            predicate_nodes_index = 1\n",
    "            predicate_nodes[statement['predicate']['id']] = 1\n",
    "        else:\n",
    "            predicate_nodes[statement['predicate']['id']] += 1\n",
    "            predicate_nodes_index = predicate_nodes[statement['predicate']['id']]\n",
    "\n",
    "        # add the predicate node\n",
    "        predicate_node_id = (statement['predicate']['id'])\n",
    "        if indexing_predicates: predicate_node_id += \"-\" + str(predicate_nodes_index)\n",
    "        \n",
    "        graph.add_node(predicate_node_id, name=get_wd_label(statement['predicate']['id']), type='predicate', turn=turn)\n",
    "\n",
    "        # add the two edges (entity->predicate->object)\n",
    "        #statement['entity']['id'] in BANNED_WD_IDS \n",
    "        #statement['object']['id'] in BANNED_WD_IDS\n",
    "        #statement['predicate']['id'] in BANNED_WD_PRED_IDS\n",
    "        #if (statement['predicate']['id'] in BANNED_WD_PRED_IDS): break\n",
    "            \n",
    "        graph.add_edge(statement['entity']['id'], predicate_node_id)\n",
    "        graph.add_edge(predicate_node_id, statement['object']['id'])\n",
    "    \n",
    "    return graph, predicate_nodes\n",
    "\n",
    "#test_graph = make_statements_graph(test_unduplicate_statements, indexing_predicates=False)\n",
    "#print(test_graph[1])\n",
    "#plot_graph(test_graph[0],\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_lists(list_1, list_2):\n",
    "    if len(list_1) == len(list_2):\n",
    "        return [(list_1[i], list_2[i]) for i in range(0, len(list_1))]\n",
    "    else:\n",
    "        return \"Error: lists are not the same lenght\"\n",
    "\n",
    "#print(merge_lists([\"author\"],['P50']))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_themes(nlp_question, top_k=3):\n",
    "    # PART1: finding themes as the user typed it\n",
    "    filter_list = [\"PART\", \"PRON\", \"NUM\"]\n",
    "    nlp_list_src = list(nlp_question)\n",
    "    nlp_list = []\n",
    "    for w in nlp_question:\n",
    "        if w.pos_ not in filter_list:\n",
    "            nlp_list.append(w)\n",
    "    nlp_question = get_nlp(\" \".join([e.text for e in nlp_list]))\n",
    "    \n",
    "    themes = [(ent, [ent.kb_id_]) for ent in get_kb_ents(nlp_question.text) if ent.kb_id_ != \"NIL\"]\n",
    "    theme_complements = []\n",
    "    \n",
    "    noun_chunks = [chunk for chunk in nlp_question.noun_chunks]\n",
    "    theme_ids = [get_wd_ids(chunk.text, top_k=top_k) for chunk in noun_chunks]\n",
    "\n",
    "    for i, chunk in enumerate(theme_ids):\n",
    "        if chunk: themes.append((noun_chunks[i], chunk))\n",
    "        else: theme_complements.append(noun_chunks[i])\n",
    "    \n",
    "    # PART2: finding themes with the question capitalized\n",
    "    #print(nlp_question)\n",
    "    nlp_list_cap = []\n",
    "    nlp_list_low = []\n",
    "    nlp_list_lemma = []\n",
    "    nlp_list_no_det = []\n",
    "    w_filter = [\"WDT\",\"WP\",\"WP$\",\"WRB\"]\n",
    "    for w in nlp_question:\n",
    "        if w.tag_ not in w_filter:\n",
    "            nlp_list_cap.append(w.text.capitalize())\n",
    "            nlp_list_low.append(w.text.lower())\n",
    "            nlp_list_lemma.append(w.lemma_)\n",
    "        if w.pos_ == \"DET\":\n",
    "            nlp_list_no_det.append(w.text)\n",
    "            \n",
    "    nlp_question_cap = get_nlp(\" \".join([e for e in nlp_list_cap]))\n",
    "    nlp_question_low = get_nlp(\" \".join([e for e in nlp_list_low]))\n",
    "    nlp_question_lemma = get_nlp(\" \".join([e for e in nlp_list_lemma]))\n",
    "    nlp_question_no_det = get_nlp(\" \".join([e for e in nlp_list_no_det]))\n",
    "\n",
    "    themes += [(ent, [ent.kb_id_]) for ent in get_kb_ents(nlp_question_cap.text) if ent.kb_id_ != \"NIL\" and (ent, [ent.kb_id_]) not in themes]\n",
    "    themes += [(ent, [ent.kb_id_]) for ent in get_kb_ents(nlp_question_low.text) if ent.kb_id_ != \"NIL\" and (ent, [ent.kb_id_]) not in themes]\n",
    "    themes += [(ent, [ent.kb_id_]) for ent in get_kb_ents(nlp_question_lemma.text) if ent.kb_id_ != \"NIL\" and (ent, [ent.kb_id_]) not in themes]\n",
    "    themes += [(ent, [ent.kb_id_]) for ent in get_kb_ents(nlp_question_no_det.text) if ent.kb_id_ != \"NIL\" and (ent, [ent.kb_id_]) not in themes]\n",
    "    \n",
    "    #print(themes)\n",
    "    noun_chunks = []\n",
    "    \n",
    "    previous_title_position = 0\n",
    "    for i_t,t in enumerate(nlp_question):\n",
    "        tmp_row = []\n",
    "        if i_t > previous_title_position:\n",
    "            if t.is_title:\n",
    "                for i_p in range(previous_title_position,i_t+1):\n",
    "                    tmp_row.append(nlp_question[i_p])\n",
    "\n",
    "                noun_chunks.append(get_nlp(\" \".join([w.text for w in tmp_row])))\n",
    "\n",
    "        if t.is_title:\n",
    "            previous_title_position = i_t\n",
    "    \n",
    "    noun_chunks += [chunk for chunk in nlp_question_cap.noun_chunks]\n",
    "    \n",
    "    theme_ids = [get_wd_ids(chunk.text, top_k=top_k) for chunk in noun_chunks]\n",
    "\n",
    "    for i, chunk in enumerate(theme_ids):\n",
    "        if chunk: themes.append((noun_chunks[i], chunk))\n",
    "        else: theme_complements.append(noun_chunks[i])\n",
    "    \n",
    "    noun_chunks = [chunk for chunk in nlp_question_low.noun_chunks]\n",
    "    theme_ids = [get_wd_ids(chunk.text, top_k=top_k) for chunk in noun_chunks]\n",
    "\n",
    "    for i, chunk in enumerate(theme_ids):\n",
    "        if chunk: themes.append((noun_chunks[i], chunk))\n",
    "        else: theme_complements.append(noun_chunks[i])\n",
    "            \n",
    "    noun_chunks = [chunk for chunk in nlp_question_lemma.noun_chunks]\n",
    "    theme_ids = [get_wd_ids(chunk.text, top_k=top_k) for chunk in noun_chunks]\n",
    "\n",
    "    for i, chunk in enumerate(theme_ids):\n",
    "        if chunk: themes.append((noun_chunks[i], chunk))\n",
    "        else: theme_complements.append(noun_chunks[i])\n",
    "            \n",
    "    noun_chunks = [chunk for chunk in nlp_question_no_det.noun_chunks]\n",
    "    theme_ids = [get_wd_ids(chunk.text, top_k=top_k) for chunk in noun_chunks]\n",
    "\n",
    "    for i, chunk in enumerate(theme_ids):\n",
    "        if chunk: themes.append((noun_chunks[i], chunk))\n",
    "        else: theme_complements.append(noun_chunks[i])\n",
    "    \n",
    "    themes_filtered = []\n",
    "    for t in themes:\n",
    "        if t[0].text in [tf[0].text for tf in themes_filtered]:\n",
    "            index = [tf[0].text for tf in themes_filtered].index(t[0].text)\n",
    "            tmp = t[1]+[i for j in [tf[1] for index, tf in enumerate(themes_filtered) if tf[0].text == t[0].text] for i in j]\n",
    "            themes_filtered[index] = (t[0],tmp)\n",
    "\n",
    "        else:\n",
    "            themes_filtered.append(t)\n",
    "            \n",
    "    themes_filtered_undupped = []\n",
    "    for tf in themes_filtered:\n",
    "        tmp_ids = []\n",
    "        for tfid in tf[1]:\n",
    "            if tfid not in tmp_ids:\n",
    "                tmp_ids.append(tfid)\n",
    "        themes_filtered_undupped.append((tf[0],tmp_ids))\n",
    "        \n",
    "    theme_complements_undupped = []\n",
    "    [theme_complements_undupped.append(tc) for tc in theme_complements if tc.text not in [tcu.text for tcu in theme_complements_undupped]]\n",
    "    \n",
    "    #print(themes_filtered)\n",
    "    return themes_filtered_undupped, theme_complements_undupped\n",
    "\n",
    "#q0_themes = get_themes(q0_nlp, top_k=3)\n",
    "#q0_themes_test = get_themes(q0_nlp_test)\n",
    "#q0_themes_test_2 = get_themes(q0_nlp_test_2)\n",
    "#print(q0_themes)\n",
    "\n",
    "#q_test_3 = get_nlp(\"the unicorn and the raccoons love obama barack's tacos\")\n",
    "#q_test_3_themes = get_themes(q_test_3, top_k=3)\n",
    "#print(get_enhanced_themes(q_test_3_themes))\n",
    "#print(q_test_3_themes)\n",
    "\n",
    "#q_test_test = get_nlp(\"Who voiced the Unicorn in The Last Unicorn\")\n",
    "#q_test_test = get_nlp(\"What is the name of the person who created Saved by the Bell?\")\n",
    "#get_themes(q_test_test, top_k=3)\n",
    "\n",
    "#q_themes: ([(Unicorn, ['Q7246', 'Q7246', 'Q7246']), \n",
    "#(The Last Unicorn, ['Q30060419', 'Q16614390', 'Q176198', 'Q15628943', 'Q30060419', 'Q16614390', 'Q176198', 'Q15628943']), \n",
    "#(The Unicorn, ['Q17985004', 'Q18647334', 'Q17553756', 'Q65070436'])], \n",
    "#[the Unicorn, the unicorn, the last unicorn, the Unicorn, the last Unicorn])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "BANNED_WORDS = [\"...\"]\n",
    "\n",
    "def get_theme_tuples(theme_list, top_k=3):\n",
    "    return [(t, get_wd_ids(t, top_k=top_k)) for t in theme_list if t not in BANNED_WORDS]\n",
    "\n",
    "def get_theme_no_stopwords(theme_list):\n",
    "    return [s for s in theme_list if not s.is_stop]\n",
    "\n",
    "def get_theme_lemmatized(theme_list):\n",
    "    return [s.lemma_ for s in theme_list]\n",
    "\n",
    "def get_permutation_tuples(theme_list, start=2):\n",
    "    permutations = []\n",
    "    for i in range(start, len(theme_list)+1):\n",
    "        permutations += itertools.permutations(theme_list,i)\n",
    "    return permutations\n",
    "\n",
    "def get_lemma_permutation_tuples(theme_list, start=2):\n",
    "    return get_permutation_tuples(get_theme_lemmatized(theme_list), start=2)\n",
    "\n",
    "def get_non_token_tuples(theme_list):\n",
    "    return [\" \".join([e for e in list(l)]) for l in theme_list]\n",
    "\n",
    "def get_non_token_lower_tuples(theme_list):\n",
    "    return [\" \".join([e.lower() for e in list(l)]) for l in theme_list]\n",
    "\n",
    "def get_non_token_capitalize_tuples(theme_list):\n",
    "    return [\" \".join([c.capitalize() for c in [e for e in list(l)]]) for l in theme_list] \n",
    "\n",
    "def get_text_tuples(theme_list):\n",
    "    return [\" \".join([e.text for e in list(l)]) for l in theme_list]\n",
    "\n",
    "def get_lower_tuples(theme_list):\n",
    "    return [\" \".join([e.lower_ for e in list(l)]) for l in theme_list]\n",
    "\n",
    "def get_capitalized_tuples(theme_list):\n",
    "    return [\" \".join([c.capitalize() for c in [e.text for e in list(l)]]) for l in theme_list]\n",
    "\n",
    "def get_enhanced_themes(themes, top_k=3, aggressive=False):\n",
    "    if aggressive: top_k+=1\n",
    "    enhanced_themes = []\n",
    "    # permute, capitalize, lowering of the words in the complements\n",
    "    for c in themes[1]:\n",
    "        per_lemma = get_theme_tuples(get_non_token_tuples([n for n in get_permutation_tuples(get_theme_lemmatized(c))]),top_k)\n",
    "        per_nostop = get_theme_tuples(get_text_tuples(get_permutation_tuples(get_theme_no_stopwords(c),start=1)),top_k)\n",
    "        per_lemma_nostop = get_theme_tuples(get_non_token_tuples([get_theme_lemmatized(s) for s in get_permutation_tuples(get_theme_no_stopwords(c),start=1)]),top_k)\n",
    "\n",
    "        per_lemma_lower = get_theme_tuples(get_non_token_lower_tuples([n for n in get_permutation_tuples(get_theme_lemmatized(c))]),top_k)\n",
    "        per_nostop_lower = get_theme_tuples(get_lower_tuples(get_permutation_tuples(get_theme_no_stopwords(c),start=1)),top_k)\n",
    "        per_lemma_nostop_lower = get_theme_tuples(get_non_token_lower_tuples([get_theme_lemmatized(s) for s in get_permutation_tuples(get_theme_no_stopwords(c),start=1)]),top_k)\n",
    "\n",
    "        per_lemma_capitalize = get_theme_tuples(get_non_token_capitalize_tuples([n for n in get_permutation_tuples(get_theme_lemmatized(c))]),top_k)\n",
    "        per_nostop_capitalize = get_theme_tuples(get_capitalized_tuples(get_permutation_tuples(get_theme_no_stopwords(c),start=1)),top_k)\n",
    "        per_lemma_nostop_capitalize = get_theme_tuples(get_non_token_capitalize_tuples([get_theme_lemmatized(s) for s in get_permutation_tuples(get_theme_no_stopwords(c),start=1)]),top_k)\n",
    "\n",
    "        per = get_theme_tuples(get_text_tuples(get_permutation_tuples(c)),top_k)\n",
    "        per_lower = get_theme_tuples(get_lower_tuples(get_permutation_tuples(c)),top_k)\n",
    "        per_capitalize = get_theme_tuples(get_capitalized_tuples(get_permutation_tuples(c)),top_k)\n",
    "\n",
    "        for p in (per + per_lower + per_capitalize +\n",
    "                 per_lemma + per_lemma_lower + per_lemma_capitalize +\n",
    "                 per_nostop + per_nostop_lower + per_nostop_capitalize +\n",
    "                 per_lemma_nostop + per_lemma_nostop_lower + per_lemma_nostop_capitalize):\n",
    "            if p[1] and p not in enhanced_themes: enhanced_themes.append(p)\n",
    "    \n",
    "    if aggressive:\n",
    "        predicates = []\n",
    "        [predicates.append(get_wd_label(pred)) for pred in sum([p[1] for p in themes[0]],[]) if get_wd_label(pred) not in predicates]\n",
    "        predicates_ids = [get_wd_ids_online(p, is_predicate=True, top_k=top_k) for p in predicates]\n",
    "        predicated_themes = merge_lists(predicates, predicates_ids)\n",
    "        predicated_themes = [pt for pt in predicated_themes if pt[1] != '']\n",
    "        if predicates: enhanced_themes += predicated_themes\n",
    "            \n",
    "    enhanced_themes_filtered = []\n",
    "    for et in enhanced_themes:\n",
    "        #print(et[0],[t[0].text for t in themes[0]],et[0] in [t[0].text for t in themes[0]])\n",
    "        if not et[0] in [t[0].text for t in themes[0]]:\n",
    "            enhanced_themes_filtered.append(et)\n",
    "    \n",
    "    return enhanced_themes_filtered\n",
    "\n",
    "#q_test_3 = get_nlp(\"Which genre of album is harder.....faster?\",autocorrect=True)\n",
    "#q_test_3 = get_nlp(\"the unicorn and the raccoons love obama barack's tacos\")\n",
    "#q_test_3 = get_nlp(\"what was the cause of death of yves klein\")\n",
    "#q_test_3_themes = get_themes(q_test_3, top_k=3)\n",
    "#print(q_test_3_themes[0])\n",
    "#print(get_enhanced_themes(q_test_3_themes, aggressive=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(did, ['P577', 'P356']), (die, ['P569', 'P570'])]\n",
      "it was: 0.020180940628051758\n"
     ]
    }
   ],
   "source": [
    "def get_predicates_online(nlp_sentence, top_k=3, aggressive=False):\n",
    "    PASSIVE_VERBS = [\"be\"]\n",
    "    AGRESSIVE_FILTER = [\"VERB\",\"AUX\",\"NOUN\",\"ADJ\"]\n",
    "    if aggressive: predicates = [p for p in nlp_sentence if p.pos_ in AGRESSIVE_FILTER]\n",
    "    else: predicates = [p for p in nlp_sentence if p.pos_ == \"VERB\" or p.pos_ == \"AUX\"]\n",
    "\n",
    "    if len(predicates) == 1:\n",
    "        if predicates[0].lemma_ in PASSIVE_VERBS:\n",
    "            predicates += [p for p in nlp_sentence if p.pos_ in AGRESSIVE_FILTER if p not in predicates]\n",
    "    \n",
    "    predicates_filtered = []\n",
    "    for p in predicates:\n",
    "        if p.lemma_ in PASSIVE_VERBS: \n",
    "            p = get_nlp(p.lemma_)[0]\n",
    "        if len(predicates_filtered) == 0:\n",
    "            predicates_filtered.append(p)\n",
    "        if p.text not in [p.text for p in predicates_filtered]:\n",
    "            predicates_filtered.append(p)\n",
    "    \n",
    "    predicates_ids = []\n",
    "    for i_p, p in enumerate(predicates_filtered):\n",
    "        if p.lemma_ == \"be\":\n",
    "            predicates_ids.append(get_wd_ids_online(\"is\", is_predicate=True, top_k=top_k)[:1])\n",
    "        else:\n",
    "            p_id = get_wd_ids_online(p.text, is_predicate=True, top_k=top_k)\n",
    "            if not p_id:\n",
    "                p_id = get_wd_ids_online(p.lemma_, is_predicate=True, top_k=top_k)\n",
    "                if not p_id:\n",
    "                    similar_words = [w[0] for w in get_most_similar(p.lemma_, topn=top_k)]\n",
    "                    for sw in similar_words:\n",
    "                        if not p_id:\n",
    "                            p_id = get_wd_ids_online(sw, is_predicate=True, top_k=top_k)\n",
    "            predicates_ids.append(p_id[:top_k])\n",
    "    \n",
    "    return merge_lists(predicates_filtered, predicates_ids)\n",
    "\n",
    "#q_test = get_nlp(\"Who voiced the Unicorn in The Last Unicorn\")\n",
    "#q_test = get_nlp(\"Of what nationality is Ken McGoogan\")\n",
    "#q_test = get_nlp(\"Which have the nation of Martha Mattox\")\n",
    "#q_test = get_nlp(\"what city was alex golfis born in\")\n",
    "#q_test = get_nlp(\"who's born in city was alex golfis born in\")\n",
    "#q_test = get_nlp(\"what's the name fo the wife of my dads\")\n",
    "#start_time = time.time()\n",
    "#q_test = get_nlp(\"Where did roger marquis die\")\n",
    "#print(get_predicates_online(q_test, top_k=2, aggressive=False))\n",
    "#print(\"it was:\",time.time()-start_time)\n",
    "#q0_predicates_test_2 = get_predicates_online(q0_nlp_test_2, top_k=3, aggressive=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get_nlp(\"was\").similarity(get_nlp(\"instance of\"))\n",
    "#get_wd_ids_online(\"do\", is_predicate=True, top_k=3)\n",
    "#get_nlp(\"do\")[0].lemma_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predicates(nlp_sentence, themes=False, top_k=0):\n",
    "    PASSIVE_VERBS = [\"be\"]\n",
    "    predicates = [p for p in nlp_sentence if p.pos_ == \"VERB\" or p.pos_ == \"AUX\"]\n",
    "    #for i_p, p in enumerate(predicates):\n",
    "    #    if p.text == \"\\'s\":\n",
    "    #        predicates[i_p] = get_nlp(\"is\")[0]\n",
    "    #    if p.text == \"\\'re\":\n",
    "    #        predicates[i_p] = get_nlp(\"are\")[0]\n",
    "            \n",
    "    if themes:\n",
    "        for t in themes[0]:\n",
    "            for e in t[1]:\n",
    "                if is_wd_predicate(e):\n",
    "                    predicates.append(t[0])\n",
    "    \n",
    "    predicates_filtered = []\n",
    "    for p in predicates:\n",
    "        if p.lemma_ in PASSIVE_VERBS: \n",
    "            p = get_nlp(p.lemma_)[0]\n",
    "        if len(predicates_filtered) == 0:\n",
    "            predicates_filtered.append(p)\n",
    "        if p.text not in [p.text for p in predicates_filtered]:\n",
    "            predicates_filtered.append(p)\n",
    "            \n",
    "    predicates_ids = []\n",
    "    for i_p, p in enumerate(predicates_filtered):\n",
    "        if p.lemma_ in PASSIVE_VERBS: \n",
    "            predicates_ids.append(get_wd_ids(p.lemma_, is_predicate=True, top_k=top_k, limit=0)[:1])\n",
    "        else:\n",
    "            predicates_ids.append(get_wd_ids(p.text, is_predicate=True, top_k=top_k, limit=0))\n",
    "                \n",
    "    #predicates_ids = [ for p in predicates_filtered]\n",
    "    return merge_lists(predicates_filtered, predicates_ids)\n",
    "\n",
    "#q_test = get_nlp(\"Who voiced the Unicorn in The Last Unicorn\")\n",
    "#q_test = get_nlp(\"Of what nationality is Ken McGoogan\")\n",
    "#q_test = get_nlp(\"Where did roger marquis die\")\n",
    "#q_test = get_nlp(\"who's born in city was alex golfis born in\")\n",
    "#get_predicates(q_test)\n",
    "#q_test_themes = get_themes(q_test)\n",
    "#get_predicates(q_test, q_test_themes, top_k=3)\n",
    "#q0_nlp_test_0 = get_nlp(\"Voiced\")\n",
    "#q0_predicates = get_predicates(q0_nlp, top_k=3)\n",
    "#q0_predicates_test_2 = get_predicates(q0_nlp_test_2, top_k=3)\n",
    "#print(q0_predicates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_ids(to_extract):\n",
    "    return [i for i in itertools.chain.from_iterable([id[1] for id in to_extract])]\n",
    "#extract_ids([('name', ['id'])]) #q0_themes[0] #q0_focused_parts #q0_predicates\n",
    "#print(extract_ids([(\"The Last Unicorn\", ['Q16614390']),(\"Second Theme\", ['Q12345'])]))\n",
    "#extract_ids(q0_focused_parts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_similarity_by_words(nlp_word_from, nlp_word_to):\n",
    "    if not nlp_word_from or not nlp_word_to:\n",
    "        return 0\n",
    "    elif not nlp_word_from.vector_norm or not nlp_word_to.vector_norm:\n",
    "        return 0\n",
    "    else:\n",
    "        return nlp_word_from.similarity(nlp_word_to)\n",
    "\n",
    "#print(get_similarity_by_words(get_nlp(\"character role\"), get_nlp(\"voice actor\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_similarity_by_ids(word_id_from, word_id_to):\n",
    "    nlp_word_from = get_nlp(get_wd_label(word_id_from))\n",
    "    nlp_word_to = get_nlp(get_wd_label(word_id_to))\n",
    "    return get_similarity_by_words(nlp_word_from, nlp_word_to)\n",
    "\n",
    "#print(get_similarity_by_ids(\"P453\", \"P725\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_similar_statements(statements, from_token_id, similar_to_name, top_k=3, qualifier=False, statement_type=\"object\"):\n",
    "    highest_matching_similarity = -1\n",
    "    top_statements = []\n",
    "    nlp_name = get_nlp(similar_to_name)\n",
    "    \n",
    "    if get_wd_label(from_token_id):\n",
    "        for statement in statements:\n",
    "            if qualifier:\n",
    "                if statement.get('qualifiers'):\n",
    "                    for qualifier in statement['qualifiers']:\n",
    "                        nlp_word_to = get_nlp(get_wd_label(qualifier[statement_type]['id']))\n",
    "                        matching_similarity = get_similarity_by_words(nlp_name, nlp_word_to)\n",
    "                        if highest_matching_similarity == -1 or matching_similarity > highest_matching_similarity:\n",
    "                            highest_matching_similarity = matching_similarity\n",
    "                            best_statement = statement\n",
    "                            top_statements.append((highest_matching_similarity, best_statement))\n",
    "            else:\n",
    "                nlp_word_to = get_nlp(get_wd_label(statement[statement_type]['id']))\n",
    "                matching_similarity = get_similarity_by_words(nlp_name, nlp_word_to)\n",
    "                if highest_matching_similarity == -1 or matching_similarity > highest_matching_similarity:\n",
    "                    highest_matching_similarity = matching_similarity\n",
    "                    best_statement = statement\n",
    "                    top_statements.append((highest_matching_similarity, best_statement))\n",
    "            \n",
    "    return sorted(top_statements, key=lambda x: x[0], reverse=True)[:top_k]\n",
    "\n",
    "#statements = get_all_statements_of_entity('Q176198')\n",
    "#top_similar_statements = get_top_similar_statements(statements, 'Q176198', 'voiced')\n",
    "#print(top_similar_statements[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_similar_statements_by_word(from_token_ids, similar_to_name, top_k=3, qualifier=False, statement_type=\"object\"):\n",
    "    best_statements = []\n",
    "    for token in from_token_ids:\n",
    "        statements = get_all_statements_of_entity(token)\n",
    "        if statements: best_statements += get_top_similar_statements(statements, token, similar_to_name, top_k=top_k, qualifier=qualifier, statement_type=statement_type)\n",
    "\n",
    "    return sorted(best_statements, key=lambda x: x[0], reverse=True)[:top_k]\n",
    "\n",
    "#best_similar_statements = get_best_similar_statements_by_word(extract_ids(q0_themes[0]), 'voiced', top_k=3, qualifier=True, statement_type=\"qualifier_object\")\n",
    "#print(best_similar_statements[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_statements_subjects_labels(statements):\n",
    "    return [get_wd_label(t[1]['entity']['id']) for t in statements]\n",
    "#print(get_statements_subjects_labels(best_similar_statements))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_statements_predicates_labels(statements):\n",
    "    return [get_wd_label(t[1]['predicate']['id']) for t in statements]\n",
    "#print(get_statements_predicates_labels(best_similar_statements))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_statements_objects_labels(statements):\n",
    "    return [get_wd_label(t[1]['object']['id']) for t in statements]\n",
    "#print(get_statements_objects_labels(best_similar_statements))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_statements_qualifier_predicates_labels(statements):\n",
    "    return [get_wd_label(t[1]['qualifiers'][0]['qualifier_predicate']['id']) for t in statements]\n",
    "#print(get_statements_qualifier_predicates_labels(best_similar_statements))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_statements_qualifier_objects_labels(statements):\n",
    "    return [get_wd_label(t[1]['qualifiers'][0]['qualifier_object']['id']) for t in statements]\n",
    "#print(get_statements_qualifier_objects_labels(best_similar_statements))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_extend_by_words(cluster_root_ids, extending_words, top_k=3):\n",
    "    cluster = []\n",
    "    #start_time = time.time()\n",
    "    \n",
    "    for name in extending_words:\n",
    "        #start_cluster_time = time.time()\n",
    "        cluster += get_best_similar_statements_by_word(cluster_root_ids, name, top_k=top_k, qualifier=True, statement_type=\"qualifier_predicate\")\n",
    "        cluster += get_best_similar_statements_by_word(cluster_root_ids, name, top_k=top_k, qualifier=True, statement_type=\"qualifier_object\")\n",
    "        cluster += get_best_similar_statements_by_word(cluster_root_ids, name, top_k=top_k, qualifier=False, statement_type=\"predicate\")\n",
    "        cluster += get_best_similar_statements_by_word(cluster_root_ids, name, top_k=top_k, qualifier=False, statement_type=\"object\")\n",
    "        #end_time = time.time()\n",
    "        #print(\"EXTENDING Cluster with:\", name,\" ->\\tRunning time is {}s\".format(round(end_time-start_cluster_time,2)))\n",
    "    #end_time = time.time()\n",
    "    #print(\"EXTENDING Clusters ->\\tRunning time is {}s\".format(round(end_time-start_time,2)))\n",
    "    return cluster\n",
    "    \n",
    "#test_cluster = cluster_extend_by_words(extract_ids(q0_themes[0]), ['voiced'], top_k=2)\n",
    "#test_cluster_test_2 = cluster_extend_by_words(extract_ids(q0_themes_test_2[0]), ['birth'], top_k=2)\n",
    "#print(test_cluster[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sorts by the similarity value of statements[0]\n",
    "def sort_statements_by_similarity(statements):\n",
    "    return [s for s in sorted(statements, key=lambda x: x[0], reverse=True)]\n",
    "\n",
    "#test_sorted_statements = sort_statements_by_similarity(test_cluster)\n",
    "#test_sorted_statements_test_2 = sort_statements_by_similarity(test_cluster_test_2)\n",
    "#print(test_sorted_statements[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# appends spo from qualifiers, removes qualifier tags, and removes similarity scores\n",
    "def statements_flatter(statements):\n",
    "    best_statements_to_graph = []\n",
    "    for statement in statements:\n",
    "        tmp_statement = copy(statement)\n",
    "        if tmp_statement.get('qualifiers'):\n",
    "            #print(\"statement\", statement)\n",
    "            for q in tmp_statement['qualifiers']:\n",
    "                qualifier_statement = {'entity': {'id': tmp_statement['entity']['id']}}\n",
    "                qualifier_statement['predicate'] = {'id': q['qualifier_predicate']['id']}\n",
    "                qualifier_statement['object'] = {'id': q['qualifier_object']['id']}\n",
    "                best_statements_to_graph.append(qualifier_statement)\n",
    "            del(tmp_statement['qualifiers'])\n",
    "        else: \n",
    "            #print(\"tmp_statement\", tmp_statement)\n",
    "            if ('qualifiers' in tmp_statement): del(tmp_statement['qualifiers'])\n",
    "        if tmp_statement not in best_statements_to_graph:\n",
    "            #print(\"best_statements_to_graph\", tmp_statement)\n",
    "            best_statements_to_graph.append(tmp_statement)\n",
    "    return best_statements_to_graph\n",
    "\n",
    "#test_flatten_statements = statements_flatter([s[1] for s in test_sorted_statements])\n",
    "#test_flatten_statements_test_2 = statements_flatter([s[1] for s in test_sorted_statements_test_2])\n",
    "#print(test_flatten_statements[0])\n",
    "#test_flatten_statements_test_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove duplicates from statements\n",
    "def unduplicate_statements(statements):\n",
    "    filtered_statements = []\n",
    "    [filtered_statements.append(s) for s in statements if s not in [e for e in filtered_statements]]\n",
    "    return filtered_statements\n",
    "\n",
    "#test_unduplicate_statements = unduplicate_statements(test_flatten_statements)\n",
    "#print(len(test_flatten_statements))\n",
    "#print(len(test_unduplicate_statements))\n",
    "#print(test_unduplicate_statements[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#test_graph = make_statements_graph(test_unduplicate_statements)\n",
    "#print(test_graph[1])\n",
    "#plot_graph(test_graph[0], \"file_name_graph\", \"Graph_title\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_statements_by_id(statements, from_token_id, to_id, qualifier=False, statement_type=\"predicate\"):\n",
    "    id_statements = []\n",
    "    if not statements:\n",
    "        return id_statements\n",
    "    if get_wd_label(from_token_id):\n",
    "        for statement in statements:\n",
    "            if qualifier:\n",
    "                if statement.get('qualifiers'):\n",
    "                    for s in statement['qualifiers']:\n",
    "                        if to_id == s[statement_type]['id']:\n",
    "                            id_statements.append(statement)\n",
    "            else:\n",
    "                if to_id == statement[statement_type]['id']:\n",
    "                    id_statements.append(statement)\n",
    "    \n",
    "    return id_statements\n",
    "\n",
    "#statements_test = get_all_statements_of_entity('Q176198')\n",
    "#id_statements_test = get_statements_by_id(statements_test, 'Q176198', 'P725')\n",
    "#print(id_statements_test[0])\n",
    "\n",
    "#get_statements_by_id(root_statements, cluster_root_id, predicate_id, qualifier=False, statement_type=\"predicate\")\n",
    "#statements_test = get_all_statements_of_entity('Q176198')\n",
    "#id_statements_test = get_statements_by_id(statements_test, 'Q176198', 'P725')\n",
    "#id_statements_test[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters\n",
    "# cluster_root_ids: ['Qcode']\n",
    "# predicates_ids: ['Pcode']\n",
    "def cluster_extend_by_predicates_ids(cluster_root_ids, predicates_ids):\n",
    "    cluster = []\n",
    "    \n",
    "    for cluster_root_id in cluster_root_ids:\n",
    "        root_statements = get_all_statements_of_entity(cluster_root_id)\n",
    "        #print(\"root_statements\", root_statements)\n",
    "        for predicate_id in predicates_ids:\n",
    "            cluster += get_statements_by_id(root_statements, cluster_root_id, predicate_id, qualifier=True, statement_type=\"qualifier_predicate\")\n",
    "            cluster += get_statements_by_id(root_statements, cluster_root_id, predicate_id, qualifier=False, statement_type=\"predicate\")\n",
    "\n",
    "    return cluster\n",
    "    \n",
    "#test_predicate_clusters = cluster_extend_by_predicates_ids(extract_ids(q0_themes[0]), extract_ids(q0_predicates))\n",
    "#print(len(test_predicate_clusters))\n",
    "#test_predicate_clusters[0]\n",
    "\n",
    "#test_predicate_clusters_test_2 = cluster_extend_by_predicates_ids(extract_ids(q0_themes_test_2[0]), extract_ids(q0_predicates_test_2))\n",
    "#print(len(test_predicate_clusters_test_2))\n",
    "#print(test_predicate_clusters_test_2[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_extractor_from_complements(complements):\n",
    "    for c in complements:\n",
    "        [print(t.pos_) for t in c]\n",
    "    return complements\n",
    "\n",
    "#print(cluster_extractor_from_complements(q0_themes[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#TODO: add cache\n",
    "#TODO: Check if extending with predicate_ids is useful\n",
    "# parameter\n",
    "# question: nlp_string\n",
    "#limits=plt.axis('off')\n",
    "def build_graph(nlp, themes, themes_enhanced, predicates, deep_k=10):\n",
    "    #start_time = time.time()\n",
    "    theme_ids = extract_ids(themes[0])\n",
    "    theme_enhanced_ids = extract_ids(themes_enhanced)\n",
    "    predicates_ids = extract_ids(predicates)\n",
    "    predicates_enhanced_ids = [p for p in theme_enhanced_ids if is_wd_predicate(p)]\n",
    "    predicates_enhanced = merge_lists([get_nlp(get_wd_label(p)) for p in predicates_enhanced_ids], predicates_enhanced_ids)\n",
    "    \n",
    "    #print(theme_ids)\n",
    "    #print(theme_enhanced_ids)\n",
    "    for i, tei in enumerate(theme_enhanced_ids):\n",
    "        if tei in theme_ids:\n",
    "            tmp = theme_enhanced_ids.pop(i)\n",
    "    \n",
    "    init_clusters = cluster_extend_by_words(theme_ids, [p[0].text for p in predicates+predicates_enhanced], top_k=deep_k)\n",
    "    #print(\"init_clusters\",len(init_clusters))\n",
    "    init_clusters_enhanced = cluster_extend_by_words(theme_enhanced_ids, [p[0].text for p in predicates+predicates_enhanced], top_k=deep_k)\n",
    "    #print(\"init_clusters_enhanced\",len(init_clusters_enhanced))\n",
    "    init_sorted_statements = sort_statements_by_similarity(init_clusters + init_clusters_enhanced)\n",
    "    #print(\"init_sorted_statements\",len(init_sorted_statements))\n",
    "    init_flatten_statements = statements_flatter([s[1] for s in init_sorted_statements])\n",
    "    #print(\"init_flatten_statements\",len(init_flatten_statements))\n",
    "    \n",
    "    predicate_ids_clusters = cluster_extend_by_predicates_ids(theme_ids, predicates_ids+predicates_enhanced_ids)\n",
    "    #print(\"predicate_ids_clusters\",len(predicate_ids_clusters))\n",
    "    predicate_ids_enhanced_clusters = cluster_extend_by_predicates_ids(theme_enhanced_ids, predicates_ids+predicates_enhanced_ids)\n",
    "    #print(\"predicate_ids_enhanced_clusters\",len(predicate_ids_enhanced_clusters))\n",
    "    predicate_ids_flatten_statements = statements_flatter(predicate_ids_clusters+predicate_ids_enhanced_clusters)\n",
    "    #print(\"predicate_ids_flatten_statements\",len(predicate_ids_flatten_statements))\n",
    "    \n",
    "    clusters = init_flatten_statements+predicate_ids_flatten_statements\n",
    "    filtered_statements = unduplicate_statements(clusters)\n",
    "    #print(predicate_ids_enhanced_clusters)\n",
    "    graph = make_statements_graph(filtered_statements)\n",
    "\n",
    "    ##print(\"clusters:\", len(clusters))\n",
    "    ##print(\"filtered_statements:\", len(filtered_statements))\n",
    "    #end_time = time.time()\n",
    "    #print(\"->\\tRunning time is {}s\".format(round(end_time-start_time,2)))\n",
    "    \n",
    "    return graph\n",
    "\n",
    "#q0_test = questions[0]\n",
    "#q0_test = \"Which actor voiced the Unicorn in The Last Unicorn?\"\n",
    "#q0_test = \"what was the cause of death of yves klein\"\n",
    "#q0_test = \"Who is the wife of Barack Obama?\"\n",
    "#q0_test = \"Who is the author of Le Petit Prince?\"\n",
    "#q0_nlp_test = get_nlp(q0_test)\n",
    "#q0_themes_test = get_themes(q0_nlp_test, top_k=3)\n",
    "#q0_themes_enhanced_test = get_enhanced_themes(q0_themes_test, top_k=3)\n",
    "#q0_predicates_test = get_predicates_online(q0_nlp_test, top_k=3)\n",
    "#q0_focused_parts_test = []\n",
    "#graph, predicates_dict = build_graph(q0_nlp_test, q0_themes_test, q0_themes_enhanced_test, q0_predicates_test, deep_k=3)\n",
    "#print(predicates_dict)\n",
    "#plot_graph(graph, \"file_name_graph\", \"Graph_title\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the graph for complements\n",
    "# parameters\n",
    "# name: string\n",
    "def find_name_in_graph(graph, name):\n",
    "    return [x for x,y in graph.nodes(data=True) if y['name'].lower() == name.lower()]\n",
    "\n",
    "#[find_name_in_graph(c.text) for c in q0_themes[1]]\n",
    "#print(find_name_in_graph(graph, \"the unicorn\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: clean the complements by removing stopwords etc.\n",
    "def find_theme_complement(graph, themes):\n",
    "    return [i for i in itertools.chain.from_iterable(\n",
    "        [id for id in [c for c in [find_name_in_graph(graph, t.text) for t in themes[1]] if c]])]\n",
    "\n",
    "#print(find_theme_complement(graph, q0_themes_test))\n",
    "#[i for i in itertools.chain.from_iterable([id for id in check_theme_complement(graph, q0_themes)])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_paths_in_graph(graph, node_start, node_end):\n",
    "    return [p for p in nx.all_simple_paths(graph, source=node_start, target=node_end)]\n",
    "        \n",
    "#test_paths = find_paths_in_graph(graph, \"Q16205566\", \"Q7774795\")\n",
    "#print(test_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_id_in_graph(graph, node_id):\n",
    "    return graph.has_node(node_id)\n",
    "#print(is_id_in_graph(graph, \"Q24039104\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_name_in_graph(graph, node_name):\n",
    "    return find_name_in_graph(graph, node_name) != []\n",
    "#print(is_name_in_graph(graph, \"the Unicorn\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_paths_for_themes(graph, themes):\n",
    "    themes_ids = [t for t in  extract_ids(themes[0])]\n",
    "    complements_ids = find_theme_complement(graph, themes)\n",
    "    paths = []\n",
    "    for t_id in themes_ids:\n",
    "        if is_id_in_graph(graph, t_id):\n",
    "            for c_id in complements_ids:\n",
    "                if is_id_in_graph(graph, c_id):\n",
    "                    path = find_paths_in_graph(graph, t_id, c_id)\n",
    "                    if path:\n",
    "                        paths.append(path)\n",
    "    paths = [i for i in itertools.chain.from_iterable(\n",
    "        [id for id in paths])]\n",
    "    \n",
    "    return paths\n",
    "#print(find_paths_for_themes(graph, q0_themes_test))\n",
    "#print(find_paths_for_themes(graph, q0_themes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_node_predicates_from_path(paths):\n",
    "    predicates = []\n",
    "    for p in paths:\n",
    "        [predicates.append(i[:i.find(\"-\")]) for i in p if is_wd_predicate(i[:i.find(\"-\")]) and i[:i.find(\"-\")] not in predicates]\n",
    "    return predicates\n",
    "\n",
    "#test_node_predicates = get_node_predicates_from_path(test_paths)\n",
    "#print(test_node_predicates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_node_predicate_similarity_from_path(paths, predicates):\n",
    "    path_predicates = get_node_predicates_from_path(paths)\n",
    "    return sorted([(pp, get_similarity_by_ids(p2, pp)) for p in predicates for p2 in p[1] for pp in path_predicates], key=lambda x: x[-1], reverse=True)\n",
    "\n",
    "#test_node_pedicate_similarities = get_node_predicate_similarity_from_path(test_paths, q0_predicates)\n",
    "#print(test_node_pedicate_similarities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_focused_parts(nlp_sentence, themes, top_k=3):\n",
    "    W_FILTERS = [\"WDT\", \"WP\", \"WP$\", \"WRB\"]\n",
    "    V_FILTERS = [\"VERB\", \"AUX\"]\n",
    "\n",
    "    focused_parts = [t.head for t in nlp_sentence if t.tag_ in W_FILTERS] \n",
    "    for fp in focused_parts:\n",
    "        if fp.children:\n",
    "            for c in fp.children:\n",
    "                if c.tag_ not in W_FILTERS and c.text not in [fp.text for fp in focused_parts]: \n",
    "                    focused_parts.append(c)\n",
    "    \n",
    "    #print(\"focused_parts\",focused_parts)\n",
    "    #print(\"themes[0]\",themes[0])\n",
    "    \n",
    "    focused_parts_len = len(focused_parts)\n",
    "    for t in themes[0]:\n",
    "        for i_fp, fp in enumerate(focused_parts):\n",
    "            for i_w, w in enumerate([w.lower_ for w in t[0]]):\n",
    "                if fp.lower_ == w:\n",
    "                    if i_fp+1 < focused_parts_len-1:\n",
    "                        if focused_parts[i_fp+1].lower_ == t[0][i_w-1].lower_:\n",
    "                            #print(i_fp,fp, t[0][i_w-1], t[0])\n",
    "                            #print(\"BEFORE focused_parts\",focused_parts)\n",
    "                            focused_parts[i_fp] = t[0]\n",
    "                            del focused_parts[i_fp+1]\n",
    "                            #print(\"AFTER focused_parts\",focused_parts)\n",
    "                            \n",
    "    \n",
    "    #print()\n",
    "    #for fp in focused_parts:\n",
    "    #    print(type(fp))\n",
    "    #    \n",
    "    #        print(fp.as_doc())\n",
    "        #if isinstance() == 'spacy.tokens.span.Span':\n",
    "        #    print(\"in\")\n",
    "        #\n",
    "    #focused_parts = [type(fp) for fp in focused_parts]\n",
    "    #print(\"focused_parts\",focused_parts)\n",
    "    \n",
    "    focused_parts_ids = [get_wd_ids(p.text, top_k=top_k) for p in focused_parts]\n",
    "    #print(\"focused_parts_ids\",focused_parts_ids)\n",
    "    merged_list = merge_lists(focused_parts, focused_parts_ids)\n",
    "    #print(\"merged_list\",merged_list)\n",
    "    \n",
    "    dummy_span = get_nlp(\"dummy span\")[:]\n",
    "    merged_list_filtered = []\n",
    "    for ml in merged_list:\n",
    "        if ml[1]:\n",
    "            if type(ml[0]) == type(dummy_span):\n",
    "                merged_list_filtered.append(ml)\n",
    "            elif ml[0].pos_ not in V_FILTERS and not ml[0].is_stop:\n",
    "                merged_list_filtered.append(ml)\n",
    "                    \n",
    "    return merged_list_filtered\n",
    "\n",
    "#q_test_nlp = get_nlp(\"what's akbar tandjung's ethnicity\")\n",
    "#print(get_focused_parts(q0_nlp_test))\n",
    "\n",
    "#q_test_nlp = get_nlp(\"Who voiced the Unicorn in The Last Unicorn?\")\n",
    "#print(get_focused_parts(q0_nlp_test))\n",
    "\n",
    "#q_test_nlp = get_nlp(\"Who is the author that wrote the book Moby Dick\")\n",
    "#q_test_themes = get_themes(q_test_nlp, top_k=3)\n",
    "#get_focused_parts(q_test_nlp,q_test_themes, top_k=3)\n",
    "\n",
    "#q_focused_parts: [(Unicorn, ['Q18356448', 'Q21070472', 'Q22043340', 'Q1565614', 'Q30060419']),\n",
    "#(in, ['P642', 'Q29733109', 'P361', 'P131']),\n",
    "#(the, ['Q1408543', 'Q2865743', 'Q29423', 'Q21121474']),\n",
    "#(Unicorn, ['Q18356448', 'Q21070472', 'Q22043340', 'Q1565614', 'Q30060419']),\n",
    "#(The, ['Q1067527', 'Q13423400', 'Q28457426', 'Q24406786', 'Q2430521', 'Q37199001']),\n",
    "#(Last, ['Q16995904', 'Q20072822', 'Q24229340', 'Q20155285'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#questions_2 = (\"what was the cause of death of yves klein\",\n",
    "#               \"Who is the wife of Barack Obama?\",\n",
    "#               \"Who is the president of the United States?\",\n",
    "#               \"When was produced the first Matrix movie?\",\n",
    "#               \"Who made the soundtrack of the The Last Unicorn movie?\",\n",
    "#               \"Who is the author of Le Petit Prince?\",\n",
    "#               \"Which actor voiced the Unicorn in The Last Unicorn?\",\n",
    "#               \"how is called the rabbit in Alice in Wonderland?\",\n",
    "#               \"what city was alex golfis born in\",\n",
    "#               \"which stadium do the wests tigers play in\",\n",
    "#               \"Which nation is Martha Mattox from\"\n",
    "#              )\n",
    "#\n",
    "#question_2 = questions_2[6] #\"what city was alex golfis born in\"#\n",
    "#question_2 = \"what's akbar tandjung's ethnicity\"\n",
    "#q_nlp_2 = get_nlp(question_2)\n",
    "#q_themes_2 = get_themes(q_nlp_2, top_k=3)\n",
    "#q_themes_enhanced_2 = get_enhanced_themes(q_themes_2, top_k=3)\n",
    "#q_predicates_2 = get_predicates(q_nlp_2, top_k=3)\n",
    "#if q_predicates_2:\n",
    "#        if not q_predicates_2[0][1]: q_predicates_2 = get_predicates_online(q_nlp_2, top_k=3)\n",
    "#q_focused_parts_2 = get_focused_parts(q_nlp_2)\n",
    "#print(\"q_nlp:\", q_nlp_2)\n",
    "#print(\"e\\t\\te.pos_\\te.tag_\\te.dep_\\te.head\\te.children\")\n",
    "#for e in q_nlp_2:\n",
    "#    print(e.text,\"\\t\\t\", e.pos_,\"\\t\", e.tag_,\"\\t\", e.dep_,\"\\t\", e.head, \"\\t\", [child for child in e.children])\n",
    "#\n",
    "#print(\"\\nq_themes:\", q_themes_2)\n",
    "#print(\"q_themes_enhanced:\",q_themes_enhanced_2)\n",
    "#print(\"q_predicates:\", q_predicates_2)\n",
    "#print(\"q_focused_parts:\", q_focused_parts_2)\n",
    "#\n",
    "#graph_2, predicates_dict_2 = build_graph(q_nlp_2, q_themes_2, q_themes_enhanced_2, q_predicates_2, deep_k=40)\n",
    "#print(len(graph_2), \"nodes and\", graph_2.size(), \"edges\")\n",
    "#print(predicates_dict_2)\n",
    "##plot_graph(graph_2, \"main_graph\", \"Main_graph_title\")\n",
    "##answers_2 = find_anwser_from_graph_2(graph, q0_nlp, q0_themes, q_themes_enhanced_2, q_predicates_2, q_focused_parts_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot_graph(graph_2, \"test_file_name_graph\", \"Graph_title\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_compound(nlp_list, themes):\n",
    "    compounded = []\n",
    "    #if not nlp_list[0]:\n",
    "    #    return compounded\n",
    "    try:\n",
    "        for t in [e[0] for e in themes[0]] + themes[1]:\n",
    "            for l in [n[0] for n in nlp_list]:\n",
    "                if l.text.lower() in t.text.lower():\n",
    "                    compounded.append(t.text)\n",
    "        return compounded\n",
    "    except:\n",
    "        return compounded\n",
    "\n",
    "# TODO: make the predicate search go further in the path list for the !i%2\n",
    "def find_paths_keywords(graph, nlp, themes, themes_enhanced, predicates, focused_parts):\n",
    "    WH_FILTER = [\"WDT\", \"WP\", \"WP$\", \"WRB\"]\n",
    "    VERB_FILTER = [\"VERB\", \"AUX\"]\n",
    "    NOUN_FILTER = [\"NOUN\",\"PROPN\"]\n",
    "    POSITION_FILTER = [\"ADP\"]\n",
    "    \n",
    "    focused_parts_words = [t[0].text for t in focused_parts]\n",
    "    focused_parts_ids = [j for i in [t[1] for t in focused_parts] for j in i]\n",
    "    focused_parts_predicates_ids = [f for f in focused_parts_ids if is_wd_predicate(f)]\n",
    "    focused_parts_words_ids = [f for f in focused_parts_ids if is_wd_entity(f)]\n",
    "    focused_parts_words_ids_labeled = [get_wd_label(p) for p in focused_parts_words_ids]\n",
    "    #print(focused_parts_words_2)\n",
    "\n",
    "    question_anchors = [t for t in nlp if t.tag_ in WH_FILTER]\n",
    "    themes_enhanced_list = [t[0] for t in themes_enhanced]\n",
    "    focus_themes = [t[0].text for t in themes[0]]\n",
    "    focus_path_by_tails = [[c for c in t.head.children if c.pos_ in NOUN_FILTER] for t in nlp if t.pos_ == \"PRON\"]\n",
    "    focus_part_by_head = [t.head for t in question_anchors]\n",
    "    predicates_nlp = [t for t in nlp if t.pos_ in VERB_FILTER]\n",
    "    predicates_lemma = [t.lemma_ for t in predicates_nlp]\n",
    "    predicates_attention = [t for t in nlp if t.head in predicates_nlp]\n",
    "    predicates_attention_tails = [[c for c in t.children] for t in predicates_attention]\n",
    "    in_attention_heads = [t.head.text for t in nlp if t.pos_ in POSITION_FILTER]\n",
    "    in_attention_tails = add_compound([[c for c in t.children] for t in nlp if t.pos_ in POSITION_FILTER], themes)\n",
    "    focus_themes_enhanced = [t[0] for t in themes_enhanced\n",
    "                             if t[0].lower() in [a.lower() for a in in_attention_tails]\n",
    "                             or t[0].lower() in [a.lower() for a in in_attention_heads]]\n",
    "    \n",
    "    theme_enhanced_ids = extract_ids(themes_enhanced)\n",
    "    predicates_enhanced_ids = [(p) for p in theme_enhanced_ids if is_wd_predicate(p)]\n",
    "    [predicates_enhanced_ids.append(p) for p in focused_parts_predicates_ids if p not in predicates_enhanced_ids]\n",
    "    \n",
    "    alterniative_words = {}\n",
    "    for t in themes_enhanced:\n",
    "        for e in predicates_enhanced_ids:\n",
    "            if e in t[1]:\n",
    "                alterniative_words[t[0]] = [get_nlp(get_wd_label(e)),[e]]\n",
    "            else:\n",
    "                alterniative_words[get_wd_label(e)] = [get_nlp(get_wd_label(e)),[e]]\n",
    "    \n",
    "    #print(\"focused_parts_predicates_ids\",focused_parts_predicates_ids)\n",
    "    #print(\"focused_parts_words_ids\",focused_parts_words_ids)\n",
    "    #print(\"alterniative_words\",alterniative_words)\n",
    "    #print(\"predicates_enhanced_ids\",predicates_enhanced_ids)\n",
    "    ##print(\"predicates_enhanced\",predicates_enhanced)\n",
    "    #print(\"question_anchors\",question_anchors)\n",
    "    #print(\"in_attention_heads\",in_attention_heads)\n",
    "    #print(\"in_attention_tails\",in_attention_tails)\n",
    "    #print(\"focus_themes\",focus_themes)\n",
    "    #print(\"themes_enhanced_list\",themes_enhanced_list)\n",
    "    #print(\"focus_themes_enhanced\",focus_themes_enhanced)\n",
    "    #print(\"focus_path_by_tails\",focus_path_by_tails)\n",
    "    #print(\"focus_part_by_head\",focus_part_by_head)\n",
    "    #print(\"predicates_nlp\",predicates_nlp)\n",
    "    #print(\"predicates_lemma\",predicates_lemma)\n",
    "    #print(\"predicates_attention\",predicates_attention)\n",
    "    #print(\"predicates_attention_tails\",predicates_attention_tails)\n",
    "    #\n",
    "    #print(\"\\n\")\n",
    "    paths_keywords = []\n",
    "    [paths_keywords.append(e.lower()) for e in focused_parts_words + in_attention_heads + in_attention_tails + focus_themes + focus_themes_enhanced + focused_parts_words_ids_labeled if e.lower() not in paths_keywords]\n",
    "    #print(paths_keywords)\n",
    "    #paths_keywords = [p for p in itertools.permutations(paths_keywords)]\n",
    "    #print(paths_keywords)\n",
    "    return paths_keywords, alterniative_words, question_anchors\n",
    "    \n",
    "    #initial_paths = find_paths_for_themes(graph, themes)\n",
    "    #predicate_id_similarities = get_node_predicate_similarity_from_path(initial_paths, predicates)\n",
    "    #best_path = [p for p in initial_paths if predicate_id_similarities[0][0] == p[1][:p[1].find(\"-\")]]\n",
    "    #path_answer = get_wd_label(best_path[0][2]) if best_path else []\n",
    "    \n",
    "    #return (path_answer, best_path[0][2]) if path_answer else (False, False)\n",
    "\n",
    "#paths_keywords_2 = find_paths_keywords(graph_2, q_nlp_2, q_themes_2, q_themes_enhanced_2, q_predicates_2, q_focused_parts_2)\n",
    "#paths_keywords_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_paths_keywords_nodes(graph, keywords,threshold=0.9,top_performance=50):\n",
    "    keywords_nodes = []\n",
    "    for k in keywords:\n",
    "        nlp_lookup = get_nlp(k)\n",
    "        keywords_nodes.append([x for x,y in graph.nodes(data=True)\n",
    "               if get_nlp(y['name']).similarity(nlp_lookup) >= threshold])\n",
    "    keywords_nodes = [k for k in keywords_nodes if k]\n",
    "    \n",
    "    #keywords_nodes [['Q17521117', 'Q17521118', 'Q557214', 'Q421946', 'Q11282976', 'Q4677712', 'Q33999'], ['Q7246', 'Q1307944', 'Q21070472', 'Q18356448', 'Q1863113', 'Q20983877', 'Q226755', 'Q22043340'], ['Q176198', 'Q967268', 'Q17553756', 'Q30060419', 'Q17985004', 'Q16614390', 'Q18647334', 'Q15628943'], ['Q176198', 'Q967268', 'Q17553756', 'Q30060419', 'Q17985004', 'Q16614390', 'Q18647334', 'Q15628943'], []]\n",
    "    #keywords_nodes[0] ['Q17521117', 'Q17521118', 'Q557214', 'Q421946', 'Q11282976', 'Q4677712', 'Q33999']\n",
    "    #keywords_nodes[1] ['Q7246', 'Q1307944', 'Q21070472', 'Q18356448', 'Q1863113', 'Q20983877', 'Q226755', 'Q22043340']\n",
    "    \n",
    "    keywords_nodes_per = []\n",
    "    if keywords_nodes:\n",
    "        if len(keywords_nodes) > 1:\n",
    "            for kn_i, kn in enumerate(keywords_nodes):\n",
    "                if kn_i + 1 < len(keywords_nodes):\n",
    "                    if len(kn) * len(keywords_nodes[kn_i+1]) > top_performance:\n",
    "                        if len(kn) <= int(sqrt(top_performance)):\n",
    "                            keywords_nodes[kn_i+1] = keywords_nodes[kn_i+1][:int(top_performance/len(kn))]\n",
    "                        elif len(kn) >= len(keywords_nodes[kn_i+1]):\n",
    "                            kn = kn[:int(top_performance/len(keywords_nodes[kn_i+1]))]\n",
    "                        else:\n",
    "                            kn = kn[:int(sqrt(top_performance))]\n",
    "                            keywords_nodes[kn_i+1] = keywords_nodes[kn_i+1][:int(sqrt(top_performance))]\n",
    "            \n",
    "            keywords_nodes_per = [p for p in itertools.permutations(keywords_nodes, 2)]\n",
    "                            \n",
    "        else:\n",
    "            keywords_nodes_per = [(keywords_nodes+keywords_nodes)]\n",
    "    \n",
    "    paths_keyword_nodes = []\n",
    "    for pkn in keywords_nodes_per:\n",
    "        for pkn1 in pkn[0]:\n",
    "            for pkn2 in pkn[1]:\n",
    "                [paths_keyword_nodes.append(p) for p in nx.all_simple_paths(graph, source=pkn1, target=pkn2) if p not in paths_keyword_nodes]\n",
    "    \n",
    "    return paths_keyword_nodes\n",
    "\n",
    "def find_path_nodes_from_graph(graph, keywords, threshold=0.9, thres_inter=0.15, top_performance=50,min_paths=3000):\n",
    "    #print(\"current threshold\", str(round(threshold, 1)))\n",
    "    main_keyword_paths = get_paths_keywords_nodes(graph, keywords[0],threshold=threshold,top_performance=top_performance)\n",
    "    alternative_keyword_paths = []\n",
    "    \n",
    "    for k_1 in keywords[1]:\n",
    "        for i, k_0 in enumerate(keywords[0]):\n",
    "            if k_1==k_0:\n",
    "                tmp_keywords = keywords[0].copy()\n",
    "                tmp_keywords[i] = keywords[1][k_1][0].text\n",
    "                alternative_keyword_paths += get_paths_keywords_nodes(graph, tmp_keywords, threshold=threshold,top_performance=top_performance)\n",
    "    \n",
    "    keyword_paths = main_keyword_paths+alternative_keyword_paths\n",
    "    \n",
    "    #print(\"len(keyword_paths)\",len(keyword_paths))\n",
    "    if len(keyword_paths) < min_paths:\n",
    "        if threshold == 0: return keyword_paths\n",
    "        threshold -= thres_inter\n",
    "        if threshold < 0: threshold = 0\n",
    "        keyword_paths = find_path_nodes_from_graph(graph, keywords, threshold, thres_inter,top_performance,min_paths)\n",
    "    \n",
    "    return keyword_paths\n",
    "\n",
    "#start_time = time.time()\n",
    "#path_nodes_2 = find_path_nodes_from_graph(graph_2, paths_keywords_2, threshold=0.9, thres_inter=0.15, top_performance=50, min_paths=3000)\n",
    "#end_time = time.time()\n",
    "#print(\"Finding path nodes ->\\tRunning time is {}s\".format(round(end_time-start_time,2))) \n",
    "#print(path_nodes_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "#node_predicates_names_2 = get_node_predicates_from_path(path_nodes_2)\n",
    "\n",
    "def is_sublist(a, b):\n",
    "    if not a: return True\n",
    "    if not b: return False\n",
    "    #if a == b: return False\n",
    "    return b[:len(a)] == a or is_sublist(a, b[1:])\n",
    "\n",
    "def paths_nodes_filter(path_nodes, graph):\n",
    "    filtered_paths = []\n",
    "    \n",
    "    for path in path_nodes:\n",
    "        filtered_row = []\n",
    "        for i,p in enumerate(path):\n",
    "            if is_wd_predicate(p[:p.find(\"-\")]):\n",
    "                if i == 0:\n",
    "                    #if p[:p.find(\"-\")] == \"P725\":\n",
    "                    #    print(p)\n",
    "                    neighbor = [k for k in graph[p].keys() if k != path[i+1]]\n",
    "                    if neighbor:\n",
    "                        filtered_row.append(neighbor[0])\n",
    "                        filtered_row.append(p[:p.find(\"-\")])\n",
    "                    else:\n",
    "                        continue\n",
    "                    #print(filtered_row)\n",
    "                elif i > 0 and i < len(path)-1:\n",
    "                    filtered_row.append(p[:p.find(\"-\")])\n",
    "                else:\n",
    "                    neighbor = [k for k in graph[p].keys() if k != path[i-1]]\n",
    "                    if neighbor:\n",
    "                        filtered_row.append(p[:p.find(\"-\")])\n",
    "                        filtered_row.append(neighbor[0])\n",
    "                    else:\n",
    "                        continue\n",
    "            else: filtered_row.append(p)\n",
    "        \n",
    "        #print(\"filtered_paths\",filtered_paths)\n",
    "        \n",
    "        if len(filtered_row) > 1 and filtered_row not in filtered_paths: \n",
    "            filtered_paths.append(filtered_row)\n",
    "    \n",
    "    unique_paths = filtered_paths.copy()\n",
    "    for i,fp in enumerate(filtered_paths):\n",
    "        for fp_2 in filtered_paths:\n",
    "            if (is_sublist(fp, fp_2) and fp!=fp_2):\n",
    "                unique_paths[i] = []\n",
    "                break\n",
    "    \n",
    "    unique_paths = [p for p in unique_paths if p]\n",
    "    \n",
    "    unique_paths_with_reversed = []\n",
    "    \n",
    "    for up in unique_paths:\n",
    "        reversed_up = list(reversed(up))\n",
    "        if up not in unique_paths_with_reversed: \n",
    "            unique_paths_with_reversed.append(up)\n",
    "        if reversed_up not in unique_paths_with_reversed: \n",
    "            unique_paths_with_reversed.append(reversed_up)\n",
    "        \n",
    "    #print(\"unique_paths\",len(unique_paths))\n",
    "    \n",
    "    #for i, up in enumerate(unique_paths):\n",
    "    #    for up_2 in unique_paths:\n",
    "    #        if (list(reversed(up)) == up_2):\n",
    "    #            unique_paths[i] = []\n",
    "    #            break\n",
    "    \n",
    "    \n",
    "    #cleaned_paths = []\n",
    "    #unique_paths = [up for up in unique_paths if up]\n",
    "    \n",
    "    #for up in unique_paths:\n",
    "    #    for i,e in enumerate(up):\n",
    "    #        if not is_wd_predicate(e):\n",
    "    #            for j,r in enumerate(list(reversed(up))): \n",
    "    #                if not is_wd_predicate(r):\n",
    "    #                    cleaned_paths.append(up[i:-j])\n",
    "    #            break\n",
    "                \n",
    "    #print(\"cleaned_paths\",len(cleaned_paths))\n",
    "                \n",
    "    #cleaned_paths = [c for c in cleaned_paths if len(c) > 2]\n",
    "    \n",
    "    #unique_paths = cleaned_paths.copy()\n",
    "    #for i,fp in enumerate(cleaned_paths):\n",
    "    #    for fp_2 in cleaned_paths:\n",
    "    #        if (is_sublist(fp, fp_2) and fp!=fp_2):\n",
    "    #            unique_paths[i] = []\n",
    "    #            break\n",
    "    \n",
    "    #unique_paths = [p for p in unique_paths if len(p) > 2]       \n",
    "    \n",
    "    #for i, up in enumerate(unique_paths):\n",
    "    #    for up_2 in unique_paths:\n",
    "    #        if (list(reversed(up)) == up_2):\n",
    "    #            unique_paths[i] = []\n",
    "    #            break\n",
    "        \n",
    "        #print(up)\n",
    "    #[up for up in unique_paths if up and not is_wd_predicate(up[-1]) and not is_wd_predicate(up[0])]\n",
    "    #print()\n",
    "    #for up in unique_paths:\n",
    "    #    print(up)\n",
    "    #    break\n",
    "    #    return []\n",
    "    \n",
    "    return [p for p in unique_paths_with_reversed if len(p) > 2] #False#[up for up in unique_paths if up and not is_wd_predicate(up[-1]) and not is_wd_predicate(up[0])]#False# [p for p in unique_paths if p]\n",
    "                \n",
    "#paths_nodes_filtered_2 = paths_nodes_filter(path_nodes_2, graph_2)\n",
    "#print(\"unique_paths\", len(paths_nodes_filtered_2))\n",
    "#for p in paths_nodes_filtered_2:\n",
    "#    print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def w_converter(nlp):\n",
    "    w_positions = []\n",
    "    w_names = []\n",
    "    for i_q,q in enumerate(nlp):\n",
    "        if q.lemma_ == \"where\": \n",
    "            w_positions.append((i_q))\n",
    "            w_names.append((i_q,\"location\"))\n",
    "        elif q.lemma_ == \"when\": \n",
    "            w_positions.append((i_q))\n",
    "            w_names.append((i_q,\"date\"))\n",
    "        elif q.lemma_ == \"who\": \n",
    "            w_positions.append((i_q))\n",
    "            w_names.append((i_q,\"person\"))    \n",
    "        elif q.lemma_ == \"why\": \n",
    "            w_positions.append(i_q)\n",
    "            w_names.append((i_q,\"cause\"))\n",
    "        elif q.lemma_ == \"which\": \n",
    "            w_positions.append(i_q)\n",
    "            w_names.append((i_q,\"which\"))\n",
    "        elif q.lemma_ == \"what\": \n",
    "            w_positions.append(i_q)\n",
    "            w_names.append((i_q,\"what\"))\n",
    "        elif i_q+1 < len(nlp) and q.lemma_ == \"how\" and (nlp[i_q+1].lemma_ == \"much\" or nlp[i_q+1].lemma_ == \"many\"): \n",
    "            w_positions.append(i_q)\n",
    "            w_names.append((i_q,\"quantity\"))\n",
    "    return w_positions, w_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_entity_similarity(word_id, entity_type, max_reward=2.0):\n",
    "    LOCATION_FILTER = [\"GPE\", \"FAC\", \"LOC\"]\n",
    "    PERSON_FILTER = [\"PERSON\",\"NORP\",\"ORG\",\"PER\"]\n",
    "    DATE_FILTER = [\"DATE\",\"TIME\"]\n",
    "    CAUSE_FILTER = [\"NORP\",\"PRODUCT\",\"EVENT\",\"MISC\"]\n",
    "    WHICH_FILTER = PERSON_FILTER+DATE_FILTER+[\"GPE\",\"LOC\",\"PRODUCT\",\"EVENT\",\n",
    "                    \"WORK_OF_ART\",\"LAW\",\"LANGUAGE\",\"MISC\"]\n",
    "    WHAT_FILTER = LOCATION_FILTER+DATE_FILTER+CAUSE_FILTER+[\"NORP\",\"ORG\",\n",
    "                     \"PER\",\"WORK_OF_ART\",\"LAW\",\"LANGUAGE\"]\n",
    "                    \n",
    "    QUANTITY_FILTER = [\"PERCENT\", \"MONEY\", \"QUANTITY\", \"ORDINAL\", \"CARDINAL\"]\n",
    "    \n",
    "    ALL_FILTER = LOCATION_FILTER + PERSON_FILTER + DATE_FILTER + CAUSE_FILTER + WHICH_FILTER + WHAT_FILTER + QUANTITY_FILTER\n",
    "    \n",
    "    similarities = []\n",
    "    word_label = get_wd_label(word_id)\n",
    "    if word_label == \"\":\n",
    "        return similarities\n",
    "    \n",
    "    word_ents = get_kb_ents(word_label)\n",
    "    if word_ents:\n",
    "        for ent in word_ents:\n",
    "            if (entity_type in ALL_FILTER and ent.label_ == entity_type):\n",
    "                similarities.append(max_reward)\n",
    "            \n",
    "            elif ent.kb_id_ == word_id:\n",
    "                if entity_type == \"location\" and ent.label_ in LOCATION_FILTER:\n",
    "                    similarities.append(max_reward)\n",
    "                elif entity_type == \"person\" and ent.label_ in PERSON_FILTER:\n",
    "                    similarities.append(max_reward)\n",
    "                elif entity_type == \"date\" and ent.label_ in DATE_FILTER:\n",
    "                    similarities.append(max_reward)\n",
    "                elif entity_type == \"cause\" and ent.label_ in CAUSE_FILTER:\n",
    "                    similarities.append(max_reward)\n",
    "                elif entity_type == \"which\" and ent.label_ in WHICH_FILTER:\n",
    "                    similarities.append(max_reward)\n",
    "                elif entity_type == \"what\" and ent.label_ in WHAT_FILTER:\n",
    "                    similarities.append(max_reward)\n",
    "                elif entity_type == \"quantity\" and ent.label_ in QUANTITY_FILTER:\n",
    "                    similarities.append(max_reward)\n",
    "                else:                     \n",
    "                    similarities.append(get_similarity_by_words(get_nlp(word_label),get_nlp(entity_type)))\n",
    "            else: similarities.append(get_similarity_by_words(get_nlp(word_label),get_nlp(entity_type)))\n",
    "    else:\n",
    "        similarities.append(get_similarity_by_words(get_nlp(word_label),get_nlp(entity_type)))\n",
    "    #print(\"get_entity_similarity:\",word_label, entity_type, similarities)\n",
    "    return similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_question = \"Of what nationality is Ken McGoogan\"#\"Which is the nation of Martha Mattox\"#\"Who voiced the Unicorn in The Last Unicorn?\"\n",
    "#\n",
    "#test_verbose = True\n",
    "#test_deep_k = 30\n",
    "#test_deep_k_step = 20\n",
    "#\n",
    "#test_q_nlp = get_nlp(test_question)\n",
    "#if test_verbose: print(\"-> test_q_nlp:\",test_q_nlp)\n",
    "#test_q_themes = get_themes(test_q_nlp, top_k=3)\n",
    "#if test_verbose: print(\"-> test_q_themes:\",test_q_themes)\n",
    "#test_q_themes_enhanced = get_enhanced_themes(test_q_themes, top_k=1, aggressive=False)\n",
    "#if test_verbose: print(\"-> test_q_themes_enhanced:\",test_q_themes_enhanced)\n",
    "#test_q_predicates = get_predicates(test_q_nlp, test_q_themes, top_k=0)\n",
    "#if test_q_predicates:\n",
    "#    test_has_predicates = False\n",
    "#    for test_qp in test_q_predicates:\n",
    "#        if test_qp[1]:\n",
    "#            test_has_predicates = True\n",
    "#    if not test_has_predicates: test_q_predicates = get_predicates_online(test_q_nlp, top_k=2, aggressive=False)\n",
    "#else:\n",
    "#    test_q_predicates = get_predicates_online(test_q_nlp, top_k=2, aggressive=True)    \n",
    "#if test_verbose: print(\"-> test_q_predicates:\",test_q_predicates)\n",
    "#test_q_focused_parts = get_focused_parts(test_q_nlp, test_q_themes, top_k=3)\n",
    "#if test_verbose: print(\"-> test_q_focused_parts:\",test_q_focused_parts)\n",
    "#if test_verbose: print(\"-> Building the graph with test_k_deep\",str(test_deep_k),\"... (could be long)\")\n",
    "#if test_deep_k<=10:\n",
    "#    test_deep_k = 10\n",
    "#    test_graph, test_predicates_dict = build_graph(test_q_nlp, test_q_themes, test_q_themes_enhanced, test_q_predicates, deep_k=test_deep_k)\n",
    "#else:\n",
    "#    for k in range(10, test_deep_k, test_deep_k_step):\n",
    "#        test_graph, test_predicates_dict = build_graph(test_q_nlp, test_q_themes, test_q_themes_enhanced, test_q_predicates, deep_k=test_deep_k)\n",
    "#        if test_graph.size() > 1000 or len(test_graph) > 1000 or test_deep_k<=10:\n",
    "#            break\n",
    "#        elif test_graph.size() > 500 or len(test_graph) > 500:\n",
    "#            test_deep_k -= test_deep_k_step\n",
    "#            if test_verbose: print(\"---> Rebuilding the graph with k_deep\",str(test_deep_k), \"... Previously:\",len(test_graph), \"nodes or\", test_graph.size(), \"edges was above the limit...\")\n",
    "#        else: break\n",
    "#if test_verbose: print(\"--> \",len(test_graph), \"nodes and\", test_graph.size(), \"edges\")\n",
    "#if test_graph.size() > 510 or len(test_graph) > 510:\n",
    "#    if test_verbose: print(\"Stopping the computing here, too computational.\")\n",
    "#if test_verbose: print(\"-> test_predicates_dict:\",test_predicates_dict)\n",
    "#test_paths_keywords = find_paths_keywords(test_graph, test_q_nlp, test_q_themes, test_q_themes_enhanced, test_q_predicates, test_q_focused_parts)\n",
    "#if test_verbose: print(\"-> test_paths_keywords:\",test_paths_keywords)\n",
    "#if test_verbose: print(\"-> Computing possible paths... (could be long)\")\n",
    "#test_path_nodes = find_path_nodes_from_graph(test_graph, test_paths_keywords, threshold=0.8, thres_inter=0.1, top_performance=test_graph.size(),min_paths=3000)\n",
    "#if test_verbose: print(\"--> len(path_nodes):\",len(test_path_nodes))\n",
    "#if len(test_path_nodes) < 20000:\n",
    "#    if test_verbose: print(\"-> Filtering paths... (could be long)\")\n",
    "#    test_paths_nodes_filtered = paths_nodes_filter(test_path_nodes, test_graph)\n",
    "#    if test_verbose: \n",
    "#        print(\"--> len(paths_nodes_filtered):\",len(test_paths_nodes_filtered))\n",
    "#else: \n",
    "#    if test_verbose: print(\"--> Skipping paths filtering... (too much paths)\")\n",
    "#    test_paths_nodes_filtered = test_path_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hypothesises(nlp, predicates, themes, paths_keywords, filtered_paths, threshold=0.5, max_reward=2.0):#, themes, themes_enhanced):\n",
    "    \n",
    "    complementary_predicates = paths_keywords[0]+[p[0] for p in list(paths_keywords[1].values())]\n",
    "    #print(\"complementary_predicates\",complementary_predicates)\n",
    "    \n",
    "    #locate positions   \n",
    "    anchors_positions = []\n",
    "    anchors_focuses = []\n",
    "    #keywords_positions = []\n",
    "    #predicates_positions = []\n",
    "    \n",
    "    theme_keywords = [t[0] for t in themes[0]]\n",
    "    \n",
    "    predicate_ids = sum([p[1] for p in predicates if p[1]],[])\n",
    "    predicate_names = [get_nlp(p[0].text) for p in predicates]\n",
    "    #print(\"predicate_ids\",predicate_ids)\n",
    "    #print(\"predicate_names\",predicate_names)\n",
    "    \n",
    "    w_positions, w_names = w_converter(nlp)\n",
    "    w_names_only = [wn[1] for wn in w_names]\n",
    "    #print(\"w_positions\",w_positions)\n",
    "    #print(\"w_names\",w_names)\n",
    "    #print(\"w_names_only\",w_names_only)\n",
    "    \n",
    "    [anchors_positions.append(i) for i, w in enumerate(nlp) if w in paths_keywords[2]]\n",
    "    #print(\"\\nanchors_positions:\",anchors_positions)\n",
    "    \n",
    "    #anchors_childrens\n",
    "    for p in anchors_positions:\n",
    "        children = [c for c in nlp[p].children]\n",
    "        if children == []: \n",
    "            children = [c for c in nlp[p].head.children]\n",
    "        else: \n",
    "            if nlp[p].head:\n",
    "                children.append(nlp[p].head)\n",
    "        \n",
    "        anchors_focuses += ([c for c in children\n",
    "               if c not in [nlp[a] for a in anchors_positions]\n",
    "               and c.pos_ != \"PUNCT\"])\n",
    "        \n",
    "        if not anchors_focuses:\n",
    "            anchors_focuses = [nlp[p].head]\n",
    "        \n",
    "        anchors_focuses += complementary_predicates\n",
    "        #print(\"\\nanchors_focuses\",anchors_focuses)\n",
    "    \n",
    "    anchors_focuses_filtered = []\n",
    "    \n",
    "    for af in anchors_focuses:\n",
    "        if isinstance(af, str):\n",
    "            anchors_focuses_filtered.append(af)\n",
    "        else:\n",
    "            anchors_focuses_filtered.append(af.text)\n",
    "        \n",
    "    anchors_focuses = []\n",
    "    [anchors_focuses.append(af) for af in anchors_focuses_filtered if af not in anchors_focuses and af]\n",
    "    #print(\"\\nanchors_focuses\",anchors_focuses)\n",
    "    \n",
    "    #find anchor position in paths\n",
    "    anchors_predicates = []\n",
    "    \n",
    "    main_predicate_ids = []\n",
    "    main_predicate_names = []\n",
    "    [main_predicate_ids.append(p) for p in predicate_ids+sum([p[1] for p in list(paths_keywords[1].values())],[]) if p not in main_predicate_ids]\n",
    "    #print(\"paths_keywords[1]\",paths_keywords[1])\n",
    "    #print(\"main_predicate_ids\",main_predicate_ids)\n",
    "    \n",
    "    #print(\"[p[0] for p in list(paths_keywords[1].values())]\",[p[0].text for p in list(paths_keywords[1].values())])\n",
    "    [main_predicate_names.append(p) for p in predicate_names+[get_nlp(p[0].text) for p in list(paths_keywords[1].values())] if p not in main_predicate_names]\n",
    "    #print(\"paths_keywords[1]\",paths_keywords[1])\n",
    "    #print(\"main_predicate_names\",main_predicate_names)\n",
    "    \n",
    "    #return 0\n",
    "    for p in filtered_paths:\n",
    "        p_len = len(p)\n",
    "        for i_e, e in enumerate(p):\n",
    "            if is_wd_predicate(e):\n",
    "                if main_predicate_ids:\n",
    "                    if e in main_predicate_ids:\n",
    "                        if e not in [ap[0] for ap in anchors_predicates]:\n",
    "                            anchors_predicates.append((e, 1.0))\n",
    "                    elif e not in [ap[0] for ap in anchors_predicates]:\n",
    "                        stat_count = 0\n",
    "                        stat_current = 0\n",
    "                        for pn in main_predicate_names:\n",
    "                            stat_current += get_similarity_by_words(get_nlp(get_wd_label(e)),pn)\n",
    "                            stat_count += 1\n",
    "                        for pi in main_predicate_ids:\n",
    "                            stat_current += get_similarity_by_words(get_nlp(get_wd_label(e)),get_nlp(get_wd_label(pi)))\n",
    "                            stat_count += 1\n",
    "                        anchors_predicates.append((e, stat_current/stat_count))\n",
    "                        \n",
    "                elif e not in [ap[0] for ap in anchors_predicates]:\n",
    "                    stat_count = 0\n",
    "                    stat_current = 0\n",
    "                    for af in anchors_focuses:\n",
    "                        stat_current += get_similarity_by_words(get_nlp(get_wd_label(e)),get_nlp(af))\n",
    "                        stat_count += 1\n",
    "                    anchors_predicates.append((e, stat_current/stat_count))\n",
    "            \n",
    "    \n",
    "    #print(\"filtered_paths\",filtered_paths)\n",
    "    #for p in filtered_paths:\n",
    "    #    for af in anchors_focuses:\n",
    "    #        #print(af, p)\n",
    "    #        for e in p:\n",
    "    #            #print(af,get_wd_label(e))\n",
    "    #            if is_wd_predicate(e):# and e not in [ap[0] for ap in anchors_predicates]:\n",
    "    #                #print(af,get_wd_label(e))\n",
    "    #                anchors_predicates.append([e, get_similarity_by_words(get_nlp(get_wd_label(e)),get_nlp(af))])\n",
    "                \n",
    "    #print(\"\\nanchors_predicates\",anchors_predicates)\n",
    "    \n",
    "    anchors_predicates_filtered = []\n",
    "    [anchors_predicates_filtered.append(ap) for ap in anchors_predicates if ap not in anchors_predicates_filtered]\n",
    "    \n",
    "    #anchors_predicates = [a for a in sorted(anchors_predicates_filtered, key=lambda x: x[-1], reverse=True) if a[1] > threshold]\n",
    "    \n",
    "    for thres in [e/100 for e in reversed(range(10, int(threshold*100)+10, 10))]:\n",
    "        #print(\"anchors_predicates current thres\",thres)\n",
    "        anchors_predicates = [a for a in sorted(anchors_predicates_filtered, key=lambda x: x[-1], reverse=True) if a[1] > thres]\n",
    "        if anchors_predicates:\n",
    "            break\n",
    "    \n",
    "    #print(\"len(anchors_predicates sorted)\",len(anchors_predicates))\n",
    "    #print(\"anchors_predicates sorted\",anchors_predicates)\n",
    "    \n",
    "    #anchors_predicates_filtered = []\n",
    "    #for ap in anchors_predicates:\n",
    "    #    for af in anchors_focuses:\n",
    "    #        anchors_predicates_filtered.append([ap[0],get_similarity_by_words(get_nlp(get_wd_label(ap[0])),get_nlp(af))])\n",
    "    #\n",
    "    #anchors_predicates_filtered = [a for a in sorted(anchors_predicates_filtered, key=lambda x: x[-1], reverse=True) if a[1] > 0]\n",
    "    \n",
    "    #for thres in [e/100 for e in reversed(range(10, int(threshold*100)+10, 10))]:\n",
    "    #    print(\"anchors_predicates_filtered current thres\",thres)\n",
    "    #    if not anchors_predicates_filtered:\n",
    "    #        anchors_predicates_filtered = anchors_predicates\n",
    "    #        break\n",
    "    #    anchors_predicates_filtered = [a for a in sorted(anchors_predicates_filtered, key=lambda x: x[-1], reverse=True) if a[1] > thres]\n",
    "    #    if len(anchors_predicates) > 10:\n",
    "    #        break\n",
    "    \n",
    "    #print(\"len(anchors_predicates_filtered)\",len(anchors_predicates_filtered))\n",
    "    #print(\"anchors_predicates_filtered\",anchors_predicates_filtered)\n",
    "    #\n",
    "    #anchors_predicates=[]\n",
    "    #[anchors_predicates.append(apf) for apf in anchors_predicates_filtered if apf not in anchors_predicates]\n",
    "    #print(\"len(anchors_predicates)\",len(anchors_predicates))\n",
    "    #print(\"anchors_predicates\",anchors_predicates)\n",
    "    \n",
    "    hypothesises_tuples = []\n",
    "    for ap in anchors_predicates:\n",
    "        for fp in filtered_paths:\n",
    "            #if \"Q4985\" in fp:\n",
    "            #    print(\"Q4985 in fp\",fp, ap)\n",
    "            for i, e in enumerate(fp):\n",
    "                #print(e)\n",
    "                if e == ap[0] and i>1 and i<len(fp)-1:\n",
    "                    #print(i, [fp[i-1], fp[i], fp[i+1]])\n",
    "                    hypothesis_tuple = [fp[i-1], fp[i], fp[i+1]]\n",
    "                    if hypothesis_tuple not in hypothesises_tuples:\n",
    "                        hypothesises_tuples.append(hypothesis_tuple)\n",
    "                        #if \"Q4985\" in hypothesis_tuple:\n",
    "                        #    print(\"Q4985 hypothesis_tuple\",hypothesis_tuple, ap,fp)\n",
    "                        \n",
    "    #print(\"hypothesises_tuples\",hypothesises_tuples)\n",
    "    #print(\"hypothesises_tuples\",hypothesises_tuples)\n",
    "    #print([a[0] for a in anchors_predicates])\n",
    "    keywords_names = [af for af in anchors_focuses]\n",
    "    #print(\"keywords_names\",keywords_names)\n",
    "    #keywords_ids = [i for j in [get_wd_ids(k) for k in keywords_names if get_wd_ids(k)] for i in j]\n",
    "    #print(\"keywords_names\",keywords_ids)\n",
    "    #print(extract_ids(themes[0]))\n",
    "    #print(extract_ids(themes_enhanced))\n",
    "    #keywords_ids = []\n",
    "    #[keywords_ids.append(i) for i in extract_ids(themes[0]) + extract_ids(themes_enhanced) if i not in keywords_ids]\n",
    "    #print(\"keywords_ids\",keywords_ids)\n",
    "    \n",
    "    #print(\"anchors_predicates\",anchors_predicates)\n",
    "    \n",
    "    #print(\"-------START FILTERING-------\")\n",
    "    \n",
    "    hypothesises = []\n",
    "    hypothesises_all = []\n",
    "    hypothesises_tuples_len = len(hypothesises_tuples)\n",
    "    for ht in hypothesises_tuples:\n",
    "        if ht[1] in [a[0] for a in anchors_predicates]:\n",
    "            for i_af, af in enumerate(anchors_focuses):\n",
    "                hypo_sum = 0\n",
    "                nlp_af = get_nlp(af)\n",
    "                nlp_ht0 = get_nlp(get_wd_label(ht[0]))\n",
    "                nlp_ht2 = get_nlp(get_wd_label(ht[2]))\n",
    "                if not nlp_ht2:\n",
    "                    break\n",
    "\n",
    "                af_lemma = ' '.join([e.lower_ for e in nlp_af if e.pos_ != \"DET\"])\n",
    "                ht0_lemma = ' '.join([e.lower_ for e in nlp_ht0 if e.pos_ != \"DET\"])\n",
    "                ht2_lemma = ' '.join([e.lower_ for e in nlp_ht2 if e.pos_ != \"DET\"])\n",
    "                \n",
    "                #if get_wd_label(ht[0]).lower() not in anchors_focuses and get_wd_label(ht[2]).lower() not in anchors_focuses:\n",
    "                #    for es in get_entity_similarity(ht[0], wn[1], max_reward=max_reward):\n",
    "                #        hypo_sum += es\n",
    "                \n",
    "                if (nlp_af.text.lower() != nlp_ht2.text.lower() \n",
    "                    and af_lemma != nlp_ht2[0].text.lower()\n",
    "                    and nlp_af.text.lower() != ht2_lemma\n",
    "                    and af_lemma != ht2_lemma\n",
    "                   ):\n",
    "                    hypo_sum += get_similarity_by_words(nlp_ht2, nlp_af)\n",
    "                    \n",
    "                    if i_af in w_positions:\n",
    "                        for wn in w_names:\n",
    "                            if i_af == wn[0]:\n",
    "                                for es in get_entity_similarity(ht[0], wn[1], max_reward=max_reward):\n",
    "                                    #if ht[0] == \"Q4985\": print(\"before Q4985:\",i_af, es,ht[0], wn[1], hypo_sum)\n",
    "                                    hypo_sum += es\n",
    "                                    #if ht[0] == \"Q4985\": print(\"after Q4985:\",i_af, es,ht[0], wn[1], hypo_sum)\n",
    "                                                       \n",
    "                    for ap in anchors_predicates:\n",
    "                        if ap[0] == ht[1]:\n",
    "                            ht0_label = get_wd_label(ht[0]).lower()\n",
    "                            ht2_label = get_wd_label(ht[2]).lower()\n",
    "                            \n",
    "                            ht0_sum = 0\n",
    "                            ht2_sum = 0\n",
    "                            \n",
    "                            for wn in w_names_only:\n",
    "                                for es in get_entity_similarity(ht[0], wn, max_reward=max_reward):\n",
    "                                    ht0_sum += es\n",
    "                                for es in get_entity_similarity(ht[2], wn, max_reward=max_reward):\n",
    "                                    ht2_sum += es\n",
    "                                    \n",
    "                            for tk in theme_keywords:\n",
    "                                if ht0_label == tk.text.lower() and ht[1] in main_predicate_ids:\n",
    "                                    #print(\"ht0_label\",ht0_label)\n",
    "                                    for wn in w_names_only:\n",
    "                                        for es in get_entity_similarity(ht[2], wn, max_reward=max_reward*2):\n",
    "                                            #print(\"ht0_sum before\",ht0_sum)\n",
    "                                            ht0_sum += es\n",
    "                                            #print(\"ht0_label\",ht2_label,es, ht0_sum, ht)\n",
    "                                if ht2_label == tk.text.lower() and ht[1] in main_predicate_ids:\n",
    "                                    for wn in w_names_only:\n",
    "                                        for es in get_entity_similarity(ht[0], wn, max_reward=max_reward*2):\n",
    "                                            #print(\"ht2_sum before\",ht0_sum)\n",
    "                                            ht2_sum += es\n",
    "                                            #print(\"ht2_label\",ht0_label,es, ht2_sum, ht)\n",
    "                                    \n",
    "                            \n",
    "                            if ht2_label in keywords_names and ht0_label not in keywords_names:\n",
    "                                hypo_sum += ht2_sum\n",
    "                            elif ht0_label in keywords_names and ht2_label not in keywords_names:\n",
    "                                hypo_sum += ht0_sum\n",
    "                            else:\n",
    "                                hypo_sum += ht0_sum\n",
    "                                hypo_sum += ht2_sum\n",
    "                                \n",
    "                            \n",
    "                            hypo_sum *= ap[1]\n",
    "                            \n",
    "                            #if get_wd_label(ht[0]).lower() in keywords_names:\n",
    "                            #    if not i_af in w_positions: \n",
    "                            #        hypo_sum += abs(ap[1])\n",
    "                            #    else: hypo_sum -= abs(ap[1])\n",
    "                            \n",
    "                                \n",
    "                                    \n",
    "                            #if ht[0] == \"Q202725\": print(\"hypo_sum\",hypo_sum)\n",
    "                            \n",
    "                                        \n",
    "\n",
    "                            #else: hypo_sum = ap[1]\n",
    "                            #hypo_sum *= abs(ap[1])\n",
    "                                                        \n",
    "                            \n",
    "                            #break\n",
    "                            #print(\"ap\",ap, \"ht\",ht, \"hypo_sum\",hypo_sum)\n",
    "                            #print(ht)\n",
    "                            #break\n",
    "                            #hypo_sum = abs(hypo_sum)\n",
    "                            #hypo_sum += abs(ap[1])\n",
    "                            #hypo_sum += abs(ap[1])\n",
    "                            #hypo_sum += ap[1]\n",
    "                            #hypo_sum += abs(hypo_sum)\n",
    "                            #hypo_sum *= abs(ap[1])\n",
    "                            \n",
    "                            \n",
    "                            #hypo_sum = abs(hypo_sum)\n",
    "                            #hypo_sum /= ap[1]\n",
    "                            #hypo_sum -= ap[1]\n",
    "                            #hypo_sum += hypo_sum/ap[1]\n",
    "                    \n",
    "                    if get_wd_label(ht[0]).lower() in anchors_focuses: \n",
    "                        if not get_wd_label(ht[2]).lower() in anchors_focuses:\n",
    "                            hypo = ht[2]\n",
    "                        if get_wd_label(ht[2]).lower() in anchors_focuses:\n",
    "                            break\n",
    "                    elif not get_wd_label(ht[0]).lower() in anchors_focuses:\n",
    "                        if get_wd_label(ht[2]).lower() in anchors_focuses:\n",
    "                            hypo = ht[0]\n",
    "                        if not get_wd_label(ht[2]).lower() in anchors_focuses:\n",
    "                            hypothesises_all.append(ht[0])\n",
    "                            if not hypothesises: hypothesises.append([ht[0], hypo_sum])\n",
    "                            else: \n",
    "                                if ht[0] in [h[0] for h in hypothesises]:\n",
    "                                    for i, h in enumerate(hypothesises):\n",
    "                                        if ht[0] == h[0]: hypothesises[i] = [ht[0], hypo_sum+hypothesises[i][1]]\n",
    "                                else: hypothesises.append([ht[0], hypo_sum])\n",
    "                            \n",
    "                            #if \"Q4985\" in ht: print(\"Q4985 ALONE hypo and sum:\", ht[0], hypo_sum)\n",
    "                            hypo = ht[2]\n",
    "                    else:\n",
    "                        #print(\"BREAK\", ht)\n",
    "                        break\n",
    "                    \n",
    "                    #if \"Q4985\" in ht:\n",
    "                    #    print(\"Q4985 hypo and sum:\", hypo, hypo_sum)\n",
    "                    \n",
    "                    hypothesises_all.append(hypo)\n",
    "                    if not hypothesises: hypothesises.append([hypo, hypo_sum])\n",
    "                    else: \n",
    "                        if hypo in [h[0] for h in hypothesises]:\n",
    "                            for i, h in enumerate(hypothesises):\n",
    "                                if hypo == h[0]: hypothesises[i] = [hypo, hypo_sum+hypothesises[i][1]]\n",
    "                        else: hypothesises.append([hypo, hypo_sum])\n",
    "                            \n",
    "    for i_h, h in enumerate(hypothesises):\n",
    "        h_sum = hypothesises_all.count(h[0])\n",
    "        #print(\"h_sum\",h_sum)\n",
    "        #print(\"BEFORE: hypothesises[i_h][1]\",hypothesises[i_h][1])\n",
    "        hypothesises[i_h][1] = hypothesises[i_h][1]/h_sum\n",
    "        #print(\"AFTER: hypothesises[i_h][1]\",hypothesises[i_h][1])\n",
    "    \n",
    "    #print(\"hypothesises_all\",hypothesises_all)    \n",
    "    return sorted(hypothesises, key=lambda x: x[-1], reverse=True)\n",
    "    \n",
    "#hypothesises_2 = get_hypothesises(q_nlp_2, paths_keywords_2, paths_nodes_filtered_2)#, q_themes_2, q_themes_enhanced_2)\n",
    "#print(\"q_nlp_2\", q_nlp_2)\n",
    "#print(\"paths_keywords_2\", paths_keywords_2)\n",
    "#print(\"paths_nodes_filtered\", paths_nodes_filtered)\n",
    "#print(\"hypothesises_2\", hypothesises_2)\n",
    "#print(hypothesises_2)\n",
    "#print([(get_wd_label(h[0]),h[1]) for i,h in enumerate(hypothesises_2)])\n",
    "\n",
    "#if test_verbose: print(\"-> Computing hypothesises...\")\n",
    "#test_hypothesises = get_hypothesises(test_q_nlp, test_q_predicates, test_q_themes, test_paths_keywords, test_paths_nodes_filtered, threshold=0.5, max_reward=2.0)\n",
    "#if test_verbose: print(\"\\n\\n--> test_hypothesises:\",test_hypothesises)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_by_n(l, i):\n",
    "    list_n = []\n",
    "    for j in range(0, len(l)+1, 1):\n",
    "        tmp = l[j-i:i+j-i]\n",
    "        if tmp:\n",
    "            list_n.append(tmp)\n",
    "    return list_n\n",
    "\n",
    "def match_hypothesises(graph, question, themes, predicates, hypothesises, paths, threshold=0.8, max_reward=2.0):\n",
    "    meaningful_paths = []\n",
    "    \n",
    "    w_positions, w_names = w_converter(question)\n",
    "    \n",
    "    theme_ids = sum([t[1] for t in themes[0]],[])\n",
    "    for p in paths:\n",
    "        counter = 0\n",
    "        \n",
    "        for ti in theme_ids:\n",
    "            if ti in p and p not in meaningful_paths:\n",
    "                counter += 1\n",
    "        for pred in [p[1] for p in predicates]:\n",
    "            for e in pred:\n",
    "                if e in p:\n",
    "                    counter += 1\n",
    "                else:\n",
    "                    counter = 0\n",
    "                    \n",
    "        for i_wp, wp in enumerate(w_positions):\n",
    "            if w_names[i_wp][1] and wp<len(p):\n",
    "                for es in get_entity_similarity(p[wp], w_names[i_wp][1], max_reward=max_reward):\n",
    "                    counter += es\n",
    "                \n",
    "        for hypo in hypothesises:\n",
    "            if hypo[0] in p:\n",
    "                counter += 1\n",
    "            if hypo[0] == p[0]:\n",
    "                counter += 1\n",
    "            if hypo[0] == p[-1]:\n",
    "                counter += 1\n",
    "\n",
    "        if counter > 0: meaningful_paths.append((counter, p))\n",
    "\n",
    "    meaningful_paths = sorted(meaningful_paths, key=lambda x: x[0], reverse=True)\n",
    "    #print(\"len(meaningful_paths):\",len(meaningful_paths))\n",
    "    #print(\"\\n\")\n",
    "          \n",
    "    looped_paths = []\n",
    "    for hypo in hypothesises:\n",
    "        for mp in meaningful_paths:\n",
    "            if mp[1][0] == hypo[0] or mp[1][-1] == hypo[0]:\n",
    "                if graph.has_node(mp[1][0]) and graph.has_node(mp[1][-1]):\n",
    "                    path_tmp = list(nx.all_simple_paths(graph, mp[1][0],mp[1][-1]))\n",
    "                    if len(path_tmp)>1:\n",
    "                        for p in path_tmp:\n",
    "                            if p not in [lp[1] for lp in looped_paths]:\n",
    "                                looped_paths.append((mp[0],p))\n",
    "                #else:\n",
    "                #    if not graph.has_node(mp[1][0]):\n",
    "                #        print(\"MISSING NODE:\", mp[1][0], get_wd_label(mp[1][0]))\n",
    "                #    if not graph.has_node(mp[1][-1]):\n",
    "                #        print(\"MISSING NODE:\", mp[1][-1], get_wd_label(mp[1][-1]))\n",
    "                \n",
    "    #print(\"len(looped_paths)\", len(looped_paths))\n",
    "    \n",
    "    looped_paths_untagged = []\n",
    "    for lp in looped_paths:\n",
    "        row_tmp = []\n",
    "        for w in lp[1]:\n",
    "            if w.find(\"-\") > 0:\n",
    "                row_tmp.append(w[:w.find(\"-\")])\n",
    "            else:\n",
    "                row_tmp.append(w)\n",
    "        looped_paths_untagged.append((lp[0],row_tmp))\n",
    "        \n",
    "    #print(\"looped_paths_untagged\",looped_paths_untagged)\n",
    "    \n",
    "    mp_similarities_untagged = []\n",
    "    mp_similarities_tagged = []\n",
    "    mp_similarities_untagged_hypo = []\n",
    "    mp_similarities_tagged_hypo = []\n",
    "    \n",
    "    question_enhanced = []\n",
    "    for q in question:\n",
    "        if q.lemma_ == \"where\": question_enhanced.append(\"location\")\n",
    "        elif q.lemma_ == \"when\": question_enhanced.append(\"date\")\n",
    "        elif q.lemma_ == \"who\": question_enhanced.append(\"person\")    \n",
    "        elif q.lemma_ == \"why\": question_enhanced.append(\"cause\")\n",
    "        else: question_enhanced.append(q.text)\n",
    "    \n",
    "    question_enhanced = nlp(\" \".join([q for q in question_enhanced]))\n",
    "    \n",
    "    #print(\"question\",question)\n",
    "    #print(\"question_enhanced\",question_enhanced)\n",
    "    \n",
    "    #print(\"[h[0] for h in hypothesises]\",[h[0] for h in hypothesises])\n",
    "    \n",
    "    for i_lp, lp in enumerate(looped_paths_untagged):\n",
    "        #print(lp)\n",
    "        sentence = get_nlp(\" \".join([get_wd_label(w) for w in lp[1]]))\n",
    "        similarity = get_similarity_by_words(sentence, question)\n",
    "        similarity_enhanced = get_similarity_by_words(sentence, question_enhanced)\n",
    "        similarity_avg = (similarity+similarity_enhanced)/2*lp[0]\n",
    "        #print(sentence,question,question_enhanced)\n",
    "        #print(\"similarity\", similarity)\n",
    "        #print(\"question_enhanced\", similarity_enhanced)\n",
    "        #mp_similarities_untagged.append((similarity_enhanced,lp[1]))\n",
    "        #mp_similarities_tagged.append((similarity_enhanced,looped_paths[i_lp][1]))\n",
    "        \n",
    "        if lp[1][0] in [h[0] for h in hypothesises]:\n",
    "            #print(\"lp[1][0]\",lp[1][0])\n",
    "            mp_similarities_untagged_hypo.append((similarity_avg, lp[1]))\n",
    "            mp_similarities_tagged_hypo.append((similarity_avg, looped_paths[i_lp][1]))\n",
    "        \n",
    "        mp_similarities_untagged.append((similarity_avg, lp[1]))\n",
    "        mp_similarities_tagged.append((similarity_avg, looped_paths[i_lp][1]))\n",
    "        \n",
    "    #print(\"mp_similarities_untagged\",len(mp_similarities_untagged))\n",
    "    #print(\"mp_similarities_untagged_hypo\",len(mp_similarities_untagged_hypo))\n",
    "    #print(\"mp_similarities_untagged\",mp_similarities_untagged)\n",
    "    \n",
    "    mp_similarities_tagged = sorted(mp_similarities_tagged, key=lambda x: x[0], reverse=True)\n",
    "    mp_similarities_tagged = [mp for mp in mp_similarities_tagged if mp[0] > threshold]\n",
    "    \n",
    "    mp_similarities_untagged = sorted(mp_similarities_untagged, key=lambda x: x[0], reverse=True)\n",
    "    mp_similarities_untagged = [mp for mp in mp_similarities_untagged if mp[0] > threshold]\n",
    "    \n",
    "    #print(\"mp_similarities_untagged\",len(mp_similarities_untagged))\n",
    "            \n",
    "    [mp_similarities_untagged.append(suh) for suh in mp_similarities_untagged_hypo if not suh in mp_similarities_untagged]\n",
    "    [mp_similarities_tagged.append(sth) for sth in mp_similarities_tagged_hypo if not sth in mp_similarities_tagged]\n",
    "    \n",
    "    #print(\"mp_similarities_untagged\",len(mp_similarities_untagged))\n",
    "    #print(\"mp_similarities_tagged\",len(mp_similarities_tagged))\n",
    "    \n",
    "    #WH_FILTER = [\"WDT\", \"WP\", \"WP$\", \"WRB\"]\n",
    "    #wh_position = [w.i for w in question if w.tag_ in WH_FILTER][0]\n",
    "    #question_list = [w.lower_ for w in question if not w.is_punct]\n",
    "    #question_list_filtered = [w.lower_ for w in question if not w.is_punct and w.tag_ not in WH_FILTER]\n",
    "    \n",
    "    #golden_paths = []\n",
    "    #for mp in mp_similarities_tagged:\n",
    "    #    #print(\"mp[1]\",mp[1])\n",
    "    #    for i_e, e in enumerate(mp[1]):\n",
    "    #        if i_e <= 1 or i_e >= len(mp[1])-2:\n",
    "    #            continue\n",
    "    #        if not is_wd_entity(e):\n",
    "    #            continue\n",
    "#\n",
    "    #        mp_e_statements = get_all_statements_of_entity(e)\n",
    "    #        extended_paths = get_statements_by_id(mp_e_statements, e, mp[1][i_e+1][:mp[1][i_e+1].find(\"-\")], qualifier=False, statement_type=\"predicate\")\n",
    "    #        extended_paths_qualifier = get_statements_by_id(mp_e_statements, e, mp[1][i_e+1][:mp[1][i_e+1].find(\"-\")], qualifier=True, statement_type=\"qualifier_predicate\")\n",
    "#\n",
    "    #        for ep in extended_paths_qualifier:\n",
    "    #            if (ep['entity']['id'] == mp[1][i_e] and \n",
    "    #                ep['predicate']['id'] == mp[1][i_e-1][:mp[1][i_e-1].find(\"-\")] and\n",
    "    #                ep['object']['id'] == mp[1][i_e-2] and\n",
    "    #                ep['qualifiers']):\n",
    "    #                for q in ep['qualifiers']:\n",
    "    #                    if(q['qualifier_predicate'][\"id\"] == mp[1][i_e+1][:mp[1][i_e+1].find(\"-\")] and\n",
    "    #                      q['qualifier_object'][\"id\"] == mp[1][i_e+2]):\n",
    "    #                        if mp[1] not in golden_paths:\n",
    "    #                            golden_paths.append(mp[1])\n",
    "#\n",
    "    #            if (ep['entity']['id'] == mp[1][i_e+2] and \n",
    "    #                ep['predicate']['id'] == mp[1][i_e+1][:mp[1][i_e+1].find(\"-\")] and\n",
    "    #                ep['object']['id'] == mp[1][i_e] and\n",
    "    #                ep['qualifiers']):\n",
    "    #                for q in ep['qualifiers']:\n",
    "    #                    if(q['qualifier_predicate'][\"id\"] == mp[1][i_e-1][:mp[1][i_e-1].find(\"-\")] and\n",
    "    #                      q['qualifier_object'][\"id\"] == mp[1][i_e-2]):\n",
    "    #                        if mp[1] not in golden_paths:\n",
    "    #                            golden_paths.append(mp[1])\n",
    "#\n",
    "    #        for ep in extended_paths:\n",
    "    #            if (ep['entity']['id'] == mp[1][i_e] and \n",
    "    #                ep['predicate']['id'] == mp[1][i_e-1][:mp[1][i_e-1].find(\"-\")] and\n",
    "    #                ep['object']['id'] == mp[1][i_e-2] and\n",
    "    #                ep['qualifiers']):\n",
    "    #                for q in ep['qualifiers']:\n",
    "    #                    if(q['qualifier_predicate'][\"id\"] == mp[1][i_e+1][:mp[1][i_e+1].find(\"-\")] and\n",
    "    #                      q['qualifier_object'][\"id\"] == mp[1][i_e+2]):\n",
    "    #                        if mp[1] not in golden_paths:\n",
    "    #                            golden_paths.append(mp[1])\n",
    "#\n",
    "    #            if (ep['entity']['id'] == mp[1][i_e+2] and \n",
    "    #                ep['predicate']['id'] == mp[1][i_e+1][:mp[1][i_e+1].find(\"-\")] and\n",
    "    #                ep['object']['id'] == mp[1][i_e] and\n",
    "    #                ep['qualifiers']):\n",
    "    #                for q in ep['qualifiers']:\n",
    "    #                    if(q['qualifier_predicate'][\"id\"] == mp[1][i_e-1][:mp[1][i_e-1].find(\"-\")] and\n",
    "    #                      q['qualifier_object'][\"id\"] == mp[1][i_e-2]):\n",
    "    #                        if mp[1] not in golden_paths:\n",
    "    #                            golden_paths.append(mp[1])    \n",
    "    #\n",
    "    #print(\"len(golden_paths)\",len(golden_paths))\n",
    "    sorted_golden_paths = []\n",
    "    #for gp in golden_paths:\n",
    "    #    tmp_gp = []\n",
    "    #    #if gp[0] in [h[0] for h in hypothesises]:\n",
    "    #    for e in gp:\n",
    "    #        if is_wd_entity(e):\n",
    "    #            tmp_gp.append(get_wd_label(e))\n",
    "    #        else:\n",
    "    #            tmp_gp.append(get_wd_label(e[:e.find(\"-\")]))\n",
    "    #    nlp_gp = get_nlp(\" \".join(tmp_gp))\n",
    "    #    sorted_golden_paths.append((get_similarity_by_words(question,nlp_gp), gp))\n",
    "#\n",
    "    #sorted_golden_paths = sorted(sorted_golden_paths, key=lambda x: x[0], reverse=True)\n",
    "    #print(\"len(sorted_golden_paths) BEFORE\",len(sorted_golden_paths))\n",
    "    \n",
    "    sorted_golden_paths = [sgp[1] for sgp in sorted_golden_paths]\n",
    "    \n",
    "    if not sorted_golden_paths: \n",
    "        for lp in [lp[1] for lp in looped_paths]:\n",
    "            if lp[0] == hypothesises[0][0]:\n",
    "                if lp not in sorted_golden_paths:\n",
    "                    sorted_golden_paths.append(lp)\n",
    "            if lp[-1] == hypothesises[0][0]:\n",
    "                lp = list(reversed(lp))\n",
    "                if lp not in sorted_golden_paths:\n",
    "                    sorted_golden_paths.append(lp)\n",
    "    \n",
    "    #print(\"len(sorted_golden_paths) AFTER\",len(sorted_golden_paths))\n",
    "        \n",
    "    if not sorted_golden_paths: \n",
    "        for p in paths:\n",
    "            #print(p)\n",
    "            if p[0] == hypothesises[0][0]:\n",
    "                #print(p)\n",
    "                if p not in sorted_golden_paths:\n",
    "                    sorted_golden_paths.append(p)\n",
    "            if p[-1] == hypothesises[0][0]:\n",
    "                p = list(reversed(p))\n",
    "                if p not in sorted_golden_paths:\n",
    "                    sorted_golden_paths.append(p)\n",
    "                    \n",
    "    #print(\"len(sorted_golden_paths) AFTER AFTER\",len(sorted_golden_paths))\n",
    "    \n",
    "    if not sorted_golden_paths:\n",
    "        for p in paths:\n",
    "            if hypothesises[0][0] in p:\n",
    "                if p not in sorted_golden_paths:\n",
    "                    sorted_golden_paths.append(p)\n",
    "                    \n",
    "    #print(\"len(sorted_golden_paths) AFTER AFTER AFTER\",len(sorted_golden_paths))\n",
    "    \n",
    "    \n",
    "    golden_paths_filtered = []\n",
    "    for gp in sorted_golden_paths:\n",
    "        tmp_path = []\n",
    "        for i_e, e in enumerate(gp):\n",
    "            if i_e < len(gp)-2 and not is_wd_entity(e):\n",
    "                if e == gp[i_e+2]:\n",
    "                    golden_paths_filtered.append(gp[:gp.index(e)+2])\n",
    "                    break\n",
    "                else:\n",
    "                    tmp_path.append(e)\n",
    "            else:\n",
    "                tmp_path.append(e)\n",
    "        \n",
    "        if tmp_path:\n",
    "            for i_e, e in enumerate(tmp_path):\n",
    "                if is_wd_entity(e):\n",
    "                    if tmp_path.count(e) > 1:\n",
    "                        pass\n",
    "                    else:\n",
    "                        if tmp_path not in golden_paths_filtered:\n",
    "                            golden_paths_filtered.append(tmp_path)\n",
    "                            \n",
    "    #print(\"len(golden_paths_filtered)\",len(golden_paths_filtered))\n",
    "    \n",
    "    golden_unique_paths = golden_paths_filtered.copy()\n",
    "    for i_sgp, sgp in enumerate(golden_paths_filtered):\n",
    "        for sgp_2 in golden_paths_filtered:\n",
    "            if (is_sublist(sgp, sgp_2) and sgp!=sgp_2):\n",
    "                golden_unique_paths[i_sgp] = []\n",
    "                break\n",
    "    \n",
    "    golden_unique_paths = [gup for gup in golden_unique_paths if gup]\n",
    "    hypothesises_names = [h[0] for h in hypothesises]\n",
    "    \n",
    "    #print(\"golden_paths_filtered\",golden_paths_filtered)\n",
    "    #print(\"before hypothesises_names\",hypothesises_names)\n",
    "    #print(\"golden_unique_paths[0][0]\",golden_unique_paths[0][0])\n",
    "    #print(\"hypothesises_names\",hypothesises_names)\n",
    "    \n",
    "    #if is_valide_wd_id(hypothesises_names[0]):\n",
    "    \n",
    "    if golden_unique_paths and hypothesises_names:\n",
    "        if golden_unique_paths[0] and hypothesises_names[0]:\n",
    "            if golden_unique_paths[0][0]:\n",
    "                if (not is_wd_entity(hypothesises_names[0])\n",
    "                    and is_wd_entity(golden_unique_paths[0][0]) \n",
    "                    or hypothesises_names[0] != golden_unique_paths[0][0]):\n",
    "                    if golden_unique_paths[0][0] in hypothesises_names:\n",
    "                        hypothesises_names.pop(hypothesises_names.index(golden_unique_paths[0][0]))\n",
    "                        hypothesises_names.insert(0,golden_unique_paths[0][0])\n",
    "    \n",
    "    #print(\"after hypothesises_names\",hypothesises_names)\n",
    "            \n",
    "    golden_unique_paths = [gup for gup in golden_unique_paths if gup[0] == hypothesises_names[0]]\n",
    "            \n",
    "    #elif hypothesises_names[0] != golden_unique_paths[0][0]\n",
    "    \n",
    "    golden_unique_paths = [hypothesises_names]+golden_unique_paths\n",
    "    \n",
    "    return golden_unique_paths\n",
    "\n",
    "\n",
    "#start_time = time.time()\n",
    "#golden_paths_2 = match_hypothesises(graph_2, q_nlp_2, q_themes_2, q_predicates_2, hypothesises_2, paths_nodes_filtered_2)\n",
    "#end_time = time.time()\n",
    "#print(\"Golden paths ->\\tRunning time is {}s\".format(round(end_time-start_time,2)))\n",
    "#print(golden_paths_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## questions = (\"what was the cause of death of yves klein\",\n",
    "#               \"Who is the wife of Barack Obama?\",\n",
    "#               \"Who is the president of the United States?\",\n",
    "#               \"When was produced the first Matrix movie?\",\n",
    "#               \"Who made the soundtrack of the The Last Unicorn movie?\",\n",
    "#               \"Who is the author of Le Petit Prince?\",\n",
    "#               \"Which actor voiced the Unicorn in The Last Unicorn?\",\n",
    "#               \"how is called the rabbit in Alice in Wonderland?\"\n",
    "#              )\n",
    "\n",
    "#def print_running_time(start_time, end_time=time.time()):\n",
    "#    print(\"->\\tRunning time is {}s\".format(round(end_time-start_time,2)))\n",
    "\n",
    "def answer_question(question, verbose=False, aggressive=False, looped=False, deep_k=30, deep_k_step=20, timer=False, g_paths=False):\n",
    "    if verbose: start_time = time.time()\n",
    "    if timer: timer_time = time.time()\n",
    "    if verbose: print(\"Auto correcting question:\",question)\n",
    "    q_nlp = get_nlp(question, autocorrect=True)\n",
    "    if verbose: print(\"-> q_nlp:\",q_nlp)\n",
    "    q_themes = get_themes(q_nlp, top_k=3)\n",
    "    if verbose: print(\"-> q_themes:\",q_themes)\n",
    "    q_themes_enhanced = get_enhanced_themes(q_themes, top_k=1, aggressive=aggressive)\n",
    "    if verbose: print(\"-> q_themes_enhanced:\",q_themes_enhanced)\n",
    "    if verbose: print(\"--> Calculating predicates... (could be long.. depends on uncached unpure predicates)\")\n",
    "    q_predicates_db = get_predicates(q_nlp, q_themes, top_k=0)\n",
    "    q_predicates_online = get_predicates_online(q_nlp, top_k=2, aggressive=aggressive)\n",
    "    q_predicates = []\n",
    "    q_predicates_db_ids = [p[1] for p in q_predicates_db]\n",
    "    q_predicates_db_names = [p[0] for p in q_predicates_db]\n",
    "    q_predicates_online_ids = [p[1] for p in q_predicates_online]\n",
    "    q_predicates_online_names = [p[0] for p in q_predicates_online]\n",
    "    for i_n,n in enumerate(q_predicates_db_names):\n",
    "        pn_online_text = [n.text for n in q_predicates_online_names]\n",
    "        tmp_ids = q_predicates_db_ids[i_n]\n",
    "        if n.text in pn_online_text:\n",
    "            for p_o in q_predicates_online_ids[pn_online_text.index(n.text)]:\n",
    "                if p_o not in tmp_ids:\n",
    "                    tmp_ids.append(p_o)\n",
    "        q_predicates.append((n,tmp_ids))\n",
    "    \n",
    "    for i_n_o,n_o in enumerate(q_predicates_online_names):\n",
    "        n_db_text = [n.text for n in q_predicates_db_names]\n",
    "        if n_o.text not in n_db_text:\n",
    "            q_predicates.append((n_o, q_predicates_online_ids[i_n_o]))\n",
    "    \n",
    "    if verbose: print(\"-> q_predicates:\",q_predicates)\n",
    "    if timer: \n",
    "        print(\"->\\tRunning time is {}s\".format(round(time.time()-timer_time,2)))\n",
    "        timer_time = time.time()\n",
    "    q_focused_parts = get_focused_parts(q_nlp, q_themes, top_k=2)\n",
    "    if verbose: print(\"-> q_focused_parts:\",q_focused_parts)\n",
    "    if verbose: print(\"-> Building the graph with k_deep\",str(deep_k),\"... (could be long)\")\n",
    "    if deep_k<=10:\n",
    "        deep_k = 10\n",
    "        graph, predicates_dict = build_graph(q_nlp, q_themes, q_themes_enhanced, q_predicates, deep_k=deep_k)\n",
    "        if timer: \n",
    "            print(\"->\\tRunning time is {}s\".format(round(time.time()-timer_time,2)))\n",
    "            timer_time = time.time()\n",
    "    else:\n",
    "        for k in range(10, deep_k, deep_k_step):\n",
    "            graph, predicates_dict = build_graph(q_nlp, q_themes, q_themes_enhanced, q_predicates, deep_k=deep_k)\n",
    "            if timer: \n",
    "                print(\"->\\tRunning time is {}s\".format(round(time.time()-timer_time,2)))\n",
    "                timer_time = time.time()\n",
    "            if graph.size() > 1000 or len(graph) > 1000 or deep_k<=10:\n",
    "                break\n",
    "            elif graph.size() > 500 or len(graph) > 500:\n",
    "                deep_k -= deep_k_step\n",
    "                if verbose: print(\"---> Rebuilding the graph with k_deep\",str(deep_k), \"... Previously:\",len(graph), \"nodes or\", graph.size(), \"edges was above the limit...\")\n",
    "            else: break\n",
    "    if verbose: print(\"--> \",len(graph), \"nodes and\", graph.size(), \"edges\")\n",
    "    if graph.size() > 510 or len(graph) > 510:\n",
    "        if verbose: print(\"Stopping the computing here, too computational.\")\n",
    "        return False\n",
    "    if verbose: print(\"-> predicates_dict:\",predicates_dict)\n",
    "    paths_keywords = find_paths_keywords(graph, q_nlp, q_themes, q_themes_enhanced, q_predicates, q_focused_parts)\n",
    "    if verbose: print(\"-> paths_keywords:\",paths_keywords)\n",
    "    if timer: timer_time = time.time()\n",
    "    if verbose: print(\"-> Computing possible paths... (could be long)\")\n",
    "    path_nodes = find_path_nodes_from_graph(graph, paths_keywords, threshold=0.8, thres_inter=0.1, top_performance=graph.size(),min_paths=3000)\n",
    "    if verbose: print(\"--> len(path_nodes):\",len(path_nodes))\n",
    "    if timer: \n",
    "        print(\"->\\tRunning time is {}s\".format(round(time.time()-timer_time,2)))\n",
    "        timer_time = time.time()\n",
    "    \n",
    "    if len(path_nodes) < 20000:\n",
    "        if verbose: print(\"-> Filtering paths... (could be long)\")\n",
    "        paths_nodes_filtered = paths_nodes_filter(path_nodes, graph)\n",
    "        if verbose: print(\"--> len(paths_nodes_filtered):\",len(paths_nodes_filtered))\n",
    "        if timer: \n",
    "            print(\"->\\tRunning time is {}s\".format(round(time.time()-timer_time,2)))\n",
    "            timer_time = time.time()\n",
    "    else: \n",
    "        if verbose: print(\"--> Skipping paths filtering... (too much paths)\")\n",
    "        paths_nodes_filtered = path_nodes\n",
    "    if verbose: print(\"-> Computing hypothesises...\")\n",
    "    hypothesises = get_hypothesises(q_nlp, q_predicates, q_themes, paths_keywords, paths_nodes_filtered, threshold=0.5, max_reward=2.0) \n",
    "    if verbose: print(\"--> hypothesises:\",hypothesises)\n",
    "    if timer: \n",
    "        print(\"->\\tRunning time is {}s\".format(round(time.time()-timer_time,2)))\n",
    "        timer_time = time.time()\n",
    "    if g_paths:\n",
    "        if hypothesises:\n",
    "            if verbose: print(\"-> Computing golden paths...\")\n",
    "            golden_paths = match_hypothesises(graph, q_nlp, q_themes, q_predicates, hypothesises, paths_nodes_filtered, threshold=0.8, max_reward=2.0)\n",
    "            if verbose: print(\"--> len(golden_paths):\",len(golden_paths)-1)\n",
    "            if timer: \n",
    "                print(\"->\\tRunning time is {}s\".format(round(time.time()-timer_time,2)))\n",
    "                timer_time = time.time()\n",
    "        else:\n",
    "            if not looped:\n",
    "                if verbose: print(\"-> Looping on aggressive mode...\")\n",
    "                golden_paths = answer_question(question, verbose=verbose, aggressive=True, looped=True, deep_k=deep_k)\n",
    "            else: \n",
    "                if verbose: print(\"--> End of loop\")\n",
    "                golden_paths=[]\n",
    "\n",
    "    save_cache_data()\n",
    "    \n",
    "    if g_paths:\n",
    "        if golden_paths:\n",
    "            cleared_golden_paths = [golden_paths[0].copy()]\n",
    "            for p in golden_paths[1:]:\n",
    "                tmp_translation = []\n",
    "                for e in p:\n",
    "                    if is_wd_entity(e) or is_wd_predicate(e):\n",
    "                        tmp_translation.append(e)\n",
    "                    else:\n",
    "                        if e[0] == \"P\": tmp_translation.append(e[:e.find(\"-\")])\n",
    "                        else: tmp_translation.append(e)\n",
    "                if tmp_translation not in cleared_golden_paths:\n",
    "                    cleared_golden_paths.append(tmp_translation)\n",
    "\n",
    "            if verbose: print(\"--> len(cleared_golden_paths):\",len(cleared_golden_paths)-1)\n",
    "            if timer: timer_time = time.time()\n",
    "            \n",
    "    if verbose: print(\"->\\tTotal Running time is {}s\".format(round(time.time()-start_time,2)))\n",
    "    \n",
    "    if g_paths:\n",
    "        if golden_paths:\n",
    "            return cleared_golden_paths\n",
    "        else: return False\n",
    "    else:\n",
    "        if hypothesises:\n",
    "            return [[a[0] for a in hypothesises]] + [[hypothesises[0][0]]]\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "\n",
    "#answer = answer_question(\"what film is by the writer phil hay?\", verbose=True, timer=True) #444.36s\n",
    "#answer = answer_question(\"When was produced the first Matrix movie?\", verbose=True, timer=True) #70.67s\n",
    "#answer = answer_question(\"Which actor voiced the Unicorn in The Last Unicorn?\", verbose=True, timer=True) #works 312.12s\n",
    "#answer = answer_question(\"Who voiced the Unicorn in The Last Unicorn?\", verbose=True, timer=True) #works 323.52s\n",
    "#answer = answer_question(\"How many actors voiced the Unicorn in The Last Unicorn?\", verbose=True, timer=True) #592.22s \n",
    "#answer = answer_question(\"Which is the nation of Martha Mattox\", verbose=True, timer=True) #97.89s\n",
    "#answer = answer_question(\"Who made the soundtrack of the The Last Unicorn movie?\", verbose=True, timer=True)\n",
    "#answer = answer_question(\"Who is the author of Le Petit Prince?\", verbose=True, timer=True)\n",
    "\n",
    "#answer = answer_question(\"When was produced the first Matrix movie?\", verbose=True, timer=True)\n",
    "#answer = answer_question(\"Who is the president of the United States?\", verbose=True, timer=True) #node Q76 not in graph 324.88s\n",
    "#answer = answer_question(\"Who is the wife of Barack Obama?\", verbose=True, timer=True) #works 275.94s\n",
    "#answer = answer_question(\"what was the cause of death of yves klein\", verbose=True, timer=True) #309.06s\n",
    "#answer = answer_question(\"what city was alex golfis born in\", verbose=True, timer=True)\n",
    "#answer = answer_question(\"which stadium do the wests tigers play in\", verbose=True, timer=True) #462.47s\n",
    "#answer = answer_question(\"lol\", verbose=True, timer=True)\n",
    "#answer = answer_question(\"what's akbar tandjung's ethnicity\", verbose=True, timer=True)\n",
    "\n",
    "#answer = answer_question(\"Which equestrian was is in dublin ?\", verbose=True, timer=True)\n",
    "#answer = answer_question(\"how does engelbert zaschka identify\t\", verbose=True, timer=True)\n",
    "#answer = answer_question(\"Who influenced michael mcdowell?\", verbose=True, timer=True)\n",
    "#answer = answer_question(\"what does  2674 pandarus orbit\", verbose=True, timer=True)\n",
    "#answer = answer_question(\"what production company was involved in smokin' aces 2: assasins' ball\", verbose=True, timer=True)\n",
    "#answer = answer_question(\"who's a kung fu star from hong kong\", verbose=True, timer=True)\n",
    "#answer = answer_question(\"Where did roger marquis die\", verbose=True, timer=True) # works 64.56s\n",
    "#answer = answer_question(\"Which genre of album is harder.....faster?\", verbose=True, timer=True)\n",
    "#answer = answer_question(\"Which equestrian was born in dublin?\", verbose=True, timer=True)\n",
    "#answer = answer_question(\"Who is the author that wrote the book Moby Dick\", verbose=True, timer=True) #314.04s works\n",
    "#answer = answer_question(\"Name a person who died from bleeding.\", verbose=True, timer=True) # 117.35s\n",
    "#answer = answer_question(\"What is the name of the person who created Saved by the Bell?\", verbose=True, timer=True)\n",
    "#answer = answer_question(\"of what nationality is ken mcgoogan\", verbose=True, timer=True) #works 51.39s\n",
    "\n",
    "#answer = answer_question(\"Which actor voiced the Unicorn in The Last Unicorn?\", verbose=True, timer=True, g_paths=False)\n",
    "#if answer:\n",
    "#    print(\"Answer:\",get_wd_label(answer[0][0]), \"(\"+str(answer[0][0])+\")\")\n",
    "#    print(\"Paths:\",[[get_wd_label(e) for e in row] for row in answer[1:]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "#questions = [\n",
    "#    \"Where did roger marquis die\",\n",
    "#    \"Which genre of album is harder.....faster?\",\n",
    "#    \"Which actor voiced the Unicorn in The Last Unicorn?\",\n",
    "#    \"Who voiced the Unicorn in The Last Unicorn?\",\n",
    "#    \"Which is the nation of Martha Mattox\",\n",
    "#    \"Who is the wife of Barack Obama?\",\n",
    "#    \"of what nationality is ken mcgoogan\",\n",
    "#    \"which stadium do the wests tigers play in\",\n",
    "#    \"Who is the author that wrote the book Moby Dick\",\n",
    "#    \"Which equestrian was is in dublin ?\",\n",
    "#    \"how does engelbert zaschka identify\t\",\n",
    "#    \"Who influenced michael mcdowell?\",\n",
    "#    \"what does  2674 pandarus orbit\"\n",
    "#            ]\n",
    "#\n",
    "#for question in questions:\n",
    "#    answer = answer_question(question, verbose=True, timer=True, g_paths=False)\n",
    "#    print(\"Answer:\",get_wd_label(answer[0][0]), \"(\"+str(answer[0][0])+\")\\n\")\n",
    "#    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to_translate = ['Q13133', 'P26', 'Q76', 'P31', 'Q5', 'P31', 'Q24039104', 'P21', 'Q6581072', 'P1552', 'Q188830', 'P26', 'Q18531596']\n",
    "#to_translate = ['Q202725', 'P725', 'Q176198', 'P453', 'Q30060419', 'P31', 'Q30167264', 'P1889', 'Q7246', 'P138', 'Q18356448']\n",
    "#\n",
    "#masked = []\n",
    "#for tt in to_translate:\n",
    "#    masked.append(get_wd_label(tt))\n",
    "#    masked.append(\"[MASK]\")\n",
    "#print(\"marked\",masked)\n",
    "#\n",
    "#print(\"->\",[get_wd_label(e) for e in to_translate])\n",
    "#print(\"-->\",\" \".join([get_wd_label(e) for e in to_translate]))\n",
    "#\n",
    "#print(\"-->\",\" [MASK] \".join([get_wd_label(e) for e in to_translate]))\n",
    "#print(\"-->\",\"[\" +\" , [MASK] , \".join([get_wd_label(e) for e in to_translate])+\"]\")\n",
    "#\n",
    "#FILTER_ELEMENTS = ['P31']\n",
    "#filtered_by_elements = [\"[MASK]\" if x in FILTER_ELEMENTS else get_wd_label(x) for x in to_translate]\n",
    "#\n",
    "#print(\"-->\",\" [MASK] \".join([e for e in filtered_by_elements]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#graph_2.has_node(\"Q13133\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get_similarity_by_words(get_nlp(\"mia farrow\"), get_nlp(\"farrow mia\")) #1.000000077374981\n",
    "#get_similarity_by_words(get_nlp(\"actor voiced\"), get_nlp(\"voice actor\")) #0.8541489425987572 \n",
    "#get_similarity_by_words(get_nlp(\"actor voiced the unicorn in the last unicorn\"), \n",
    "#                        get_nlp(\"the unicorn last unicorn actor voiced\")) #0.9573255410217848\n",
    "#get_similarity_by_words(get_nlp(\"voice actor\"),get_nlp(\"instance of\")) #0.30931508860569823\n",
    "#get_similarity_by_words(get_nlp(\"voice actor\"),get_nlp(\"present in work\")) #0.34966764303274056\n",
    "#get_similarity_by_words(get_nlp(\"voice actor\"),get_nlp(\"subject has role\")) #0.5026860362728758\n",
    "\n",
    "#get_similarity_by_words(get_nlp(\"voice actor\"),get_nlp(\"protagonist\")) #0.4688377364169893\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#subgraphs = [graph.subgraph(c) for c in nx.connected_components(graph)]\n",
    "#print(len(subgraphs))\n",
    "#[len(s.nodes) for s in subgraphs]\n",
    "#len(subgraphs[0].nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for path in nx.all_simple_paths(graph, source=\"Q176198\", target=\"Q202725\"):\n",
    "#    print(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nx.shortest_path(graph, source=\"Q176198\", target=\"Q202725\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nlp_lookup_test = get_nlp(\"klein yves\")\n",
    "#[y['name'] for x,y in graph.nodes(data=True) if get_nlp(y['name']).similarity(nlp_lookup_test) >= 0.9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#list(nx.dfs_labeled_edges(graph, source=get_themes(q0_nlp, top_k=3)[0][0][1][0], depth_limit=4))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot_graph(graph_2, \"test_file_name_graph\", \"Graph_title\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nlp_lookup_test = get_nlp(\"klein yves\")\n",
    "#[y['name'] for x,y in graph.nodes(data=True) if get_nlp(y['name']).similarity(nlp_lookup_test) >= 0.9]\n",
    "#[y['name'] for x,y in graph_2.nodes(data=True) if y['type'] == 'predicate']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get_nlp(get_wd_label(\"Q13133\")).similarity(get_nlp(\"person\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get_similarity_by_words(get_nlp(\"PERSON\"),get_nlp(\"person\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get_similarity_by_words(get_nlp(\"GPE\"),get_nlp(\"location\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get_most_similar(\"Michelle Obama\", topn=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#doc_ents_tmp = get_kb_ents(\"Michelle Obama\")\n",
    "#for ent in doc_ents_tmp:\n",
    "#    print(\" \".join([\"ent\", ent.text, ent.label_, ent.kb_id_]))\n",
    "#\n",
    "#doc_ents_tmp = get_kb_ents(\"New York\")\n",
    "#for ent in doc_ents_tmp:\n",
    "#    print(\" \".join([\"ent\", ent.text, ent.label_, ent.kb_id_]))\n",
    "#    \n",
    "#doc_ents_tmp = get_kb_ents(\"Electrocution\")\n",
    "#for ent in doc_ents_tmp:\n",
    "#    print(\" \".join([\"ent\", ent.text, ent.label_, ent.kb_id_]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nlp.get_vector(\"Q42\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:qa]",
   "language": "python",
   "name": "conda-env-qa-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
