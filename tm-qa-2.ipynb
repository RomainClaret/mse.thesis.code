{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "IPython.notebook.set_autosave_interval(180000)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autosaving every 180 seconds\n"
     ]
    }
   ],
   "source": [
    "%autosave 180\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /data/users/romain.claret/miniconda3/envs/qa/lib/python3.7/site-packages/txt2txt/txt2txt.py:25: The name tf.GPUOptions is deprecated. Please use tf.compat.v1.GPUOptions instead.\n",
      "\n",
      "WARNING:tensorflow:From /data/users/romain.claret/miniconda3/envs/qa/lib/python3.7/site-packages/txt2txt/txt2txt.py:27: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "WARNING:tensorflow:From /data/users/romain.claret/miniconda3/envs/qa/lib/python3.7/site-packages/txt2txt/txt2txt.py:27: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the params file\n",
      "Input encoding {'o': 2, '{': 3, '.': 4, 'J': 5, '0': 6, '1': 7, '<': 8, 'B': 9, 'd': 10, '£': 11, 'e': 12, '6': 13, '!': 14, 'O': 15, 'M': 16, 'X': 17, 'f': 18, 't': 19, 'C': 20, 'V': 21, 'z': 22, 'K': 23, '\\\\': 24, '9': 25, 'P': 26, 'S': 27, '/': 28, '₹': 29, 'F': 30, 'G': 31, '=': 32, '8': 33, ')': 34, '+': 35, ']': 36, 'U': 37, \"'\": 38, '\"': 39, 'g': 40, 'N': 41, 'r': 42, 'u': 43, '&': 44, '$': 45, 'x': 46, '%': 47, ':': 48, '@': 49, '^': 50, 'I': 51, 'L': 52, 'Z': 53, 'h': 54, 'W': 55, 'A': 56, 'v': 57, '?': 58, '2': 59, '~': 60, 's': 61, 'T': 62, 'R': 63, ',': 64, '|': 65, '4': 66, '>': 67, 'y': 68, '(': 69, '[': 70, 'k': 71, 'H': 72, 'l': 73, 'j': 74, '7': 75, 'n': 76, 'i': 77, 'D': 78, 'Q': 79, ' ': 80, 'm': 81, 'Y': 82, '*': 83, '}': 84, '#': 85, 'p': 86, 'q': 87, '5': 88, 'c': 89, '`': 90, 'a': 91, 'b': 92, 'w': 93, '3': 94, 'E': 95, ';': 96, '-': 97}\n",
      "Input decoding {2: 'o', 3: '{', 4: '.', 5: 'J', 6: '0', 7: '1', 8: '<', 9: 'B', 10: 'd', 11: '£', 12: 'e', 13: '6', 14: '!', 15: 'O', 16: 'M', 17: 'X', 18: 'f', 19: 't', 20: 'C', 21: 'V', 22: 'z', 23: 'K', 24: '\\\\', 25: '9', 26: 'P', 27: 'S', 28: '/', 29: '₹', 30: 'F', 31: 'G', 32: '=', 33: '8', 34: ')', 35: '+', 36: ']', 37: 'U', 38: \"'\", 39: '\"', 40: 'g', 41: 'N', 42: 'r', 43: 'u', 44: '&', 45: '$', 46: 'x', 47: '%', 48: ':', 49: '@', 50: '^', 51: 'I', 52: 'L', 53: 'Z', 54: 'h', 55: 'W', 56: 'A', 57: 'v', 58: '?', 59: '2', 60: '~', 61: 's', 62: 'T', 63: 'R', 64: ',', 65: '|', 66: '4', 67: '>', 68: 'y', 69: '(', 70: '[', 71: 'k', 72: 'H', 73: 'l', 74: 'j', 75: '7', 76: 'n', 77: 'i', 78: 'D', 79: 'Q', 80: ' ', 81: 'm', 82: 'Y', 83: '*', 84: '}', 85: '#', 86: 'p', 87: 'q', 88: '5', 89: 'c', 90: '`', 91: 'a', 92: 'b', 93: 'w', 94: '3', 95: 'E', 96: ';', 97: '-'}\n",
      "Output encoding {'o': 2, '{': 3, '.': 4, 'J': 5, '0': 6, '1': 7, '<': 8, 'B': 9, 'd': 10, '£': 11, 'e': 12, '6': 13, '!': 14, 'O': 15, 'M': 16, 'X': 17, 'f': 18, 't': 19, 'C': 20, 'V': 21, 'z': 22, 'K': 23, '\\\\': 24, '9': 25, 'P': 26, 'S': 27, '/': 28, '₹': 29, 'F': 30, 'G': 31, '=': 32, '8': 33, ')': 34, '+': 35, ']': 36, 'U': 37, \"'\": 38, '\"': 39, 'g': 40, 'N': 41, 'r': 42, 'u': 43, '&': 44, '$': 45, 'x': 46, '%': 47, ':': 48, '@': 49, '^': 50, 'I': 51, 'L': 52, 'Z': 53, 'h': 54, 'W': 55, 'A': 56, 'v': 57, '?': 58, '2': 59, '~': 60, 's': 61, 'T': 62, 'R': 63, ',': 64, '|': 65, '4': 66, '>': 67, 'y': 68, '(': 69, '[': 70, 'k': 71, 'H': 72, 'l': 73, 'j': 74, '7': 75, 'n': 76, 'i': 77, 'D': 78, 'Q': 79, ' ': 80, 'm': 81, 'Y': 82, '*': 83, '}': 84, '#': 85, 'p': 86, 'q': 87, '5': 88, 'c': 89, '`': 90, 'a': 91, 'b': 92, 'w': 93, '3': 94, 'E': 95, ';': 96, '-': 97}\n",
      "Output decoding {2: 'o', 3: '{', 4: '.', 5: 'J', 6: '0', 7: '1', 8: '<', 9: 'B', 10: 'd', 11: '£', 12: 'e', 13: '6', 14: '!', 15: 'O', 16: 'M', 17: 'X', 18: 'f', 19: 't', 20: 'C', 21: 'V', 22: 'z', 23: 'K', 24: '\\\\', 25: '9', 26: 'P', 27: 'S', 28: '/', 29: '₹', 30: 'F', 31: 'G', 32: '=', 33: '8', 34: ')', 35: '+', 36: ']', 37: 'U', 38: \"'\", 39: '\"', 40: 'g', 41: 'N', 42: 'r', 43: 'u', 44: '&', 45: '$', 46: 'x', 47: '%', 48: ':', 49: '@', 50: '^', 51: 'I', 52: 'L', 53: 'Z', 54: 'h', 55: 'W', 56: 'A', 57: 'v', 58: '?', 59: '2', 60: '~', 61: 's', 62: 'T', 63: 'R', 64: ',', 65: '|', 66: '4', 67: '>', 68: 'y', 69: '(', 70: '[', 71: 'k', 72: 'H', 73: 'l', 74: 'j', 75: '7', 76: 'n', 77: 'i', 78: 'D', 79: 'Q', 80: ' ', 81: 'm', 82: 'Y', 83: '*', 84: '}', 85: '#', 86: 'p', 87: 'q', 88: '5', 89: 'c', 90: '`', 91: 'a', 92: 'b', 93: 'w', 94: '3', 95: 'E', 96: ';', 97: '-'}\n",
      "WARNING:tensorflow:From /data/users/romain.claret/miniconda3/envs/qa/lib/python3.7/site-packages/tensorflow/python/keras/backend.py:3673: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            (None, 202)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_1 (InputLayer)            (None, 202)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)         (None, 202, 256)     25088       input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 202, 128)     12544       input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_2 (LSTM)                   (None, 202, 256)     525312      embedding_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional) (None, 202, 256)     263168      embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dot_1 (Dot)                     (None, 202, 202)     0           lstm_2[0][0]                     \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "attention (Activation)          (None, 202, 202)     0           dot_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dot_2 (Dot)                     (None, 202, 256)     0           attention[0][0]                  \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 202, 512)     0           dot_2[0][0]                      \n",
      "                                                                 lstm_2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_1 (TimeDistrib (None, 202, 128)     65664       concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_2 (TimeDistrib (None, 202, 98)      12642       time_distributed_1[0][0]         \n",
      "==================================================================================================\n",
      "Total params: 904,418\n",
      "Trainable params: 904,418\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#import convex as cx\n",
    "import requests\n",
    "import time\n",
    "import itertools as it\n",
    "import re\n",
    "#import numpy\n",
    "from copy import copy\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "#from pprint import pprint\n",
    "import time\n",
    "import json\n",
    "import os\n",
    "import networkx as nx\n",
    "from math import sqrt\n",
    "import spacy\n",
    "from hdt import HDTDocument\n",
    "import multiprocessing as mp\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = \"1,2\"\n",
    "from deepcorrect import DeepCorrect\n",
    "\n",
    "#import deepcorrect\n",
    "#print(deepcorrect.__file__)\n",
    "corrector = DeepCorrect('data/deep_punct/deeppunct_params_en', 'data/deep_punct/deeppunct_checkpoint_wikipedia')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#corrector.correct('of what nationality is ken mcgoogan')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdt_wd = HDTDocument(\"data/kb/wikidata2018_09_11.hdt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nlp = spacy.load(\"en_core_web_lg\")\n",
    "nlp = spacy.load(\"/data/users/romain.claret/tm/wiki-kb-linked-entities/nlp_custom_6\")\n",
    "#print(nlp.pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load settings\n",
    "with open( \"settings-graphqa.json\", \"r\") as settings_data:\n",
    "    settings = json.load(settings_data)\n",
    "    use_cache = settings['use_cache']\n",
    "    save_cache = settings['save_cache']\n",
    "    cache_path = settings['cache_path']\n",
    "#cache_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_cache = True\n",
    "def save_cache_data():\n",
    "    if save_cache:\n",
    "        with open(os.path.join(cache_path,'wd_local_statements_dict.json'), 'wb') as outfile:\n",
    "            outfile.write(json.dumps(wd_local_statements_dict, separators=(',',':')).encode('utf8'))\n",
    "        with open(os.path.join(cache_path,'wd_labels_dict.json'), 'wb') as outfile:\n",
    "            outfile.write(json.dumps(wd_labels_dict, separators=(',',':')).encode('utf8'))\n",
    "        with open(os.path.join(cache_path,'wd_local_word_ids_dict.json'), 'wb') as outfile:\n",
    "            outfile.write(json.dumps(wd_local_word_ids_dict, separators=(',',':')).encode('utf8'))\n",
    "        with open(os.path.join(cache_path,'wd_online_word_ids_dict.json'), 'wb') as outfile:\n",
    "            outfile.write(json.dumps(wd_online_word_ids_dict, separators=(',',':')).encode('utf8'))\n",
    "        with open(os.path.join(cache_path,'wd_local_predicate_ids_dict.json'), 'wb') as outfile:\n",
    "            outfile.write(json.dumps(wd_local_predicate_ids_dict, separators=(',',':')).encode('utf8'))\n",
    "        with open(os.path.join(cache_path,'wd_online_predicate_ids_dict.json'), 'wb') as outfile:\n",
    "            outfile.write(json.dumps(wd_online_predicate_ids_dict, separators=(',',':')).encode('utf8'))\n",
    "        with open(os.path.join(cache_path,'word_similarities_dict.json'), 'wb') as outfile:\n",
    "            outfile.write(json.dumps(word_similarities_dict, separators=(',',':')).encode('utf8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load statements cache\n",
    "use_cache = True\n",
    "if use_cache:\n",
    "    path_wd_local_statements_dict = \"wd_local_statements_dict.json\"\n",
    "    path_wd_labels_dict = 'wd_labels_dict.json'\n",
    "    path_wd_local_word_ids_dict = 'wd_local_word_ids_dict.json'\n",
    "    path_wd_online_word_ids_dict = 'wd_online_word_ids_dict.json'\n",
    "    path_wd_local_predicate_ids_dict = 'wd_local_predicate_ids_dict.json'\n",
    "    path_wd_online_predicate_ids_dict = 'wd_online_predicate_ids_dict.json'\n",
    "    path_word_similarities_dict = 'word_similarities_dict.json'\n",
    "else:\n",
    "    path_wd_local_statements_dict = \"wd_local_statements_dict_empty.json\"\n",
    "    path_wd_labels_dict = 'wd_labels_dict_empty.json'\n",
    "    path_wd_local_word_ids_dict = 'wd_local_word_ids_dict_empty.json'\n",
    "    path_wd_online_word_ids_dict = 'wd_online_word_ids_dict_empty.json'\n",
    "    path_wd_local_predicate_ids_dict = 'wd_local_predicate_ids_dict_empty.json'\n",
    "    path_wd_online_predicate_ids_dict = 'wd_online_predicate_ids_dict_empty.json'\n",
    "    path_word_similarities_dict = 'word_similarities_dict_empty.json'\n",
    "\n",
    "with open(os.path.join(cache_path,path_wd_local_statements_dict), \"rb\") as data:\n",
    "    wd_local_statements_dict = json.load(data)\n",
    "with open(os.path.join(cache_path,path_wd_labels_dict), \"rb\") as data:\n",
    "    wd_labels_dict = json.load(data)\n",
    "with open(os.path.join(cache_path,path_wd_local_word_ids_dict), \"rb\") as data:\n",
    "    wd_local_word_ids_dict = json.load(data)\n",
    "with open(os.path.join(cache_path,path_wd_online_word_ids_dict), \"rb\") as data:\n",
    "    wd_online_word_ids_dict = json.load(data)\n",
    "with open(os.path.join(cache_path,path_wd_local_predicate_ids_dict), \"rb\") as data:\n",
    "    wd_local_predicate_ids_dict = json.load(data)\n",
    "with open(os.path.join(cache_path,path_wd_online_predicate_ids_dict), \"rb\") as data:\n",
    "    wd_online_predicate_ids_dict = json.load(data)\n",
    "with open(os.path.join(cache_path,path_word_similarities_dict), \"rb\") as data:\n",
    "    word_similarities_dict = json.load(data)\n",
    "\n",
    "#print(\"len(wd_local_statements_dict)\",len(wd_local_statements_dict))\n",
    "#print(\"len(wd_labels_dict)\",len(wd_labels_dict))\n",
    "#print(\"len(wd_local_word_ids_dict)\",len(wd_local_word_ids_dict))\n",
    "#print(\"len(wd_online_word_ids_dict)\",len(wd_online_word_ids_dict))\n",
    "#print(\"len(wd_local_predicate_ids_dict)\",len(wd_local_word_ids_dict))\n",
    "#print(\"len(wd_online_predicate_ids_dict)\",len(wd_online_word_ids_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_kb_ents(text):\n",
    "    #doc = nlp_kb(text)\n",
    "    doc = nlp(text)\n",
    "    #for ent in doc.ents:\n",
    "    #    print(\" \".join([\"ent\", ent.text, ent.label_, ent.kb_id_]))\n",
    "    return doc.ents\n",
    "        \n",
    "#ent_text_test = (\n",
    "#    \"In The Hitchhiker's Guide to the Galaxy, written by Douglas Adams, \"\n",
    "#    \"Douglas reminds us to always bring our towel, even in China or Brazil. \"\n",
    "#    \"The main character in Doug's novel is the man Arthur Dent, \"\n",
    "#    \"but Dougledydoug doesn't write about George Washington or Homer Simpson.\"\n",
    "#)\n",
    "#\n",
    "#en_text_test_2 = (\"Which actor voiced the Unicorn in The Last Unicorn?\")\n",
    "#\n",
    "#print([ent.kb_id_ for ent in get_kb_ents(ent_text_test)])\n",
    "#[ent.kb_id_ for ent in get_kb_ents(en_text_test_2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nlp(sentence, autocorrect=False):\n",
    "    \n",
    "    sentence = sentence.replace(\"’\", \"\\'\")\n",
    "    nlp_sentence = nlp(sentence)\n",
    "    nlp_sentence_list = list(nlp_sentence)\n",
    "    meaningful_punct = []\n",
    "    \n",
    "    for i_t, t in enumerate(nlp_sentence_list):\n",
    "        #print(t,t.pos_, t.lemma_)\n",
    "        if t.lemma_ == \"year\":\n",
    "            nlp_sentence_list[i_t] = \"date\"\n",
    "        elif t.text == \"\\'s\" or t.text == \"s\":\n",
    "            if t.lemma_ == \"be\" or t.lemma_ == \"s\":\n",
    "                nlp_sentence_list[i_t] = \"is\"\n",
    "            else: nlp_sentence_list[i_t] = \"\"\n",
    "        elif t.text == \"\\'ve\" or t.text == \"ve\":\n",
    "            if t.lemma_ == \"have\":\n",
    "                nlp_sentence_list[i_t] = \"have\"\n",
    "            else: nlp_sentence_list[i_t] = \"\"\n",
    "        elif t.text == \"\\'re\" or t.text == \"re\":\n",
    "            if t.lemma_ == \"be\":\n",
    "                nlp_sentence_list[i_t] = \"are\"\n",
    "            else: nlp_sentence_list[i_t] = \"\"\n",
    "        elif t.text == \"\\'ll\" or t.text == \"ll\":\n",
    "            if t.lemma_ == \"will\":\n",
    "                nlp_sentence_list[i_t] = \"will\"\n",
    "            else: nlp_sentence_list[i_t] = \"\"\n",
    "        elif t.text == \"\\'d\" or t.text == \"d\":\n",
    "            if t.lemma_ == \"have\":\n",
    "                nlp_sentence_list[i_t] = \"had\"\n",
    "            elif t.lemma_ == \"would\":\n",
    "                nlp_sentence_list[i_t] = \"would\"\n",
    "            else: nlp_sentence_list[i_t] = \"\"   \n",
    "        elif t.is_space:\n",
    "            nlp_sentence_list[i_t] = \"\"\n",
    "        elif t.pos_ == \"PUNCT\":\n",
    "            if t.text.count(\".\") > 2:\n",
    "                meaningful_punct.append((i_t,\"...\"))\n",
    "                nlp_sentence_list[i_t] = \"...\"\n",
    "            else:\n",
    "                nlp_sentence_list[i_t] = \"\"\n",
    "        else: nlp_sentence_list[i_t] = nlp_sentence_list[i_t].text\n",
    "    \n",
    "    nlp_sentence_list = [w for w in nlp_sentence_list if w]\n",
    "    \n",
    "    #print(\"nlp_sentence_list\",nlp_sentence_list)\n",
    "    \n",
    "    if autocorrect:\n",
    "        nlp_sentence = \" \".join(nlp_sentence_list)\n",
    "        nlp_sentence = (nlp_sentence.replace(\"’\", \"\\'\").replace(\"€\", \"euro\").replace(\"ç\", \"c\")\n",
    "                        .replace(\"à\", \"a\").replace(\"é\",\"e\").replace(\"ä\",\"a\").replace(\"ö\",\"o\")\n",
    "                       .replace(\"ü\",\"u\").replace(\"è\",\"e\").replace(\"¨\",\"\").replace(\"ê\",\"e\")\n",
    "                       .replace(\"â\",\"a\").replace(\"ô\",\"o\").replace(\"î\",\"i\").replace(\"û\",\"u\")\n",
    "                        .replace(\"_\",\" \").replace(\"°\",\"degree\").replace(\"§\",\"section\").replace(\"š\",\"s\")\n",
    "                       .replace(\"Š\",\"S\").replace(\"ć\",\"c\").replace(\"Ç\", \"C\")\n",
    "                        .replace(\"À\", \"A\").replace(\"É\",\"E\").replace(\"Ä\",\"A\").replace(\"Ö\",\"O\")\n",
    "                       .replace(\"Ü\",\"U\").replace(\"È\",\"E\").replace(\"Ê\",\"E\")\n",
    "                       .replace(\"Â\",\"A\").replace(\"Ô\",\"O\").replace(\"Î\",\"I\").replace(\"Û\",\"U\")\n",
    "                        .replace(\"á\",\"a\").replace(\"Á\",\"Á\").replace(\"ó\",\"o\").replace(\"Ó\",\"O\")\n",
    "                        .replace(\"ú\",\"u\").replace(\"Ú\",\"U\").replace(\"í\",\"i\").replace(\"Í\",\"I\")\n",
    "                        .replace(\"–\",\"-\")\n",
    "                       )\n",
    "        nlp_sentence = corrector.correct(nlp_sentence)\n",
    "        nlp_sentence = nlp_sentence[0][\"sequence\"]\n",
    "    \n",
    "        nlp_sentence = nlp(nlp_sentence)\n",
    "        nlp_sentence_list = list(nlp_sentence)\n",
    "\n",
    "        for i_t, t in enumerate(nlp_sentence_list):\n",
    "            if t.pos_ == \"PUNCT\":\n",
    "                if i_t in [mpunct[0] for mpunct in meaningful_punct]:\n",
    "                    for mpunct in meaningful_punct:\n",
    "                        if i_t == mpunct[0]:\n",
    "                            nlp_sentence_list[mpunct[0]] = mpunct[1]\n",
    "                else: nlp_sentence_list[i_t] = ''\n",
    "\n",
    "            else:\n",
    "                nlp_sentence_list[i_t] = nlp_sentence_list[i_t].text\n",
    "\n",
    "        for mpunct in meaningful_punct:\n",
    "            if mpunct[0] < len(nlp_sentence_list):\n",
    "                if nlp_sentence_list[mpunct[0]] != mpunct[1]:\n",
    "                    nlp_sentence_list.insert(mpunct[0], mpunct[1])\n",
    "        \n",
    "    return nlp(\" \".join(nlp_sentence_list).replace(\"  \", \" \").replace(\". &\",\".\").replace(\". /\",\".\"))\n",
    "\n",
    "\n",
    "#get_nlp(\"Which genre of album is harder.....faster?\", autocorrect=True)\n",
    "#get_nlp(\"Which genre of album is Harder ... Faster\", autocorrect=True)\n",
    "#get_nlp(\"Which home is an example of italianate architecture?\", autocorrect=True)\n",
    "#get_nlp(\"Your mom's father, were nice in the Years.!?\\'\\\":`’^!$£€\\(\\)ç*+%&/\\\\\\{\\};,àéäöüè¨êâôîû~-_<>°§...@.....\", autocorrect=True)\n",
    "#get_nlp(\"of what nationality is ken mcgoogan\", autocorrect=True)\n",
    "#get_nlp(\"you're fun\", autocorrect=True)\n",
    "#get_nlp(\"where's the fun\", autocorrect=True)\n",
    "#get_nlp(\"whats the name of the organization that was founded by  frei otto\", True)\n",
    "#get_nlp(\"Hurry! We’re late!\",True)\n",
    "#get_nlp(\"Who was an influential figure for miško Šuvaković\",True)\n",
    "#get_nlp(\"what is the second level division of the division crixás do tocantins\",True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_wd_entity(to_check):\n",
    "    pattern = re.compile('^Q[0-9]*$')\n",
    "    if pattern.match(to_check.strip()): return True\n",
    "    else: return False\n",
    "\n",
    "def is_wd_predicate(to_check):\n",
    "    pattern = re.compile('^P[0-9]*$')\n",
    "    if pattern.match(to_check.strip()): return True\n",
    "    else: return False\n",
    "    \n",
    "def is_valide_wd_id(to_check):\n",
    "    if is_wd_entity(to_check) or is_wd_predicate(to_check): return True\n",
    "    else: return False\n",
    "\n",
    "#print(is_wd_entity(\"Q155\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO redo the functions and optimize\n",
    "\n",
    "def is_entity_or_literal(to_check):\n",
    "    if is_wd_entity(to_check.strip()):\n",
    "        return True\n",
    "    pattern = re.compile('^[A-Za-z0-9]*$')\n",
    "    if len(to_check) == 32 and pattern.match(to_check.strip()):\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "# return if the given string is a literal or a date\n",
    "def is_literal_or_date(to_check): \n",
    "    return not('www.wikidata.org' in to_check)\n",
    "\n",
    "# return if the given string describes a year in the format YYYY\n",
    "def is_year(year):\n",
    "    pattern = re.compile('^[0-9][0-9][0-9][0-9]$')\n",
    "    if not(pattern.match(year.strip())):\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "\n",
    "# return if the given string is a date\n",
    "def is_date(date):\n",
    "    pattern = re.compile('^[0-9]+ [A-z]+ [0-9][0-9][0-9][0-9]$')\n",
    "    if not(pattern.match(date.strip())):\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "\n",
    "# return if the given string is a timestamp\n",
    "def is_timestamp(timestamp):\n",
    "    pattern = re.compile('^[0-9][0-9][0-9][0-9]-[0-9][0-9]-[0-9][0-9]T00:00:00Z')\n",
    "    if not(pattern.match(timestamp.strip())):\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "\n",
    "# convert the given month to a number\n",
    "def convert_month_to_number(month):\n",
    "    return{\n",
    "        \"january\" : \"01\",\n",
    "        \"february\" : \"02\",\n",
    "        \"march\" : \"03\",\n",
    "        \"april\" : \"04\",\n",
    "        \"may\" : \"05\",\n",
    "        \"june\" : \"06\",\n",
    "        \"july\" : \"07\",\n",
    "        \"august\" : \"08\",\n",
    "        \"september\" : \"09\", \n",
    "        \"october\" : \"10\",\n",
    "        \"november\" : \"11\",\n",
    "        \"december\" : \"12\"\n",
    "    }[month.lower()]\n",
    "\n",
    "# convert a date from the wikidata frontendstyle to timestamp style\n",
    "def convert_date_to_timestamp (date):\n",
    "    sdate = date.split(\" \")\n",
    "    # add the leading zero\n",
    "    if (len(sdate[0]) < 2):\n",
    "        sdate[0] = \"0\" + sdate[0]\n",
    "    return sdate[2] + '-' + convert_month_to_number(sdate[1]) + '-' + sdate[0] + 'T00:00:00Z'\n",
    "\n",
    "# convert a year to timestamp style\n",
    "def convert_year_to_timestamp(year):\n",
    "    return year + '-01-01T00:00:00Z'\n",
    "\n",
    "# get the wikidata id of a wikidata url\n",
    "def wikidata_url_to_wikidata_id(url):\n",
    "    if not url:\n",
    "        return False\n",
    "    if \"XMLSchema#dateTime\" in url or \"XMLSchema#decimal\" in url:\n",
    "        date = url.split(\"\\\"\", 2)[1]\n",
    "        date = date.replace(\"+\", \"\")\n",
    "        return date\n",
    "    if(is_literal_or_date(url)):\n",
    "        if is_year(url):\n",
    "            return convert_year_to_timestamp(url)\n",
    "        if is_date(url):\n",
    "            return convert_date_to_timestamp(url)\n",
    "        else:\n",
    "            url = url.replace(\"\\\"\", \"\")\n",
    "            return url\n",
    "    else:\n",
    "        url_array = url.split('/')\n",
    "        # the wikidata id is always in the last component of the id\n",
    "        return url_array[len(url_array)-1]\n",
    "    \n",
    "# fetch all statements where the given qualifier statement occurs as subject\n",
    "def get_all_statements_with_qualifier_as_subject(qualifier):\n",
    "    statements = []\n",
    "    triples, cardinality = hdt_wd.search_triples(qualifier, \"\", \"\")\n",
    "    for triple in triples:\n",
    "        sub, pre, obj = triple\n",
    "        # only consider triples with a wikidata-predicate\n",
    "        if pre.startswith(\"http://www.wikidata.org/\"):\n",
    "            statements.append({'entity': sub, 'predicate': pre, 'object': obj})\n",
    "    return statements\n",
    "\n",
    "# fetch the statement where the given qualifier statement occurs as object\n",
    "def get_statement_with_qualifier_as_object(qualifier):\n",
    "    triples, cardinality = hdt_wd.search_triples(\"\", \"\", qualifier)\n",
    "    for triple in triples:\n",
    "        sub, pre, obj = triple\n",
    "        # only consider triples with a wikidata-predicate\n",
    "        if pre.startswith(\"http://www.wikidata.org/\") and sub.startswith(\"http://www.wikidata.org/entity/Q\"):\n",
    "            return (sub, pre, obj)\n",
    "    return False\n",
    "\n",
    "# returns all statements that involve the given entity\n",
    "def get_all_statements_of_entity(entity_id):\n",
    "    # check entity pattern\n",
    "    if not is_wd_entity(entity_id.strip()):\n",
    "        return False\n",
    "    if wd_local_statements_dict.get(entity_id) != None:\n",
    "        #print(\"saved statement\")\n",
    "        return wd_local_statements_dict[entity_id]\n",
    "    entity = \"http://www.wikidata.org/entity/\"+entity_id\n",
    "    statements = []\n",
    "    # entity as subject\n",
    "    triples_sub, cardinality_sub = hdt_wd.search_triples(entity, \"\", \"\")\n",
    "    # entity as object\n",
    "    triples_obj, cardinality_obj = hdt_wd.search_triples(\"\", \"\", entity)\n",
    "    if cardinality_sub + cardinality_obj > 5000:\n",
    "        wd_local_statements_dict[entity_id] = []\n",
    "        return []\n",
    "    # iterate through all triples in which the entity occurs as the subject\n",
    "    for triple in triples_sub:\n",
    "        sub, pre, obj = triple\n",
    "        # only consider triples with a wikidata-predicate or if it is an identifier predicate\n",
    "        if not pre.startswith(\"http://www.wikidata.org/\"):# or (wikidata_url_to_wikidata_id(pre) in identifier_predicates):\n",
    "            continue\n",
    "        # object is statement\n",
    "        if obj.startswith(\"http://www.wikidata.org/entity/statement/\"):\n",
    "            qualifier_statements = get_all_statements_with_qualifier_as_subject(obj)\n",
    "            qualifiers = []\n",
    "            for qualifier_statement in qualifier_statements:\n",
    "                if qualifier_statement['predicate'] == \"http://www.wikidata.org/prop/statement/\" + wikidata_url_to_wikidata_id(pre):\n",
    "                        obj = qualifier_statement['object']\n",
    "                elif is_entity_or_literal(wikidata_url_to_wikidata_id(qualifier_statement['object'])):\n",
    "                    qualifiers.append({\n",
    "                        \"qualifier_predicate\":{\n",
    "                            \"id\": wikidata_url_to_wikidata_id(qualifier_statement['predicate'])\n",
    "                        }, \n",
    "                        \"qualifier_object\":{\n",
    "                            \"id\": wikidata_url_to_wikidata_id(qualifier_statement['object'])\n",
    "                        }})\n",
    "            statements.append({'entity': {'id': wikidata_url_to_wikidata_id(sub)}, 'predicate': {'id': wikidata_url_to_wikidata_id(pre)}, 'object': {'id': wikidata_url_to_wikidata_id(obj)}, 'qualifiers': qualifiers})\n",
    "        else:\n",
    "            statements.append({'entity': {'id': wikidata_url_to_wikidata_id(sub)}, 'predicate': {'id': wikidata_url_to_wikidata_id(pre)}, 'object': {'id': wikidata_url_to_wikidata_id(obj)}, 'qualifiers': []})\n",
    "    # iterate through all triples in which the entity occurs as the object\n",
    "    for triple in triples_obj:\n",
    "        sub, pre, obj = triple\n",
    "        # only consider triples with an entity as subject and a wikidata-predicate or if it is an identifier predicate\n",
    "        if not sub.startswith(\"http://www.wikidata.org/entity/Q\"):# or not pre.startswith(\"http://www.wikidata.org/\") or wikidata_url_to_wikidata_id(pre) in identifier_predicates:\n",
    "            continue\n",
    "        if sub.startswith(\"http://www.wikidata.org/entity/statement/\"):\n",
    "            statements_with_qualifier_as_object =  get_statement_with_qualifier_as_object(sub, process)\n",
    "            # if no statement was found continue\n",
    "            if not statements_with_qualifier_as_object:\n",
    "                continue\n",
    "            main_sub, main_pred, main_obj = statements_with_qualifier_as_object\n",
    "            qualifier_statements = get_all_statements_with_qualifier_as_subject(sub)\n",
    "            qualifiers = []\n",
    "            for qualifier_statement in qualifier_statements:\n",
    "                if wikidata_url_to_wikidata_id(qualifier_statement['predicate']) == wikidata_url_to_wikidata_id(main_pred):\n",
    "                    main_obj = qualifier_statement['object']\n",
    "                elif is_entity_or_literal(wikidata_url_to_wikidata_id(qualifier_statement['object'])):\n",
    "                    qualifiers.append({\n",
    "                        \"qualifier_predicate\":{\"id\": wikidata_url_to_wikidata_id(qualifier_statement['predicate'])}, \n",
    "                        \"qualifier_object\":{\"id\": wikidata_url_to_wikidata_id(qualifier_statement['object'])}\n",
    "                    })\n",
    "            statements.append({\n",
    "                            'entity': {'id': wikidata_url_to_wikidata_id(main_sub)},\n",
    "                            'predicate': {'id': wikidata_url_to_wikidata_id(main_pred)},\n",
    "                            'object': {'id': wikidata_url_to_wikidata_id(main_obj)},\n",
    "                            'qualifiers': qualifiers\n",
    "                              })\n",
    "        else:\n",
    "            statements.append({'entity': {'id': wikidata_url_to_wikidata_id(sub)}, 'predicate': {'id': wikidata_url_to_wikidata_id(pre)}, 'object': {'id': wikidata_url_to_wikidata_id(obj)}, 'qualifiers': []})\n",
    "    # cache the data\n",
    "    wd_local_statements_dict[entity_id] = statements\n",
    "    return statements\n",
    "\n",
    "#print(len(get_all_statements_of_entity(\"Q267721\")))\n",
    "#for s in get_all_statements_of_entity(\"Q267721\"):\n",
    "#    print(s)\n",
    "#save_cache_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wd_ids_online(name, is_predicate=False, top_k=3):\n",
    "    name = name.split('(')[0]\n",
    "    \n",
    "    if is_predicate and wd_online_predicate_ids_dict.get(name) != None and use_cache and len(wd_online_predicate_ids_dict)>0:\n",
    "        #print(\"saved predicate online\")\n",
    "        return wd_online_predicate_ids_dict[name][:top_k]\n",
    "    elif not is_predicate and wd_online_word_ids_dict.get(name) != None and use_cache and len(wd_online_word_ids_dict)>0:\n",
    "        #print(\"saved word online\")\n",
    "        return wd_online_word_ids_dict[name][:top_k]\n",
    "\n",
    "    request_successfull = False\n",
    "    entity_ids = \"\"\n",
    "    while not request_successfull:\n",
    "        try:\n",
    "            if is_predicate:\n",
    "                entity_ids = requests.get('https://www.wikidata.org/w/api.php?action=wbsearchentities&format=json&language=en&type=property&limit=' + str(top_k) + '&search='+name).json()\n",
    "            else:\n",
    "                entity_ids = requests.get('https://www.wikidata.org/w/api.php?action=wbsearchentities&format=json&language=en&limit=' + str(top_k) + '&search='+name).json()\n",
    "            request_successfull = True\n",
    "        except:\n",
    "            time.sleep(5)\n",
    "    results = entity_ids.get(\"search\")\n",
    "    if not results:\n",
    "        if is_predicate: wd_online_predicate_ids_dict[name] = [] \n",
    "        else: wd_online_word_ids_dict[name] = [] \n",
    "        return []\n",
    "    if not len(results):\n",
    "        if is_predicate: wd_online_predicate_ids_dict[name] = [] \n",
    "        else: wd_online_word_ids_dict[name] = []\n",
    "        return [] \n",
    "    res = []\n",
    "    for result in results:\n",
    "        res.append(result['id'])\n",
    "    \n",
    "    if is_predicate: wd_online_predicate_ids_dict[name] = res\n",
    "    else: wd_online_word_ids_dict[name] = res\n",
    "    \n",
    "    if res:\n",
    "        return res[:top_k]\n",
    "    else:\n",
    "        return []\n",
    "\n",
    "#print(get_wd_ids_online(\"be\", is_predicate=False, top_k=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# very computational\n",
    "def get_most_similar(word, top_k=3):\n",
    "    print(\"behold: get_most_similar started with:\", word)\n",
    "    word_text = str(word.lower())\n",
    "    if word_similarities_dict.get(word) != None and use_cache and len(word_similarities_dict)>0:\n",
    "        return word_similarities_dict[word][:top_k]\n",
    "    \n",
    "    word = nlp.vocab[word_text]\n",
    "    queries = [w for w in word.vocab if w.is_lower == word.is_lower and w.prob >= -15]\n",
    "    by_similarity = sorted(queries, key=lambda w: word.similarity(w), reverse=True)\n",
    "    \n",
    "    word_similarities = [(w.text.lower(),float(w.similarity(word))) for w in by_similarity[:10] if w.lower_ != word.lower_]\n",
    "\n",
    "    word_similarities_dict[word_text] = word_similarities\n",
    "    \n",
    "    save_cache_data()\n",
    "    \n",
    "    return word_similarities[:top_k]\n",
    "\n",
    "#print(get_most_similar(\"voiced\", top_k=3))\n",
    "#save_cache_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wd_ids(word, is_predicate=False, top_k=3, limit=6, online=False):\n",
    "    if is_predicate and wd_local_predicate_ids_dict.get(word) != None and use_cache and len(wd_local_predicate_ids_dict)>0:\n",
    "        #print(\"saved predicate local\")\n",
    "        return wd_local_predicate_ids_dict[word][:top_k]\n",
    "    elif not is_predicate and wd_local_word_ids_dict.get(word) != None and use_cache and len(wd_local_word_ids_dict)>0:\n",
    "        #print(\"saved word local\")\n",
    "        return wd_local_word_ids_dict[word][:top_k]\n",
    "    \n",
    "    language = \"en\"\n",
    "    word_formated = str(\"\\\"\"+word+\"\\\"\"+\"@\"+language)\n",
    "    to_remove = len(\"http://www.wikidata.org/entity/\")\n",
    "    t_name, card_name = hdt_wd.search_triples(\"\", \"http://schema.org/name\", word_formated, limit=top_k)\n",
    "    #print(\"names cardinality of \\\"\" + word+\"\\\": %i\" % card_name)\n",
    "    t_alt, card_alt = hdt_wd.search_triples(\"\", 'http://www.w3.org/2004/02/skos/core#altLabel', word_formated, limit=top_k)\n",
    "    #print(\"alternative names cardinality of \\\"\" + word+\"\\\": %i\" % card_alt)\n",
    "    results = list(set(\n",
    "        [t[0][to_remove:] for t in t_name if is_valide_wd_id(t[0][to_remove:])] + \n",
    "        [t[0][to_remove:] for t in t_alt if is_valide_wd_id(t[0][to_remove:])]\n",
    "           ))\n",
    "    \n",
    "    if is_predicate: results = [r for r in results if is_wd_predicate(r)]\n",
    "        \n",
    "    # cache the data\n",
    "    if is_predicate: wd_local_predicate_ids_dict[word] = results\n",
    "    else: wd_local_word_ids_dict[word] = results\n",
    "    \n",
    "    return results if limit<=0 else results[:limit-1]\n",
    "     \n",
    "#print(get_wd_ids(\"\", is_predicate=False, top_k=1))\n",
    "#get_wd_ids(\"The Last Unicorn\", top_k=0, limit=10)\n",
    "#print(get_wd_ids(\"wife\", is_predicate=False , top_k=0, limit=0))\n",
    "#print(get_wd_ids(\"voiced\", is_predicate=False , top_k=0, limit=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wd_label(from_id, language=\"en\"):\n",
    "    #print(\"from_id\",from_id)\n",
    "    if is_valide_wd_id(from_id):\n",
    "        if wd_labels_dict.get(from_id) != None and use_cache and len(wd_labels_dict)>0:\n",
    "            #print(\"saved label local\")\n",
    "            return wd_labels_dict[from_id]\n",
    "        \n",
    "        id_url = \"http://www.wikidata.org/entity/\"+from_id\n",
    "        t_name, card_name = hdt_wd.search_triples(id_url, \"http://schema.org/name\", \"\")\n",
    "        name = [t[2].split('\\\"@'+language)[0].replace(\"\\\"\", \"\") for t in t_name if \"@\"+language in t[2]]\n",
    "        #name = [t[2].split('@en')[0] for t in t_name if \"@\"+language in t[2]]\n",
    "        result = name[0] if name else ''\n",
    "        wd_labels_dict[from_id] = result #caching\n",
    "        return result\n",
    "        \n",
    "    else:\n",
    "        return from_id\n",
    "    \n",
    "#print(get_wd_label(\"P725\"))\n",
    "#get_wd_label(\"Q20789322\")\n",
    "#get_wd_label(\"Q267721\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building colors from graph\n",
    "def get_color(node_type):\n",
    "    if node_type == \"entity\": return \"violet\"#\"cornflowerblue\"\n",
    "    elif node_type == \"predicate\": return \"yellow\"\n",
    "    else: return \"red\"\n",
    "\n",
    "# Building labels for graph\n",
    "def get_elements_from_graph(graph):\n",
    "    node_names = nx.get_node_attributes(graph,\"name\")\n",
    "    node_types = nx.get_node_attributes(graph,\"type\")\n",
    "    colors = [get_color(node_types[n]) for n in node_names]\n",
    "    return node_names, colors\n",
    "\n",
    "# Plotting the graph\n",
    "def plot_graph(graph, name, title=\"Graph\"):\n",
    "    fig = plt.figure(figsize=(14,14))\n",
    "    ax = plt.subplot(111)\n",
    "    ax.set_title(str(\"answer: \"+title), fontsize=10)\n",
    "    #pos = nx.spring_layout(graph)\n",
    "    labels, colors = get_elements_from_graph(graph)\n",
    "    nx.draw(graph, node_size=30, node_color=colors, font_size=10, font_weight='bold', with_labels=True, labels=labels)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"tmqa1_graphs_imgs/\"+str(name)+\".png\", format=\"PNG\", dpi = 300)\n",
    "    plt.show()\n",
    "    \n",
    "#plot_graph(graph, \"file_name_graph\", \"Graph_title\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_statements_graph_worker(graph, predicate_nodes, turn, indexing_predicates, BANNED_WD_IDS, BANNED_WD_PRED_IDS, BANNED_WD_KEYWORDS, BANNED_WD_PRED_KEYWORDS, in_mp_queue, out_mp_queue, predicate_nodes_lock):\n",
    "    #for statement in statements:\n",
    "    sentinel = None\n",
    "    \n",
    "    for statement in iter(in_mp_queue.get, sentinel):\n",
    "        #print(\"statement\",statement)\n",
    "        #if (statement['entity']['id'][0] != \"Q\"\n",
    "        #    or statement['entity']['id'] in BANNED_WD_IDS\n",
    "        #    or statement['predicate']['id'][0] != \"P\"\n",
    "        #    or statement['predicate']['id'] in BANNED_WD_PRED_IDS\n",
    "        #    or statement['object']['id'][0] != \"Q\"\n",
    "        #    or statement['object']['id'] in BANNED_WD_IDS):\n",
    "        #    continue\n",
    "            \n",
    "        if (\n",
    "            statement['entity']['id'] in BANNED_WD_IDS\n",
    "            or statement['predicate']['id'][0] != \"P\"\n",
    "            or statement['predicate']['id'] in BANNED_WD_PRED_IDS\n",
    "            or statement['object']['id'] in BANNED_WD_IDS\n",
    "            ):\n",
    "            continue\n",
    "        \n",
    "        continue_flag = False\n",
    "        for key in BANNED_WD_PRED_KEYWORDS:\n",
    "            if (get_wd_label(statement['predicate']['id']).find(key) != -1): continue_flag = True\n",
    "                \n",
    "        for key in BANNED_WD_KEYWORDS:\n",
    "            if (get_wd_label(statement['entity']['id']).find(key) != -1): continue_flag = True\n",
    "            if (get_wd_label(statement['object']['id']).find(key) != -1): continue_flag = True\n",
    "                \n",
    "        if continue_flag: continue\n",
    "        \n",
    "        #print(statement)\n",
    "        if not statement['entity']['id'] in graph:\n",
    "            graph.add_node(statement['entity']['id'], name=get_wd_label(statement['entity']['id']), type='entity', turn=turn)\n",
    "        if not statement['object']['id'] in graph:\n",
    "            graph.add_node(statement['object']['id'], name=get_wd_label(statement['object']['id']), type='entity', turn=turn)\n",
    "        \n",
    "        with predicate_nodes_lock:\n",
    "            # increment index of predicate or set it at 0\n",
    "            if not statement['predicate']['id'] in predicate_nodes or not indexing_predicates:\n",
    "                predicate_nodes_index = 1\n",
    "                predicate_nodes[statement['predicate']['id']] = 1\n",
    "            else:\n",
    "                predicate_nodes[statement['predicate']['id']] += 1\n",
    "                predicate_nodes_index = predicate_nodes[statement['predicate']['id']]\n",
    "\n",
    "        # add the predicate node\n",
    "        predicate_node_id = (statement['predicate']['id'])\n",
    "        if indexing_predicates: predicate_node_id += \"-\" + str(predicate_nodes_index)\n",
    "        \n",
    "        graph.add_node(predicate_node_id, name=get_wd_label(statement['predicate']['id']), type='predicate', turn=turn)\n",
    "\n",
    "        # add the two edges (entity->predicate->object)\n",
    "        #statement['entity']['id'] in BANNED_WD_IDS \n",
    "        #statement['object']['id'] in BANNED_WD_IDS\n",
    "        #statement['predicate']['id'] in BANNED_WD_PRED_IDS\n",
    "        #if (statement['predicate']['id'] in BANNED_WD_PRED_IDS): break\n",
    "            \n",
    "        graph.add_edge(statement['entity']['id'], predicate_node_id)\n",
    "        graph.add_edge(predicate_node_id, statement['object']['id'])\n",
    "        \n",
    "    out_mp_queue.put(graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: handle special literals? which one\n",
    "\n",
    "def make_statements_graph(statements, indexing_predicates=True, cores=mp.cpu_count()):\n",
    "    BANNED_WD_IDS = [\n",
    "        \"Q4167410\",\"Q66087861\",\"Q65932995\",\"Q21281405\",\"Q17442446\",\"Q41770487\",\"Q29548341\",\n",
    "        \"Q29547399\",\"Q25670\"\n",
    "    ]\n",
    "    BANNED_WD_PRED_IDS = [\n",
    "        \"P1687\",\"P7087\",\"P1889\",\"P646\", \"P227\", \"P1256\", \"P1257\", \"P1258\", \"P1260\", \"P301\",\n",
    "        \"P18\",\"P1266\",\"P487\",\"P1970\",\"P2529\", \"P4390\", \"P4342\", \"P4213\", \"P487\", \"P2624\",\n",
    "        \"P4953\", \"P2241\", \"P345\",\"P703\", \"P2163\", \"P18\", \"P436\", \"P227\", \"P646\", \"P2581\",\n",
    "        \"P1006\", \"P244\", \"P214\", \"P1051\", \"P1296\",\"P461\", \"P2959\", \"P1657\", \"P3834\",\"P243\",\n",
    "        \"P3306\",\"P6932\",\"P356\",\"P1630\",\"P3303\",\"P1921\",\"P1793\",\"P1628\",\"P1184\",\"P1662\",\"P2704\",\n",
    "        \"P4793\",\"P1921\",\"P2302\",\"P6562\",\"P6127\",\"P4342\",\"P6145\",\"P5786\",\"P5099\",\"P4947\",\"P5032\",\n",
    "        \"P4933\",\"P4632\",\"P4529\",\"P4277\",\"P4282\",\"P3135\",\"P4276\",\"P3593\",\"P2638\",\"P3804\",\"P3145\",\n",
    "        \"P2509\",\"P3212\",\"P2704\",\"P480\",\"P3844\",\"P3141\",\"P3808\",\"P3933\",\"P2346\",\"P3077\",\"P3417\",\n",
    "        \"P2529\",\"P3302\",\"P3143\",\"P2334\",\"P3129\",\"P3138\",\"P3107\",\"P2603\",\"P2631\",\"P2508\",\"P2465\",\n",
    "        \"P2014\", \"P1874\", \"P2518\", \"P1265\", \"P1237\",\"P1712\", \"P1970\",\"P1804\",\"P905\",\"P1562\",\n",
    "        \"P1258\",\"P646\",\"P345\"\n",
    "    ]\n",
    "    BANNED_WD_KEYWORDS = []\n",
    "    BANNED_WD_PRED_KEYWORDS = [\n",
    "        \"ID\", \"ISBN\",\"identifier\", \"IDENTIFIER\", \"isbn\", \"ISSN\", \"issn\"\n",
    "    ]\n",
    "    \n",
    "    graph = nx.Graph()\n",
    "    turn=0\n",
    "    \n",
    "    predicate_nodes = mp.Manager().dict()\n",
    "    predicate_nodes_lock = mp.Manager().Lock()\n",
    "    \n",
    "    paths_keyword_nodes = []\n",
    "    if cores <= 0: cores = 1\n",
    "\n",
    "    out_mp_queue = mp.Queue()\n",
    "    in_mp_queue = mp.Queue()\n",
    "    sentinel = None\n",
    "\n",
    "    for statement in statements:\n",
    "        in_mp_queue.put(statement)\n",
    "\n",
    "    procs = [mp.Process(target = make_statements_graph_worker, args = (graph, predicate_nodes, turn, indexing_predicates, BANNED_WD_IDS, BANNED_WD_PRED_IDS, BANNED_WD_KEYWORDS, BANNED_WD_PRED_KEYWORDS, in_mp_queue, out_mp_queue, predicate_nodes_lock)) for i in range(cores)]\n",
    "\n",
    "    for proc in procs:\n",
    "        proc.daemon = True\n",
    "        proc.start()\n",
    "    for proc in procs:    \n",
    "        in_mp_queue.put(sentinel)\n",
    "    for proc in procs:\n",
    "        local_g = out_mp_queue.get()\n",
    "        graph = nx.compose(graph,local_g)\n",
    "    for proc in procs:\n",
    "        proc.join()\n",
    "    \n",
    "    return graph, predicate_nodes\n",
    "\n",
    "#test_graph = make_statements_graph(test_unduplicate_statements, indexing_predicates=False)\n",
    "#print(test_graph[1])\n",
    "#plot_graph(test_graph[0],\"test\")\n",
    "#print(\"len(filtered_statements)\",len(filtered_statements))\n",
    "#start_time = time.time()\n",
    "#graph, predicate_nodes = make_statements_graph(filtered_statements, indexing_predicates=True, cores=1)\n",
    "#print(time.time()-start_time)\n",
    "#print(\"--> \",len(graph), \"nodes and\", graph.size(), \"edges\")\n",
    "#print(predicate_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_lists(list_1, list_2):\n",
    "    if len(list_1) == len(list_2):\n",
    "        return [(list_1[i], list_2[i]) for i in range(0, len(list_1))]\n",
    "    else:\n",
    "        return \"Error: lists are not the same lenght\"\n",
    "\n",
    "#print(merge_lists([\"author\"],['P50']))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_themes_ids_from_chunks(noun_chunks, top_k=3, online=False):\n",
    "    if online:\n",
    "        theme_ids = [get_wd_ids_online(chunk.text, top_k=top_k)+get_wd_ids(chunk.text, top_k=top_k) for chunk in noun_chunks]\n",
    "    else:\n",
    "        theme_ids = [get_wd_ids(chunk.text, top_k=top_k) for chunk in noun_chunks]\n",
    "    return theme_ids\n",
    "\n",
    "def get_themes(nlp_question, raw_question, top_k=3, online=False, max_title_size=5):\n",
    "    #nlp_raw_question = get_nlp(raw_question, autocorrect=False)\n",
    "    #nlp_raw_question_lower = get_nlp(raw_question.lower(), autocorrect=False)\n",
    "    #nlp_raw_question_captialize = get_nlp(\" \".join([w.capitalize() for w in raw_question.split(\" \")]), autocorrect=False)\n",
    "    \n",
    "    #nlp_raw_question_list = list(nlp_raw_question)\n",
    "    #print([e for e in nlp_raw_question])\n",
    "    #print([e for e in nlp_raw_question_lower])\n",
    "    #print([e for e in nlp_raw_question_captialize])\n",
    "    \n",
    "    special_words = [w for w in raw_question.lower().split(\" \") if w not in  nlp_question.text.lower().split()]\n",
    "    special_words_capitalize = [w.capitalize() for w in special_words]\n",
    "    #special_words_capitalize_per_2 = it.permutations(special_words_capitalize,2)\n",
    "    \n",
    "    #print(\"special_words\",special_words)\n",
    "    #print(\"special_words_capitalize\",special_words_capitalize)\n",
    "    \n",
    "    special_words_titles = []\n",
    "    for per_size in range(2,len(special_words_capitalize)+1):\n",
    "        if per_size > max_title_size:\n",
    "            break\n",
    "        special_words_titles += [\" \".join(p) for p in it.permutations(special_words_capitalize,per_size)]\n",
    "\n",
    "    #print(\"special_words_titles\",special_words_titles)\n",
    "    \n",
    "    #print(nlp_raw_question_list)\n",
    "    \n",
    "    # PART1: finding themes as the user typed it\n",
    "    filter_list = [\"PART\", \"PRON\", \"NUM\"]\n",
    "    nlp_list_src = list(nlp_question)\n",
    "    nlp_list = []\n",
    "    for w in nlp_question:\n",
    "        if w.pos_ not in filter_list:\n",
    "            nlp_list.append(w)\n",
    "    nlp_question = get_nlp(\" \".join([e.text for e in nlp_list]))\n",
    "    \n",
    "    themes = [(ent, [ent.kb_id_]) for ent in get_kb_ents(nlp_question.text) if ent.kb_id_ != \"NIL\"]\n",
    "    #print(\"1 themes\",themes)\n",
    "    for w in special_words+special_words_capitalize+special_words_titles:\n",
    "        themes += [(ent, [ent.kb_id_]) for ent in get_kb_ents(w) if ent.kb_id_ != \"NIL\"]\n",
    "    #print(\"2 themes\",themes)\n",
    "    theme_complements = []\n",
    "    \n",
    "    noun_chunks = [chunk for chunk in nlp_question.noun_chunks]\n",
    "    \n",
    "    #print(\"1 noun_chunks\",noun_chunks)\n",
    "    for w in special_words+special_words_capitalize+special_words_titles:\n",
    "        noun_chunks += [chunk for chunk in get_nlp(w, autocorrect=False).noun_chunks]\n",
    "    \n",
    "    #print(\"2 noun_chunks\",noun_chunks)\n",
    "    #theme_ids = [get_wd_ids(chunk.text, top_k=top_k) for chunk in noun_chunks][:top_k]\n",
    "    theme_ids = get_themes_ids_from_chunks(noun_chunks, top_k=3, online=online)\n",
    "            \n",
    "    for i, chunk in enumerate(theme_ids):\n",
    "        if chunk: themes.append((noun_chunks[i], chunk))\n",
    "        else: theme_complements.append(noun_chunks[i])\n",
    "    \n",
    "    # PART2: finding themes with the question capitalized\n",
    "    #print(nlp_question)\n",
    "    nlp_list_cap = []\n",
    "    nlp_list_low = []\n",
    "    nlp_list_lemma = []\n",
    "    nlp_list_no_det = []\n",
    "    w_filter = [\"WDT\",\"WP\",\"WP$\",\"WRB\"]\n",
    "    for w in nlp_question:\n",
    "        if w.tag_ not in w_filter:\n",
    "            nlp_list_cap.append(w.text.capitalize())\n",
    "            nlp_list_low.append(w.text.lower())\n",
    "            nlp_list_lemma.append(w.lemma_)\n",
    "        if w.pos_ != \"DET\":\n",
    "            nlp_list_no_det.append(w.text)\n",
    "            \n",
    "    nlp_question_cap = get_nlp(\" \".join([e for e in nlp_list_cap]))\n",
    "    nlp_question_low = get_nlp(\" \".join([e for e in nlp_list_low]))\n",
    "    nlp_question_lemma = get_nlp(\" \".join([e for e in nlp_list_lemma]))\n",
    "    nlp_question_no_det = get_nlp(\" \".join([e for e in nlp_list_no_det]))\n",
    "\n",
    "    themes += [(ent, [ent.kb_id_]) for ent in get_kb_ents(nlp_question_cap.text) if ent.kb_id_ != \"NIL\" and (ent, [ent.kb_id_]) not in themes]\n",
    "    themes += [(ent, [ent.kb_id_]) for ent in get_kb_ents(nlp_question_low.text) if ent.kb_id_ != \"NIL\" and (ent, [ent.kb_id_]) not in themes]\n",
    "    themes += [(ent, [ent.kb_id_]) for ent in get_kb_ents(nlp_question_lemma.text) if ent.kb_id_ != \"NIL\" and (ent, [ent.kb_id_]) not in themes]\n",
    "    themes += [(ent, [ent.kb_id_]) for ent in get_kb_ents(nlp_question_no_det.text) if ent.kb_id_ != \"NIL\" and (ent, [ent.kb_id_]) not in themes]\n",
    "    \n",
    "    if online:\n",
    "        themes += [(ent, get_wd_ids_online(ent.text, is_predicate=False, top_k=top_k)) for ent in get_kb_ents(nlp_question_cap.text)]\n",
    "        themes += [(ent, get_wd_ids_online(ent.text, is_predicate=False, top_k=top_k)) for ent in get_kb_ents(nlp_question_low.text)]\n",
    "        themes += [(ent, get_wd_ids_online(ent.text, is_predicate=False, top_k=top_k)) for ent in get_kb_ents(nlp_question_lemma.text)]\n",
    "        themes += [(ent, get_wd_ids_online(ent.text, is_predicate=False, top_k=top_k)) for ent in get_kb_ents(nlp_question_no_det.text)]\n",
    "    \n",
    "    noun_chunks = []\n",
    "    \n",
    "    previous_title_position = 0\n",
    "    for i_t,t in enumerate(nlp_question):\n",
    "        tmp_row = []\n",
    "        if i_t > previous_title_position:\n",
    "            if t.is_title:\n",
    "                for i_p in range(previous_title_position,i_t+1):\n",
    "                    tmp_row.append(nlp_question[i_p])\n",
    "\n",
    "                noun_chunks.append(get_nlp(\" \".join([w.text for w in tmp_row])))\n",
    "\n",
    "        if t.is_title:\n",
    "            previous_title_position = i_t\n",
    "    \n",
    "    noun_chunks += [chunk for chunk in nlp_question_cap.noun_chunks]\n",
    "    \n",
    "    #theme_ids = [get_wd_ids(chunk.text, top_k=top_k) for chunk in noun_chunks][:top_k]\n",
    "    theme_ids = get_themes_ids_from_chunks(noun_chunks, top_k=3, online=online)\n",
    "    \n",
    "    for i, chunk in enumerate(theme_ids):\n",
    "        if chunk: themes.append((noun_chunks[i], chunk))\n",
    "        else: theme_complements.append(noun_chunks[i])\n",
    "    \n",
    "    noun_chunks = [chunk for chunk in nlp_question_low.noun_chunks]\n",
    "    #theme_ids = [get_wd_ids(chunk.text, top_k=top_k) for chunk in noun_chunks][:top_k]\n",
    "    theme_ids = get_themes_ids_from_chunks(noun_chunks, top_k=3, online=online)\n",
    "\n",
    "    for i, chunk in enumerate(theme_ids):\n",
    "        if chunk: themes.append((noun_chunks[i], chunk))\n",
    "        else: theme_complements.append(noun_chunks[i])\n",
    "            \n",
    "    noun_chunks = [chunk for chunk in nlp_question_lemma.noun_chunks]\n",
    "    #theme_ids = [get_wd_ids(chunk.text, top_k=top_k) for chunk in noun_chunks][:top_k]\n",
    "    theme_ids = get_themes_ids_from_chunks(noun_chunks, top_k=3, online=online)\n",
    "\n",
    "    for i, chunk in enumerate(theme_ids):\n",
    "        if chunk: themes.append((noun_chunks[i], chunk))\n",
    "        else: theme_complements.append(noun_chunks[i])\n",
    "            \n",
    "    noun_chunks = [chunk for chunk in nlp_question_no_det.noun_chunks]\n",
    "    #theme_ids = [get_wd_ids(chunk.text, top_k=top_k) for chunk in noun_chunks][:top_k]\n",
    "    theme_ids = get_themes_ids_from_chunks(noun_chunks, top_k=3, online=online)\n",
    "\n",
    "    for i, chunk in enumerate(theme_ids):\n",
    "        if chunk: themes.append((noun_chunks[i], chunk))\n",
    "        else: theme_complements.append(noun_chunks[i])\n",
    "    \n",
    "    themes_filtered = []\n",
    "    for t in themes:\n",
    "        if t[0].text in [tf[0].text for tf in themes_filtered]:\n",
    "            index = [tf[0].text for tf in themes_filtered].index(t[0].text)\n",
    "            tmp = t[1]+[i for j in [tf[1] for index, tf in enumerate(themes_filtered) if tf[0].text == t[0].text] for i in j]\n",
    "            themes_filtered[index] = (t[0],tmp)\n",
    "\n",
    "        else:\n",
    "            themes_filtered.append(t)\n",
    "    \n",
    "    # removing the same elments per rows and skipping already existing rows\n",
    "    unique_ids = []\n",
    "    themes_filtered_undupped = []\n",
    "    for tf in themes_filtered:\n",
    "        tmp_ids = []\n",
    "        for tfid in tf[1]:\n",
    "            if tfid not in unique_ids and tfid not in tmp_ids:\n",
    "                tfname = get_wd_label(tfid)\n",
    "                similarity = get_nlp(tfname).similarity(tf[0])\n",
    "                if similarity >= 0.95:\n",
    "                    tmp_ids.append(tfid)\n",
    "                unique_ids.append(tfid)\n",
    "        if tmp_ids and tmp_ids not in [tfu[1] for tfu in themes_filtered_undupped]:\n",
    "            themes_filtered_undupped.append((tf[0],tmp_ids))\n",
    "    \n",
    "    #for tf in themes_filtered:\n",
    "    #    tmp_ids = []\n",
    "    #    for tfid in tf[1]:\n",
    "    #        if tfid not in tmp_ids:\n",
    "    #            tmp_ids.append(tfid)\n",
    "    #    if tmp_ids not in [tfu[1] for tfu in themes_filtered_undupped]:\n",
    "    #        themes_filtered_undupped.append((tf[0],tmp_ids))\n",
    "\n",
    "        \n",
    "    theme_complements_undupped = []\n",
    "    [theme_complements_undupped.append(tc) for tc in theme_complements if tc.text not in [tcu.text for tcu in theme_complements_undupped]]\n",
    "    \n",
    "    \n",
    "    #print(themes_filtered)\n",
    "    return themes_filtered_undupped, theme_complements_undupped\n",
    "\n",
    "#q0_themes = get_themes(q0_nlp, top_k=3)\n",
    "#q0_themes_test = get_themes(q0_nlp_test)\n",
    "#q0_themes_test_2 = get_themes(q0_nlp_test_2)\n",
    "#print(q0_themes) \n",
    "\n",
    "#q_test_3 = get_nlp(\"the unicorn and the raccoons love obama barack's tacos\")\n",
    "#q_test_3_themes = get_themes(q_test_3, top_k=3)\n",
    "#print(get_enhanced_themes(q_test_3_themes))\n",
    "#print(q_test_3_themes)\n",
    "\n",
    "\n",
    "#q_test_test = get_nlp(\"What is a tv action show?\")\n",
    "#q_test_test = get_nlp(\"Who voiced the Unicorn in The Last Unicorn\")\n",
    "#q_test_test = get_nlp(\"What is the name of the person who created Saved by the Bell?\")\n",
    "#q_test_test = get_nlp(\"When did the movie Grease come out?\")\n",
    "#q_test_question = \"Who was an influential figure for miško Šuvaković\"\n",
    "#q_test_test = get_nlp(q_test_question,True)\n",
    "#get_themes(q_test_test, q_test_question, top_k=3, online=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "BANNED_WORDS = [\"...\"]\n",
    "\n",
    "def get_theme_tuples(theme_list, top_k=3, online=False):\n",
    "    tuples = [(t, get_wd_ids(t, top_k=top_k)) for t in theme_list if t not in BANNED_WORDS]\n",
    "    if online:\n",
    "        tuples += [(t, get_wd_ids_online(t, is_predicate=False, top_k=top_k)) for t in theme_list if t not in BANNED_WORDS]\n",
    "    return tuples\n",
    "\n",
    "def get_theme_no_stopwords(theme_list):\n",
    "    return [s for s in theme_list if not s.is_stop]\n",
    "\n",
    "def get_theme_lemmatized(theme_list):\n",
    "    return [s.lemma_ for s in theme_list]\n",
    "\n",
    "def get_permutation_tuples(theme_list, start=2):   \n",
    "    permutations = []\n",
    "    for i in range(start, len(theme_list)+1):\n",
    "        permutations += list(it.permutations(theme_list,i))\n",
    "    return permutations\n",
    "\n",
    "def get_lemma_permutation_tuples(theme_list, start=2):\n",
    "    return get_permutation_tuples(get_theme_lemmatized(theme_list), start=2)\n",
    "\n",
    "def get_non_token_tuples(theme_list):\n",
    "    return [\" \".join([e for e in list(l)]) for l in theme_list]\n",
    "\n",
    "def get_non_token_lower_tuples(theme_list):\n",
    "    return [\" \".join([e.lower() for e in list(l)]) for l in theme_list]\n",
    "\n",
    "def get_non_token_capitalize_tuples(theme_list):\n",
    "    return [\" \".join([c.capitalize() for c in [e for e in list(l)]]) for l in theme_list] \n",
    "\n",
    "def get_text_tuples(theme_list):\n",
    "    return [\" \".join([e.text for e in list(l)]) for l in theme_list]\n",
    "\n",
    "def get_lower_tuples(theme_list):\n",
    "    return [\" \".join([e.lower_ for e in list(l)]) for l in theme_list]\n",
    "\n",
    "def get_capitalized_tuples(theme_list):\n",
    "    return [\" \".join([c.capitalize() for c in [e.text for e in list(l)]]) for l in theme_list]\n",
    "\n",
    "def get_enhanced_themes(themes, top_k=3, title_limit=5, aggressive=False, online=False):\n",
    "    enhanced_themes = []\n",
    "    # permute, capitalize, lowering of the words in the complements\n",
    "    for c in themes[1]:\n",
    "        if len(c) <= title_limit:\n",
    "            per_lemma = get_theme_tuples(get_non_token_tuples([n for n in get_permutation_tuples(get_theme_lemmatized(c))]),top_k, online=online)\n",
    "            [enhanced_themes.append(p) for p in per_lemma if p[1] and p not in enhanced_themes]\n",
    "            del per_lemma\n",
    "\n",
    "            per_nostop = get_theme_tuples(get_text_tuples(get_permutation_tuples(get_theme_no_stopwords(c),start=1)),top_k, online=online)\n",
    "            [enhanced_themes.append(p) for p in per_nostop if p[1] and p not in enhanced_themes]\n",
    "            del per_nostop\n",
    "\n",
    "            per_lemma_nostop = get_theme_tuples(get_non_token_tuples([get_theme_lemmatized(s) for s in get_permutation_tuples(get_theme_no_stopwords(c),start=1)]),top_k, online=online)\n",
    "            [enhanced_themes.append(p) for p in per_lemma_nostop if p[1] and p not in enhanced_themes]\n",
    "            del per_lemma_nostop\n",
    "\n",
    "            per_lemma_lower = get_theme_tuples(get_non_token_lower_tuples([n for n in get_permutation_tuples(get_theme_lemmatized(c))]),top_k, online=online)\n",
    "            [enhanced_themes.append(p) for p in per_lemma_lower if p[1] and p not in enhanced_themes]\n",
    "            del per_lemma_lower\n",
    "\n",
    "            per_nostop_lower = get_theme_tuples(get_lower_tuples(get_permutation_tuples(get_theme_no_stopwords(c),start=1)),top_k)\n",
    "            [enhanced_themes.append(p) for p in per_nostop_lower if p[1] and p not in enhanced_themes]\n",
    "            del per_nostop_lower\n",
    "\n",
    "            per_lemma_nostop_lower = get_theme_tuples(get_non_token_lower_tuples([get_theme_lemmatized(s) for s in get_permutation_tuples(get_theme_no_stopwords(c),start=1)]),top_k, online=online)\n",
    "            [enhanced_themes.append(p) for p in per_lemma_nostop_lower if p[1] and p not in enhanced_themes]\n",
    "            del per_lemma_nostop_lower\n",
    "\n",
    "            per_lemma_capitalize = get_theme_tuples(get_non_token_capitalize_tuples([n for n in get_permutation_tuples(get_theme_lemmatized(c))]),top_k, online=online)\n",
    "            [enhanced_themes.append(p) for p in per_lemma_capitalize if p[1] and p not in enhanced_themes]\n",
    "            del per_lemma_capitalize\n",
    "\n",
    "            per_nostop_capitalize = get_theme_tuples(get_capitalized_tuples(get_permutation_tuples(get_theme_no_stopwords(c),start=1)),top_k, online=online)\n",
    "            [enhanced_themes.append(p) for p in per_nostop_capitalize if p[1] and p not in enhanced_themes]\n",
    "            del per_nostop_capitalize\n",
    "\n",
    "            per_lemma_nostop_capitalize = get_theme_tuples(get_non_token_capitalize_tuples([get_theme_lemmatized(s) for s in get_permutation_tuples(get_theme_no_stopwords(c),start=1)]),top_k, online=online)\n",
    "            [enhanced_themes.append(p) for p in per_lemma_nostop_capitalize if p[1] and p not in enhanced_themes]\n",
    "\n",
    "            per = get_theme_tuples(get_text_tuples(get_permutation_tuples(c)),top_k, online=online)\n",
    "            [enhanced_themes.append(p) for p in per if p[1] and p not in enhanced_themes]\n",
    "            del per\n",
    "\n",
    "            per_lower = get_theme_tuples(get_lower_tuples(get_permutation_tuples(c)),top_k, online=online)\n",
    "            [enhanced_themes.append(p) for p in per_lower if p[1] and p not in enhanced_themes]\n",
    "            del per_lower\n",
    "\n",
    "            per_capitalize = get_theme_tuples(get_capitalized_tuples(get_permutation_tuples(c)),top_k, online=online)\n",
    "            [enhanced_themes.append(p) for p in per_capitalize if p[1] and p not in enhanced_themes]\n",
    "            del per_capitalize\n",
    "    \n",
    "    if aggressive:\n",
    "        predicates = []\n",
    "        [predicates.append(get_wd_label(pred)) for pred in sum([p[1] for p in themes[0]],[]) if get_wd_label(pred) not in predicates]\n",
    "        predicates_ids = [get_wd_ids_online(p, is_predicate=True, top_k=top_k) for p in predicates]\n",
    "        predicated_themes = merge_lists(predicates, predicates_ids)\n",
    "        predicated_themes = [pt for pt in predicated_themes if pt[1] != '']\n",
    "        if predicates: enhanced_themes += predicated_themes\n",
    "    \n",
    "    #print(\"themes[0]\",[t[0].text for t in themes[0]])\n",
    "    #print(\"themes[0].lower()\",[t[0].text.lower() for t in themes[0]])\n",
    "    \n",
    "    enhanced_themes_filtered = []\n",
    "    for et in enhanced_themes:\n",
    "        if not et[0] in [t[0].text for t in themes[0]]:\n",
    "            #print(\"et not in themes\",et)\n",
    "            #print(len(themes[0]))\n",
    "            #print([t[0].text.find(et[0]) for t in themes[0]].count(-1))\n",
    "            if len([t for t in themes[0] if t[0].text.find(et[0]) == -1]) < len(themes[0]) or not et[1]:\n",
    "                continue\n",
    "            \n",
    "            if et[0] in [e[0] for e in enhanced_themes_filtered]:\n",
    "                index_et = [e[0] for e in enhanced_themes_filtered].index(et[0])\n",
    "                if index_et != -1:\n",
    "                    enhanced_themes_filtered[index_et] = (et[0], enhanced_themes_filtered[index_et][1]+et[1]) #.append((et[0],et[1][:top_k]))\n",
    "                else: enhanced_themes_filtered.append((et[0],et[1][:top_k]))\n",
    "            #elif et[0] not in :\n",
    "            #    print(\"et unknown\",et)\n",
    "            else:\n",
    "                enhanced_themes_filtered.append((et[0],et[1][:top_k]))\n",
    "    \n",
    "    return enhanced_themes_filtered\n",
    "\n",
    "#q_test_3 = get_nlp(\"Which genre of album is harder.....faster?\",autocorrect=True)\n",
    "#q_test_3 = get_nlp(\"the unicorn and the raccoons love obama barack's tacos\")\n",
    "#q_test_3 = get_nlp(\"what was the cause of death of yves klein\")\n",
    "#q_test_3 = get_nlp(\"Who is the author that wrote the book Moby Dick\")\n",
    "#q_test_3_themes = get_themes(q_test_3, top_k=3)\n",
    "#print(q_test_3_themes[0])\n",
    "#print(get_enhanced_themes(q_test_3_themes, aggressive=False))\n",
    "\n",
    "#print(get_enhanced_themes(q_test_3_themes, aggressive=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predicates_online(nlp_sentence, top_k=3, aggressive=False):\n",
    "    PASSIVE_VERBS = [\"be\"]\n",
    "    AGRESSIVE_FILTER = [\"VERB\",\"AUX\",\"NOUN\",\"ADJ\"]\n",
    "    if aggressive: predicates = [p for p in nlp_sentence if p.pos_ in AGRESSIVE_FILTER]\n",
    "    else: predicates = [p for p in nlp_sentence if p.pos_ == \"VERB\" or p.pos_ == \"AUX\"]\n",
    "\n",
    "    if len(predicates) == 1:\n",
    "        if predicates[0].lemma_ in PASSIVE_VERBS:\n",
    "            predicates += [p for p in nlp_sentence if p.pos_ in AGRESSIVE_FILTER if p not in predicates]\n",
    "    \n",
    "    predicates_filtered = []\n",
    "    for p in predicates:\n",
    "        if p.lemma_ in PASSIVE_VERBS: \n",
    "            p = get_nlp(p.lemma_)[0]\n",
    "        if len(predicates_filtered) == 0:\n",
    "            predicates_filtered.append(p)\n",
    "        if p.text not in [p.text for p in predicates_filtered]:\n",
    "            predicates_filtered.append(p)\n",
    "    \n",
    "    predicates_ids = []\n",
    "    for i_p, p in enumerate(predicates_filtered):\n",
    "        if p.lemma_ == \"be\":\n",
    "            predicates_ids.append(get_wd_ids_online(\"is\", is_predicate=True, top_k=top_k)[:1])\n",
    "        else:\n",
    "            p_id = get_wd_ids_online(p.text, is_predicate=True, top_k=top_k)\n",
    "            if not p_id:\n",
    "                p_id = get_wd_ids_online(p.lemma_, is_predicate=True, top_k=top_k)\n",
    "                if not p_id:\n",
    "                    similar_words = [w[0] for w in get_most_similar(p.lemma_, top_k=top_k)]\n",
    "                    for sw in similar_words:\n",
    "                        if not p_id:\n",
    "                            p_id = get_wd_ids_online(sw, is_predicate=True, top_k=top_k)\n",
    "            predicates_ids.append(p_id[:top_k])\n",
    "    \n",
    "    return merge_lists(predicates_filtered, predicates_ids)\n",
    "\n",
    "#q_test = get_nlp(\"Who voiced the Unicorn in The Last Unicorn\")\n",
    "#q_test = get_nlp(\"Of what nationality is Ken McGoogan\")\n",
    "#q_test = get_nlp(\"Which have the nation of Martha Mattox\")\n",
    "#q_test = get_nlp(\"what city was alex golfis born in\")\n",
    "#q_test = get_nlp(\"who's born in city was alex golfis born in\")\n",
    "#q_test = get_nlp(\"what's the name fo the wife of my dads\")\n",
    "#start_time = time.time()\n",
    "#q_test = get_nlp(\"Where did roger marquis die\")\n",
    "#print(get_predicates_online(q_test, top_k=2, aggressive=False))\n",
    "#print(\"it was:\",time.time()-start_time)\n",
    "#q0_predicates_test_2 = get_predicates_online(q0_nlp_test_2, top_k=3, aggressive=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predicates(nlp_sentence, themes=False, top_k=0):\n",
    "    PASSIVE_VERBS = [\"be\"]\n",
    "    predicates = [p for p in nlp_sentence if p.pos_ == \"VERB\" or p.pos_ == \"AUX\"]\n",
    "    #for i_p, p in enumerate(predicates):\n",
    "    #    if p.text == \"\\'s\":\n",
    "    #        predicates[i_p] = get_nlp(\"is\")[0]\n",
    "    #    if p.text == \"\\'re\":\n",
    "    #        predicates[i_p] = get_nlp(\"are\")[0]\n",
    "            \n",
    "    if themes:\n",
    "        for t in themes[0]:\n",
    "            for e in t[1]:\n",
    "                if is_wd_predicate(e):\n",
    "                    predicates.append(t[0])\n",
    "    \n",
    "    predicates_filtered = []\n",
    "    for p in predicates:\n",
    "        if p.lemma_ in PASSIVE_VERBS: \n",
    "            p = get_nlp(p.lemma_)[0]\n",
    "        if len(predicates_filtered) == 0:\n",
    "            predicates_filtered.append(p)\n",
    "        if p.text not in [p.text for p in predicates_filtered]:\n",
    "            predicates_filtered.append(p)\n",
    "            \n",
    "    predicates_ids = []\n",
    "    for i_p, p in enumerate(predicates_filtered):\n",
    "        if p.lemma_ in PASSIVE_VERBS: \n",
    "            predicates_ids.append(get_wd_ids(p.lemma_, is_predicate=True, top_k=top_k, limit=0)[:1])\n",
    "        else:\n",
    "            predicates_ids.append(get_wd_ids(p.text, is_predicate=True, top_k=top_k, limit=0)[:top_k])\n",
    "                \n",
    "    #predicates_ids = [ for p in predicates_filtered]\n",
    "    return merge_lists(predicates_filtered, predicates_ids)\n",
    "\n",
    "#q_test = get_nlp(\"Who voiced the Unicorn in The Last Unicorn\")\n",
    "#q_test = get_nlp(\"Of what nationality is Ken McGoogan\")\n",
    "#q_test = get_nlp(\"Where did roger marquis die\")\n",
    "#q_test = get_nlp(\"who's born in city was alex golfis born in\")\n",
    "#get_predicates(q_test)\n",
    "#q_test_themes = get_themes(q_test)\n",
    "#get_predicates(q_test, q_test_themes, top_k=3)\n",
    "#q0_nlp_test_0 = get_nlp(\"Voiced\")\n",
    "#q0_predicates = get_predicates(q0_nlp, top_k=3)\n",
    "#q0_predicates_test_2 = get_predicates(q0_nlp_test_2, top_k=3)\n",
    "#print(q0_predicates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_ids(to_extract):\n",
    "    return [i for i in it.chain.from_iterable([id[1] for id in to_extract])]\n",
    "#extract_ids([('name', ['id'])]) #q0_themes[0] #q0_focused_parts #q0_predicates\n",
    "#print(extract_ids([(\"The Last Unicorn\", ['Q16614390']),(\"Second Theme\", ['Q12345'])]))\n",
    "#extract_ids(q0_focused_parts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_similarity_by_words(nlp_word_from, nlp_word_to):\n",
    "    if not nlp_word_from or not nlp_word_to:\n",
    "        return 0\n",
    "    elif not nlp_word_from.vector_norm or not nlp_word_to.vector_norm:\n",
    "        return 0\n",
    "    else:\n",
    "        return nlp_word_from.similarity(nlp_word_to)\n",
    "\n",
    "#print(get_similarity_by_words(get_nlp(\"character role\"), get_nlp(\"voice actor\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_similarity_by_ids(word_id_from, word_id_to):\n",
    "    nlp_word_from = get_nlp(get_wd_label(word_id_from))\n",
    "    nlp_word_to = get_nlp(get_wd_label(word_id_to))\n",
    "    return get_similarity_by_words(nlp_word_from, nlp_word_to)\n",
    "\n",
    "#print(get_similarity_by_ids(\"P453\", \"P725\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_similar_statements(statements, from_token_id, similar_to_name, top_k=0, qualifier=False, statement_type=\"object\", time_sentitive=False):\n",
    "    highest_matching_similarity = -1\n",
    "    top_statements = []\n",
    "    nlp_name = get_nlp(similar_to_name)\n",
    "    \n",
    "    #print(\"get_top_similar_statements from_token_id\",from_token_id)\n",
    "    if get_wd_label(from_token_id):\n",
    "        for statement in statements:\n",
    "            if top_k>0:\n",
    "                if qualifier:\n",
    "                    for qualifier in statement['qualifiers']:\n",
    "                        if time_sentitive and is_timestamp(qualifier[statement_type]['id']):\n",
    "                            top_statements.append((1, statement))\n",
    "                        else:\n",
    "                            nlp_word_to = get_nlp(get_wd_label(qualifier[statement_type]['id']))\n",
    "                            matching_similarity = get_similarity_by_words(nlp_name, nlp_word_to)\n",
    "                            top_statements.append((matching_similarity, statement))\n",
    "                else:\n",
    "                    if time_sentitive and is_timestamp(statement[statement_type]['id']):\n",
    "                        top_statements.append((1, statement))\n",
    "                    else:\n",
    "                        nlp_word_to = get_nlp(get_wd_label(statement[statement_type]['id']))\n",
    "                        matching_similarity = get_similarity_by_words(nlp_name, nlp_word_to)\n",
    "                        top_statements.append((matching_similarity, statement))\n",
    "            else:    \n",
    "                if qualifier:\n",
    "                    if statement.get('qualifiers'):\n",
    "                        for qualifier in statement['qualifiers']:\n",
    "                            if time_sentitive and is_timestamp(qualifier[statement_type]['id']):\n",
    "                                top_statements.append((1, statement))\n",
    "                            else:\n",
    "                                nlp_word_to = get_nlp(get_wd_label(qualifier[statement_type]['id']))\n",
    "                                matching_similarity = get_similarity_by_words(nlp_name, nlp_word_to)\n",
    "                                if highest_matching_similarity == -1 or matching_similarity >= highest_matching_similarity:\n",
    "                                    highest_matching_similarity = matching_similarity\n",
    "                                    best_statement = statement\n",
    "                                    top_statements.append((highest_matching_similarity, best_statement))\n",
    "                else:\n",
    "                    if time_sentitive and is_timestamp(statement[statement_type]['id']):\n",
    "                        top_statements.append((1, statement))\n",
    "                    else:\n",
    "                        nlp_word_to = get_nlp(get_wd_label(statement[statement_type]['id']))\n",
    "                        matching_similarity = get_similarity_by_words(nlp_name, nlp_word_to)\n",
    "                        if highest_matching_similarity == -1 or matching_similarity >= highest_matching_similarity:\n",
    "                            highest_matching_similarity = matching_similarity\n",
    "                            best_statement = statement\n",
    "                            top_statements.append((highest_matching_similarity, best_statement))\n",
    "    if top_k > 0:        \n",
    "        return sorted(top_statements, key=lambda x: x[0], reverse=True)[:top_k]\n",
    "    else:\n",
    "        return sorted(top_statements, key=lambda x: x[0], reverse=True)\n",
    "    \n",
    "#statements = get_all_statements_of_entity('Q503992')\n",
    "#top_similar_statements = get_top_similar_statements(statements, 'Q267721', 'western')\n",
    "#print(top_similar_statements)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_similar_statements_by_word_worker(in_mp_queue, out_mp_queue, top_k, qualifier, statement_type, time_sentitive):\n",
    "    sentinel = None\n",
    "    best_statements = []\n",
    "    \n",
    "    for token,similar_to_name in iter(in_mp_queue.get, sentinel):\n",
    "    #    print(\"working on\",token,similar_to_name)\n",
    "        statements = get_all_statements_of_entity(token)\n",
    "        if statements: best_statements += get_top_similar_statements(statements, token, similar_to_name, top_k=top_k, qualifier=qualifier, statement_type=statement_type, time_sentitive=time_sentitive)\n",
    "    #    print(\"done with\",token,similar_to_name)\n",
    "    \n",
    "    out_mp_queue.put(best_statements)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_similar_statements_by_word(from_token_ids, similar_to_name, top_k=3, qualifier=False, statement_type=\"object\", time_sentitive=False, cores=1):\n",
    "    if not similar_to_name:\n",
    "        return []\n",
    "    \n",
    "    best_statements = []\n",
    "    if cores > 1:\n",
    "        out_mp_queue = mp.Queue()\n",
    "        in_mp_queue = mp.Queue()\n",
    "        sentinel = None\n",
    "\n",
    "        for token in from_token_ids:\n",
    "            in_mp_queue.put((token,similar_to_name))\n",
    "\n",
    "        procs = [mp.Process(target = get_best_similar_statements_by_word_worker, args = (in_mp_queue, out_mp_queue, top_k, qualifier, statement_type, time_sentitive)) for i in range(cores)]\n",
    "\n",
    "        for proc in procs:\n",
    "            proc.daemon = True\n",
    "            proc.start()\n",
    "        for proc in procs:    \n",
    "            in_mp_queue.put(sentinel)\n",
    "        for proc in procs:\n",
    "            best_statements += out_mp_queue.get()\n",
    "        for proc in procs:\n",
    "            proc.join()\n",
    "    else:\n",
    "        for token in from_token_ids:\n",
    "            statements = get_all_statements_of_entity(token)\n",
    "            if statements: best_statements += get_top_similar_statements(statements, token, similar_to_name, top_k=top_k, qualifier=qualifier, statement_type=statement_type, time_sentitive=time_sentitive)\n",
    "    \n",
    "    #print(\"best_statements\",best_statements)\n",
    "    return sorted(best_statements, key=lambda x: x[0], reverse=True)\n",
    "\n",
    "#best_similar_statements = get_best_similar_statements_by_word(extract_ids(q0_themes[0]), 'voiced', top_k=3, qualifier=True, statement_type=\"qualifier_object\")\n",
    "#print(best_similar_statements[0])\n",
    "\n",
    "#init_clusters = cluster_extend_by_words(theme_ids, [p[0].text for p in q_predicates+predicates_enhanced], top_k=deep_k, time_sentitive=time_sensitive,cores=2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_statements_subjects_labels(statements):\n",
    "    return [get_wd_label(t[1]['entity']['id']) for t in statements]\n",
    "#print(get_statements_subjects_labels(best_similar_statements))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_statements_predicates_labels(statements):\n",
    "    return [get_wd_label(t[1]['predicate']['id']) for t in statements]\n",
    "#print(get_statements_predicates_labels(best_similar_statements))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_statements_objects_labels(statements):\n",
    "    return [get_wd_label(t[1]['object']['id']) for t in statements]\n",
    "#print(get_statements_objects_labels(best_similar_statements))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_statements_qualifier_predicates_labels(statements):\n",
    "    return [get_wd_label(t[1]['qualifiers'][0]['qualifier_predicate']['id']) for t in statements]\n",
    "#print(get_statements_qualifier_predicates_labels(best_similar_statements))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_statements_qualifier_objects_labels(statements):\n",
    "    return [get_wd_label(t[1]['qualifiers'][0]['qualifier_object']['id']) for t in statements]\n",
    "#print(get_statements_qualifier_objects_labels(best_similar_statements))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_extend_by_words_worker(in_mp_queue, out_mp_queue, top_k, time_sentitive, cores):\n",
    "    sentinel = None\n",
    "    cluster = []\n",
    "    \n",
    "    for cluster_root_ids,name in iter(in_mp_queue.get, sentinel):\n",
    "        cluster += get_best_similar_statements_by_word(cluster_root_ids, name, top_k=top_k, qualifier=True, statement_type=\"qualifier_predicate\", time_sentitive=time_sentitive,cores=1)\n",
    "        cluster += get_best_similar_statements_by_word(cluster_root_ids, name, top_k=top_k, qualifier=True, statement_type=\"qualifier_object\", time_sentitive=time_sentitive,cores=1)\n",
    "        cluster += get_best_similar_statements_by_word(cluster_root_ids, name, top_k=top_k, qualifier=False, statement_type=\"predicate\", time_sentitive=time_sentitive,cores=1)\n",
    "        cluster += get_best_similar_statements_by_word(cluster_root_ids, name, top_k=top_k, qualifier=False, statement_type=\"object\", time_sentitive=time_sentitive,cores=1)\n",
    "        \n",
    "    out_mp_queue.put(cluster)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_extend_by_words(cluster_root_ids, extending_words, top_k=3, time_sentitive=False,cores=mp.cpu_count()):\n",
    "    if not cluster_root_ids or not extending_words:\n",
    "        return []\n",
    "    cluster = []\n",
    "    \n",
    "    if cores <= 0: cores = 1\n",
    "    out_mp_queue = mp.Queue()\n",
    "    in_mp_queue = mp.Queue()\n",
    "    sentinel = None\n",
    "    \n",
    "    for name in extending_words:\n",
    "        in_mp_queue.put((cluster_root_ids,name))\n",
    "        \n",
    "    procs = [mp.Process(target = cluster_extend_by_words_worker, args = (in_mp_queue, out_mp_queue, top_k, time_sentitive, cores)) for i in range(cores)]\n",
    "    \n",
    "    for proc in procs:\n",
    "        proc.daemon = True\n",
    "        proc.start()\n",
    "    for proc in procs:    \n",
    "        in_mp_queue.put(sentinel)\n",
    "    for proc in procs:\n",
    "        cluster += out_mp_queue.get()\n",
    "    for proc in procs:\n",
    "        proc.join()    \n",
    "    \n",
    "    return sorted(cluster, key=lambda x: x[0], reverse=True)\n",
    "    \n",
    "    #start_time = time.time()\n",
    "    #for name in extending_words:\n",
    "    #    #start_cluster_time = time.time()\n",
    "    #    cluster += get_best_similar_statements_by_word(cluster_root_ids, name, top_k=top_k, qualifier=True, statement_type=\"qualifier_predicate\", time_sentitive=time_sentitive,cores=cores)\n",
    "    #    cluster += get_best_similar_statements_by_word(cluster_root_ids, name, top_k=top_k, qualifier=True, statement_type=\"qualifier_object\", time_sentitive=time_sentitive,cores=cores)\n",
    "    #    cluster += get_best_similar_statements_by_word(cluster_root_ids, name, top_k=top_k, qualifier=False, statement_type=\"predicate\", time_sentitive=time_sentitive,cores=cores)\n",
    "    #    cluster += get_best_similar_statements_by_word(cluster_root_ids, name, top_k=top_k, qualifier=False, statement_type=\"object\", time_sentitive=time_sentitive,cores=cores)\n",
    "    #    #end_time = time.time()\n",
    "    #    #print(\"EXTENDING Cluster with:\", name,\" ->\\tRunning time is {}s\".format(round(end_time-start_cluster_time,2)))\n",
    "    ##end_time = time.time()\n",
    "    ##print(\"EXTENDING Clusters ->\\tRunning time is {}s\".format(round(end_time-start_time,2)))\n",
    "    \n",
    "#test_cluster = cluster_extend_by_words(extract_ids(q0_themes[0]), ['voiced'], top_k=2)\n",
    "#test_cluster_test_2 = cluster_extend_by_words(extract_ids(q0_themes_test_2[0]), ['birth'], top_k=2)\n",
    "#print(test_cluster[0])\n",
    "\n",
    "#start_time = time.time()\n",
    "#init_clusters = cluster_extend_by_words(theme_ids, [p[0].text for p in q_predicates+predicates_enhanced], top_k=deep_k, time_sentitive=time_sensitive,cores=mp.cpu_count())\n",
    "#print(\"timer\",time.time()-start_time)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sorts by the similarity value of statements[0]\n",
    "def sort_statements_by_similarity(statements):\n",
    "    return [s for s in sorted(statements, key=lambda x: x[0], reverse=True)]\n",
    "\n",
    "#test_sorted_statements = sort_statements_by_similarity(test_cluster)\n",
    "#test_sorted_statements_test_2 = sort_statements_by_similarity(test_cluster_test_2)\n",
    "#print(test_sorted_statements[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# appends spo from qualifiers, removes qualifier tags, and removes similarity scores\n",
    "def statements_flatter(statements):\n",
    "    best_statements_to_graph = []\n",
    "    for statement in statements:\n",
    "        tmp_statement = copy(statement)\n",
    "        if tmp_statement.get('qualifiers'):\n",
    "            #print(\"statement\", statement)\n",
    "            for q in tmp_statement['qualifiers']:\n",
    "                qualifier_statement = {'entity': {'id': tmp_statement['entity']['id']}}\n",
    "                qualifier_statement['predicate'] = {'id': q['qualifier_predicate']['id']}\n",
    "                qualifier_statement['object'] = {'id': q['qualifier_object']['id']}\n",
    "                best_statements_to_graph.append(qualifier_statement)\n",
    "            del(tmp_statement['qualifiers'])\n",
    "        else:\n",
    "            #print(\"tmp_statement\", tmp_statement)\n",
    "            if ('qualifiers' in tmp_statement): del(tmp_statement['qualifiers'])\n",
    "        if tmp_statement not in best_statements_to_graph:\n",
    "            #print(\"best_statements_to_graph\", tmp_statement)\n",
    "            best_statements_to_graph.append(tmp_statement)\n",
    "        \n",
    "    return best_statements_to_graph\n",
    "\n",
    "#test_flatten_statements = statements_flatter([s[1] for s in test_sorted_statements])\n",
    "#test_flatten_statements_test_2 = statements_flatter([s[1] for s in test_sorted_statements_test_2])\n",
    "#print(test_flatten_statements[0])\n",
    "#test_flatten_statements_test_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove duplicates from statements\n",
    "def unduplicate_statements(statements):\n",
    "    filtered_statements = []\n",
    "    [filtered_statements.append(s) for s in statements if s not in [e for e in filtered_statements]]\n",
    "    return filtered_statements\n",
    "\n",
    "#test_unduplicate_statements = unduplicate_statements(test_flatten_statements)\n",
    "#print(len(test_flatten_statements))\n",
    "#print(len(test_unduplicate_statements))\n",
    "#print(test_unduplicate_statements[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_statements_by_id(statements, from_token_id, to_id, qualifier=False, statement_type=\"predicate\"):\n",
    "    id_statements = []\n",
    "    if not statements:\n",
    "        return id_statements\n",
    "    if get_wd_label(from_token_id):\n",
    "        for statement in statements:\n",
    "            if qualifier:\n",
    "                if statement.get('qualifiers'):\n",
    "                    for s in statement['qualifiers']:\n",
    "                        if to_id == s[statement_type]['id']:\n",
    "                            id_statements.append(statement)\n",
    "            else:\n",
    "                if to_id == statement[statement_type]['id']:\n",
    "                    id_statements.append(statement)\n",
    "    \n",
    "    return id_statements\n",
    "\n",
    "#statements_test = get_all_statements_of_entity('Q176198')\n",
    "#id_statements_test = get_statements_by_id(statements_test, 'Q176198', 'P725')\n",
    "#print(id_statements_test[0])\n",
    "\n",
    "#get_statements_by_id(root_statements, cluster_root_id, predicate_id, qualifier=False, statement_type=\"predicate\")\n",
    "#statements_test = get_all_statements_of_entity('Q176198')\n",
    "#id_statements_test = get_statements_by_id(statements_test, 'Q176198', 'P725')\n",
    "#id_statements_test[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_extend_by_words(cluster_root_ids, extending_words, top_k=3, time_sentitive=False,cores=2):\n",
    "    if not cluster_root_ids or not extending_words:\n",
    "        return []\n",
    "    cluster = []\n",
    "    \n",
    "    if cores <= 0: cores = 1\n",
    "    out_mp_queue = mp.Queue()\n",
    "    in_mp_queue = mp.Queue()\n",
    "    sentinel = None\n",
    "    \n",
    "    for name in extending_words:\n",
    "        in_mp_queue.put((cluster_root_ids,name))\n",
    "        \n",
    "    procs = [mp.Process(target = cluster_extend_by_words_worker, args = (in_mp_queue, out_mp_queue, top_k, time_sentitive, cores)) for i in range(cores)]\n",
    "    \n",
    "    for proc in procs:\n",
    "        proc.daemon = True\n",
    "        proc.start()\n",
    "    for proc in procs:    \n",
    "        in_mp_queue.put(sentinel)\n",
    "    for proc in procs:\n",
    "        cluster += out_mp_queue.get()\n",
    "    for proc in procs:\n",
    "        proc.join()    \n",
    "    \n",
    "    return sorted(cluster, key=lambda x: x[0], reverse=True)\n",
    "    \n",
    "    #start_time = time.time()\n",
    "    #for name in extending_words:\n",
    "    #    #start_cluster_time = time.time()\n",
    "    #    cluster += get_best_similar_statements_by_word(cluster_root_ids, name, top_k=top_k, qualifier=True, statement_type=\"qualifier_predicate\", time_sentitive=time_sentitive,cores=cores)\n",
    "    #    cluster += get_best_similar_statements_by_word(cluster_root_ids, name, top_k=top_k, qualifier=True, statement_type=\"qualifier_object\", time_sentitive=time_sentitive,cores=cores)\n",
    "    #    cluster += get_best_similar_statements_by_word(cluster_root_ids, name, top_k=top_k, qualifier=False, statement_type=\"predicate\", time_sentitive=time_sentitive,cores=cores)\n",
    "    #    cluster += get_best_similar_statements_by_word(cluster_root_ids, name, top_k=top_k, qualifier=False, statement_type=\"object\", time_sentitive=time_sentitive,cores=cores)\n",
    "    #    #end_time = time.time()\n",
    "    #    #print(\"EXTENDING Cluster with:\", name,\" ->\\tRunning time is {}s\".format(round(end_time-start_cluster_time,2)))\n",
    "    ##end_time = time.time()\n",
    "    ##print(\"EXTENDING Clusters ->\\tRunning time is {}s\".format(round(end_time-start_time,2)))\n",
    "    \n",
    "#test_cluster = cluster_extend_by_words(extract_ids(q0_themes[0]), ['voiced'], top_k=2)\n",
    "#test_cluster_test_2 = cluster_extend_by_words(extract_ids(q0_themes_test_2[0]), ['birth'], top_k=2)\n",
    "#print(test_cluster[0])\n",
    "\n",
    "#start_time = time.time()\n",
    "#init_clusters = cluster_extend_by_words(theme_ids, [p[0].text for p in q_predicates+predicates_enhanced], top_k=deep_k, time_sentitive=time_sensitive,cores=mp.cpu_count())\n",
    "#print(\"timer\",time.time()-start_time)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_extend_by_predicates_ids_worker(in_mp_queue, out_mp_queue):\n",
    "    sentinel = None\n",
    "    cluster = []\n",
    "    \n",
    "    for cluster_root_id, predicate_id in iter(in_mp_queue.get, sentinel):\n",
    "        root_statements = get_all_statements_of_entity(cluster_root_id)\n",
    "        if root_statements:\n",
    "            cluster += get_statements_by_id(root_statements, cluster_root_id, predicate_id, qualifier=True, statement_type=\"qualifier_predicate\")\n",
    "            cluster += get_statements_by_id(root_statements, cluster_root_id, predicate_id, qualifier=False, statement_type=\"predicate\")\n",
    "    \n",
    "    out_mp_queue.put(cluster)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters\n",
    "# cluster_root_ids: ['Qcode']\n",
    "# predicates_ids: ['Pcode']\n",
    "def cluster_extend_by_predicates_ids(cluster_root_ids, predicates_ids, cores=mp.cpu_count()):\n",
    "    if not cluster_root_ids or not predicates_ids:\n",
    "        return []\n",
    "    \n",
    "    cluster = []\n",
    "    \n",
    "    if cores <= 0: cores = 1\n",
    "    out_mp_queue = mp.Queue()\n",
    "    in_mp_queue = mp.Queue()\n",
    "    sentinel = None\n",
    "    \n",
    "    for cluster_root_id, predicate_id in it.product(cluster_root_ids, predicates_ids):\n",
    "        #print((cluster_root_id, predicates_id))\n",
    "        in_mp_queue.put((cluster_root_id, predicate_id))\n",
    "        \n",
    "    procs = [mp.Process(target = cluster_extend_by_predicates_ids_worker, args = (in_mp_queue, out_mp_queue)) for i in range(cores)]\n",
    "    \n",
    "    for proc in procs:\n",
    "        proc.daemon = True\n",
    "        proc.start()\n",
    "    for proc in procs:    \n",
    "        in_mp_queue.put(sentinel)\n",
    "    for proc in procs:\n",
    "        cluster += out_mp_queue.get()\n",
    "    for proc in procs:\n",
    "        proc.join()    \n",
    "    \n",
    "    #for cluster_root_id in cluster_root_ids:\n",
    "    #    root_statements = get_all_statements_of_entity(cluster_root_id)\n",
    "    #    #print(\"root_statements\", root_statements)\n",
    "    #    for predicate_id in predicates_ids:\n",
    "    #        cluster += get_statements_by_id(root_statements, cluster_root_id, predicate_id, qualifier=True, statement_type=\"qualifier_predicate\")\n",
    "    #        cluster += get_statements_by_id(root_statements, cluster_root_id, predicate_id, qualifier=False, statement_type=\"predicate\")\n",
    "\n",
    "    return cluster #sorted(cluster, key=lambda x: x[0], reverse=True)\n",
    "    \n",
    "#test_predicate_clusters = cluster_extend_by_predicates_ids(extract_ids(q0_themes[0]), extract_ids(q0_predicates))\n",
    "#print(len(test_predicate_clusters))\n",
    "#test_predicate_clusters[0]\n",
    "\n",
    "#test_predicate_clusters_test_2 = cluster_extend_by_predicates_ids(extract_ids(q0_themes_test_2[0]), extract_ids(q0_predicates_test_2))\n",
    "#print(len(test_predicate_clusters_test_2))\n",
    "#print(test_predicate_clusters_test_2[-1])\n",
    "\n",
    "#predicate_ids_clusters = cluster_extend_by_predicates_ids(theme_ids, predicates_ids+predicates_enhanced_ids)\n",
    "#print(predicate_ids_clusters)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_extractor_from_complements(complements):\n",
    "    for c in complements:\n",
    "        [print(t.pos_) for t in c]\n",
    "    return complements\n",
    "\n",
    "#print(cluster_extractor_from_complements(q0_themes[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#TODO: add cache\n",
    "#TODO: Check if extending with predicate_ids is useful\n",
    "# parameter\n",
    "# question: nlp_string\n",
    "#limits=plt.axis('off')\n",
    "def build_graph(nlp, themes, themes_enhanced, predicates, deep_k=3, time_sensitive = False, cores=mp.cpu_count()):\n",
    "    #print(\"time_sensitive\",time_sensitive)\n",
    "    #start_time = time.time()\n",
    "    theme_ids = extract_ids(themes[0])\n",
    "    theme_enhanced_ids = extract_ids(themes_enhanced)\n",
    "    predicates_ids = extract_ids(predicates)\n",
    "    predicates_enhanced_ids = [p for p in theme_enhanced_ids if is_wd_predicate(p)]\n",
    "    predicates_enhanced = merge_lists([get_nlp(get_wd_label(p)) for p in predicates_enhanced_ids], predicates_enhanced_ids)\n",
    "    \n",
    "    #print(theme_ids)\n",
    "    #print(theme_enhanced_ids)\n",
    "    for i, tei in enumerate(theme_enhanced_ids):\n",
    "        if tei in theme_ids:\n",
    "            tmp = theme_enhanced_ids.pop(i)\n",
    "    \n",
    "    init_clusters = cluster_extend_by_words(theme_ids, [p[0].text for p in predicates+predicates_enhanced], top_k=deep_k, time_sentitive=time_sensitive, cores=cores)\n",
    "    #print(\"init_clusters\",len(init_clusters))\n",
    "    init_clusters_enhanced = cluster_extend_by_words(theme_enhanced_ids, [p[0].text for p in predicates+predicates_enhanced], top_k=deep_k, time_sentitive=time_sensitive, cores=cores)\n",
    "    #print(\"init_clusters_enhanced\",len(init_clusters_enhanced))\n",
    "    init_sorted_statements = sort_statements_by_similarity(init_clusters + init_clusters_enhanced)\n",
    "    #print(\"init_sorted_statements\",len(init_sorted_statements))\n",
    "    init_flatten_statements = statements_flatter([s[1] for s in init_sorted_statements])\n",
    "    #print(\"init_flatten_statements\",len(init_flatten_statements))\n",
    "    \n",
    "    predicate_ids_clusters = cluster_extend_by_predicates_ids(theme_ids, predicates_ids+predicates_enhanced_ids, cores=cores)\n",
    "    #print(\"predicate_ids_clusters\",len(predicate_ids_clusters))\n",
    "    predicate_ids_enhanced_clusters = cluster_extend_by_predicates_ids(theme_enhanced_ids, predicates_ids+predicates_enhanced_ids, cores=cores)\n",
    "    #print(\"predicate_ids_enhanced_clusters\",len(predicate_ids_enhanced_clusters))\n",
    "    predicate_ids_flatten_statements = statements_flatter(predicate_ids_clusters+predicate_ids_enhanced_clusters)\n",
    "    #print(\"predicate_ids_flatten_statements\",len(predicate_ids_flatten_statements))\n",
    "    \n",
    "    clusters = init_flatten_statements+predicate_ids_flatten_statements\n",
    "    filtered_statements = unduplicate_statements(clusters)\n",
    "    #print(predicate_ids_enhanced_clusters)\n",
    "    graph = make_statements_graph(filtered_statements, cores=cores)\n",
    "    \n",
    "    #print([get_wd_label(e) for e in g.nodes] )\n",
    "    \n",
    "\n",
    "    ##print(\"clusters:\", len(clusters))\n",
    "    ##print(\"filtered_statements:\", len(filtered_statements))\n",
    "    #end_time = time.time()\n",
    "    #print(\"->\\tRunning time is {}s\".format(round(end_time-start_time,2)))\n",
    "    \n",
    "    return graph\n",
    "\n",
    "#q0_test = questions[0]\n",
    "#q0_test = \"Which actor voiced the Unicorn in The Last Unicorn?\"\n",
    "#q0_test = \"what was the cause of death of yves klein\"\n",
    "#q0_test = \"Who is the wife of Barack Obama?\"\n",
    "#q0_test = \"Who is the author of Le Petit Prince?\"\n",
    "#q0_nlp_test = get_nlp(q0_test)\n",
    "#q0_themes_test = get_themes(q0_nlp_test, top_k=3)\n",
    "#q0_themes_enhanced_test = get_enhanced_themes(q0_themes_test, top_k=3)\n",
    "#q0_predicates_test = get_predicates_online(q0_nlp_test, top_k=3)\n",
    "#q0_focused_parts_test = []\n",
    "#graph, predicates_dict = build_graph(q0_nlp_test, q0_themes_test, q0_themes_enhanced_test, q0_predicates_test, deep_k=3)\n",
    "#print(predicates_dict)\n",
    "#plot_graph(graph, \"file_name_graph\", \"Graph_title\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_graph_by_names(graph, filtering_names, entities=True, predicates=False):\n",
    "    # remove meaningless subgraphs\n",
    "    graph_copy = graph.copy()\n",
    "    for g in [g for g in (graph.subgraph(c) for c in nx.connected_components(graph))]:\n",
    "        is_meaningful = False\n",
    "        for e in g:\n",
    "            if get_wd_label(e) in filtering_names:\n",
    "                is_meaningful = True\n",
    "                break\n",
    "        if not is_meaningful:\n",
    "            for e in g:\n",
    "                graph_copy.remove_node(e)\n",
    "                \n",
    "    return graph_copy\n",
    "\n",
    "\n",
    "#q_theme_names = [q[0].text for q in q_themes[0]]\n",
    "#q_theme_enhanced_names = [q[0] for q in q_themes_enhanced]\n",
    "#filter_graph_by_names(graph, q_theme_names+q_theme_enhanced_names, entities=True, predicates=False)\n",
    "#plot_graph(graph, \"subgraph_test\", \"subgraph_test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO\n",
    "def filter_graph_by_ids(graph, filtering_ids, entities=True, predicates=False):\n",
    "    # remove meaningless subgraphs\n",
    "    #meaningful_ids = theme_ids+theme_enhanced_ids\n",
    "    #print(\"meaningful_ids\",meaningful_ids)\n",
    "    filtered_graph = nx.Graph()\n",
    "    for g in [g for g in (graph.subgraph(c) for c in nx.connected_components(graph))]:\n",
    "        is_meaningful = False\n",
    "        for e in g:\n",
    "            #print(\"e\",e)\n",
    "            if e in filtering_ids:\n",
    "                is_meaningful = True\n",
    "                #print(\"break\")\n",
    "                break\n",
    "        if is_meaningful:\n",
    "            nx.compose(filtered_graph,g)\n",
    "    #print(\"filtered_graph\",filtered_graph)\n",
    "\n",
    "\n",
    "    #plot_graph(filtered_graph, \"subgraph_test\", \"subgraph_test\")\n",
    "    #plot_graph(graph, \"subgraph_test\", \"subgraph_test\")\n",
    "\n",
    "#q_theme_ids = extract_ids(q_themes[0])\n",
    "#q_theme_enhanced_ids = extract_ids(q_themes_enhanced)\n",
    "#filter_graph_by_ids(graph, q_theme_ids+q_theme_enhanced_ids, entities=True, predicates=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the graph for complements\n",
    "# parameters\n",
    "# name: string\n",
    "def find_name_in_graph(graph, name):\n",
    "    return [x for x,y in graph.nodes(data=True) if y['name'].lower() == name.lower()]\n",
    "\n",
    "#[find_name_in_graph(c.text) for c in q0_themes[1]]\n",
    "#print(find_name_in_graph(graph, \"the unicorn\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the graph for complements\n",
    "# parameters\n",
    "# name: string\n",
    "def find_id_in_graph(graph, id_to_find):\n",
    "    return [x for x,y in graph.nodes(data=True) if x == id_to_find]\n",
    "\n",
    "#[find_name_in_graph(c.text) for c in q0_themes[1]]\n",
    "#print(find_name_in_graph(graph, \"the unicorn\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: clean the complements by removing stopwords etc.\n",
    "def find_theme_complement(graph, themes):\n",
    "    return [i for i in it.chain.from_iterable(\n",
    "        [id for id in [c for c in [find_name_in_graph(graph, t.text) for t in themes[1]] if c]])]\n",
    "\n",
    "#print(find_theme_complement(graph, q0_themes_test))\n",
    "#[i for i in it.chain.from_iterable([id for id in check_theme_complement(graph, q0_themes)])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_paths_in_graph(graph, node_start, node_end):\n",
    "    return [p for p in nx.all_simple_paths(graph, source=node_start, target=node_end)]\n",
    "        \n",
    "#test_paths = find_paths_in_graph(graph, \"Q16205566\", \"Q7774795\")\n",
    "#print(test_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_id_in_graph(graph, node_id):\n",
    "    return graph.has_node(node_id)\n",
    "#print(is_id_in_graph(graph, \"Q24039104\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_name_in_graph(graph, node_name):\n",
    "    return find_name_in_graph(graph, node_name) != []\n",
    "#print(is_name_in_graph(graph, \"the Unicorn\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_paths_for_themes(graph, themes):\n",
    "    themes_ids = [t for t in  extract_ids(themes[0])]\n",
    "    complements_ids = find_theme_complement(graph, themes)\n",
    "    paths = []\n",
    "    for t_id in themes_ids:\n",
    "        if is_id_in_graph(graph, t_id):\n",
    "            for c_id in complements_ids:\n",
    "                if is_id_in_graph(graph, c_id):\n",
    "                    path = find_paths_in_graph(graph, t_id, c_id)\n",
    "                    if path:\n",
    "                        paths.append(path)\n",
    "    paths = [i for i in it.chain.from_iterable(\n",
    "        [id for id in paths])]\n",
    "    \n",
    "    return paths\n",
    "#print(find_paths_for_themes(graph, q0_themes_test))\n",
    "#print(find_paths_for_themes(graph, q0_themes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_node_predicates_from_path(paths):\n",
    "    predicates = []\n",
    "    for p in paths:\n",
    "        [predicates.append(i[:i.find(\"-\")]) for i in p if is_wd_predicate(i[:i.find(\"-\")]) and i[:i.find(\"-\")] not in predicates]\n",
    "    return predicates\n",
    "\n",
    "#test_node_predicates = get_node_predicates_from_path(test_paths)\n",
    "#print(test_node_predicates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_node_predicate_similarity_from_path(paths, predicates):\n",
    "    path_predicates = get_node_predicates_from_path(paths)\n",
    "    return sorted([(pp, get_similarity_by_ids(p2, pp)) for p in predicates for p2 in p[1] for pp in path_predicates], key=lambda x: x[-1], reverse=True)\n",
    "\n",
    "#test_node_pedicate_similarities = get_node_predicate_similarity_from_path(test_paths, q0_predicates)\n",
    "#print(test_node_pedicate_similarities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_focused_parts(nlp_sentence, themes, top_k=3):\n",
    "    W_FILTERS = [\"WDT\", \"WP\", \"WP$\", \"WRB\"]\n",
    "    V_FILTERS = [\"VERB\", \"AUX\"]\n",
    "    \n",
    "    dummy_doc = get_nlp(\"dummy doc\")\n",
    "    \n",
    "    focused_parts = [t.head for t in nlp_sentence if t.tag_ in W_FILTERS] \n",
    "    for fp in focused_parts:\n",
    "        if fp.children:\n",
    "            for c in fp.children:\n",
    "                if c.tag_ not in W_FILTERS and c.text not in [fp.text for fp in focused_parts]: \n",
    "                    focused_parts.append(c)\n",
    "    \n",
    "    #print(\"focused_parts\",focused_parts)\n",
    "    #print(\"themes[0]\",themes[0])\n",
    "    \n",
    "    for t in themes[0]:\n",
    "        for i_fp, fp in enumerate(focused_parts):\n",
    "            #print(\"fp\",fp, type(fp))\n",
    "            for i_w, w in enumerate([w.lower_ for w in t[0]]):\n",
    "                #print(\"w\",w, type(w))\n",
    "                if fp.lower_ == w:\n",
    "                    #print(\"MATCHING\")\n",
    "                    if i_fp+1 < len(focused_parts):\n",
    "                        #print(\"focused_parts[i_fp+1].lower_\",focused_parts[i_fp+1].lower_)\n",
    "                        #print(\"t[0][i_w-1].lower_\",t[0][i_w-1].lower_)\n",
    "                        if focused_parts[i_fp+1].lower_ == t[0][i_w-1].lower_:\n",
    "                            #print(i_fp,fp, t[0][i_w-1], t[0])\n",
    "                            #print(\"BEFORE focused_parts\",focused_parts)\n",
    "                            #print(\"t[0]\",t[0])\n",
    "                            \n",
    "                            if type(t[0]) == type(dummy_doc):\n",
    "                                focused_parts[i_fp] = t[0][:]\n",
    "                            else:\n",
    "                                focused_parts[i_fp] = t[0]\n",
    "                            del focused_parts[i_fp+1]\n",
    "                            #print(\"AFTER focused_parts\",focused_parts)\n",
    "                            \n",
    "    \n",
    "    #print()\n",
    "    #for fp in focused_parts:\n",
    "    #    print(type(fp))\n",
    "    #    \n",
    "    #        print(fp.as_doc())\n",
    "        #if isinstance() == 'spacy.tokens.span.Span':\n",
    "        #    print(\"in\")\n",
    "        #\n",
    "    #focused_parts = [type(fp) for fp in focused_parts]\n",
    "    #print(\"focused_parts\",focused_parts)\n",
    "\n",
    "    focused_parts_ids = [get_wd_ids(p.text, top_k=top_k) for p in focused_parts]\n",
    "    #focused_parts_ids = [get_wd_ids(p.text, top_k=top_k, online=True) for p in focused_parts]\n",
    "    #print(\"focused_parts_ids\",focused_parts_ids)\n",
    "    merged_list = merge_lists(focused_parts, focused_parts_ids)\n",
    "    #print(\"merged_list\",merged_list)\n",
    "    \n",
    "    dummy_span = dummy_doc[:]\n",
    "    merged_list_filtered = []\n",
    "    for ml in merged_list:\n",
    "        if ml[1]:\n",
    "            if type(ml[0]) == type(dummy_span):\n",
    "                merged_list_filtered.append(ml)\n",
    "            elif ml[0].pos_ not in V_FILTERS and not ml[0].is_stop:\n",
    "                merged_list_filtered.append(ml)\n",
    "                    \n",
    "    return merged_list_filtered\n",
    "\n",
    "#q_test_nlp = get_nlp(\"what's akbar tandjung's ethnicity\")\n",
    "#print(get_focused_parts(q0_nlp_test))\n",
    "\n",
    "#q_test_nlp = get_nlp(\"Who voiced the Unicorn in The Last Unicorn?\")\n",
    "#print(get_focused_parts(q0_nlp_test))\n",
    "\n",
    "#q_test_nlp = get_nlp(\"Who is the author that wrote the book Moby Dick\")\n",
    "#q_test_themes = get_themes(q_test_nlp, top_k=3)\n",
    "#get_focused_parts(q_test_nlp,q_test_themes, top_k=3)\n",
    "\n",
    "#q_test_nlp = get_nlp(\"Where was Shigeyasu Suzuki Place of Birth\")\n",
    "#q_test_nlp = get_nlp(\"Who is the author that wrote the book Moby Dick\")\n",
    "#q_test_nlp = get_nlp(\"Where was Shigeyasu Suzuki Place of Birth\")\n",
    "#q_test_themes = get_themes(q_test_nlp, top_k=3)\n",
    "#get_focused_parts(q_test_nlp,q_test_themes, top_k=3)\n",
    "\n",
    "#q_focused_parts: [(Unicorn, ['Q18356448', 'Q21070472', 'Q22043340', 'Q1565614', 'Q30060419']),\n",
    "#(in, ['P642', 'Q29733109', 'P361', 'P131']),\n",
    "#(the, ['Q1408543', 'Q2865743', 'Q29423', 'Q21121474']),\n",
    "#(Unicorn, ['Q18356448', 'Q21070472', 'Q22043340', 'Q1565614', 'Q30060419']),\n",
    "#(The, ['Q1067527', 'Q13423400', 'Q28457426', 'Q24406786', 'Q2430521', 'Q37199001']),\n",
    "#(Last, ['Q16995904', 'Q20072822', 'Q24229340', 'Q20155285'])]\n",
    "\n",
    "#[(author, ['P676', 'Q482980', 'Q3154968']),\n",
    "# (book, ['Q571', 'Q4942925', 'Q997698']),\n",
    "# (Dick, ['Q1471500', 'Q21510351', 'Q249606']),\n",
    "# (Moby, ['Q1954726', 'Q6887412', 'Q14045'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_in_list_by_similarity(word,list_of_words,similarity_threshold):\n",
    "        nlp_word = get_nlp(word)\n",
    "        for lw in list_of_words:\n",
    "            if nlp_word.similarity(get_nlp(lw)) > similarity_threshold:\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "#is_in_list_by_similarity(\"Moby Dick\", [\"moby-dick\",\"star wars\"],0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_compound(nlp_list, themes):\n",
    "    compounded = []\n",
    "    #if not nlp_list[0]:\n",
    "    #    return compounded\n",
    "    try:\n",
    "        for t in [e[0] for e in themes[0]] + themes[1]:\n",
    "            for l in [n[0] for n in nlp_list]:\n",
    "                if l.text.lower() in t.text.lower():\n",
    "                    compounded.append(t.text)\n",
    "        return compounded\n",
    "    except:\n",
    "        return compounded\n",
    "\n",
    "# TODO: make the predicate search go further in the path list for the !i%2\n",
    "def find_paths_keywords(graph, nlp, themes, themes_enhanced, predicates, focused_parts, keywords_len_limit=5, similarity_threshold=0.9):\n",
    "    WH_FILTER = [\"WDT\", \"WP\", \"WP$\", \"WRB\"]\n",
    "    VERB_FILTER = [\"VERB\", \"AUX\"]\n",
    "    NOUN_FILTER = [\"NOUN\",\"PROPN\"]\n",
    "    POSITION_FILTER = [\"ADP\"]\n",
    "    \n",
    "    focused_parts_words = [t[0].text for t in focused_parts]\n",
    "    focused_parts_ids = [j for i in [t[1] for t in focused_parts] for j in i]\n",
    "    focused_parts_predicates_ids = [f for f in focused_parts_ids if is_wd_predicate(f)]\n",
    "    focused_parts_words_ids = [f for f in focused_parts_ids if is_wd_entity(f)]\n",
    "    focused_parts_words_ids_labeled = [get_wd_label(p) for p in focused_parts_words_ids]\n",
    "    #print(focused_parts_words_2)\n",
    "\n",
    "    question_anchors = [t for t in nlp if t.tag_ in WH_FILTER]\n",
    "    themes_enhanced_list = [t[0] for t in themes_enhanced]\n",
    "    focus_themes = [t[0].text for t in themes[0]]\n",
    "    focus_path_by_tails = [[c for c in t.head.children if c.pos_ in NOUN_FILTER] for t in nlp if t.pos_ == \"PRON\"]\n",
    "    focus_part_by_head = [t.head for t in question_anchors]\n",
    "    predicates_nlp = [t for t in nlp if t.pos_ in VERB_FILTER]\n",
    "    predicates_lemma = [t.lemma_ for t in predicates_nlp]\n",
    "    predicates_attention = [t for t in nlp if t.head in predicates_nlp]\n",
    "    predicates_attention_tails = [[c for c in t.children] for t in predicates_attention]\n",
    "    in_attention_heads = [t.head.text for t in nlp if t.pos_ in POSITION_FILTER]\n",
    "    in_attention_tails = add_compound([[c for c in t.children] for t in nlp if t.pos_ in POSITION_FILTER], themes)\n",
    "    focus_themes_enhanced = [t[0] for t in themes_enhanced\n",
    "                             if t[0].lower() in [a.lower() for a in in_attention_tails]\n",
    "                             or t[0].lower() in [a.lower() for a in in_attention_heads]]\n",
    "    \n",
    "    theme_enhanced_ids = extract_ids(themes_enhanced)\n",
    "    predicates_enhanced_ids = [(p) for p in theme_enhanced_ids if is_wd_predicate(p)]\n",
    "    [predicates_enhanced_ids.append(p) for p in focused_parts_predicates_ids if p not in predicates_enhanced_ids]\n",
    "    \n",
    "    alterniative_words = {}\n",
    "    for t in themes_enhanced:\n",
    "        for e in predicates_enhanced_ids:\n",
    "            if e in t[1]:\n",
    "                alterniative_words[t[0]] = [get_nlp(get_wd_label(e)),[e]]\n",
    "            else:\n",
    "                alterniative_words[get_wd_label(e)] = [get_nlp(get_wd_label(e)),[e]]\n",
    "    \n",
    "    #print(\"focused_parts_predicates_ids\",focused_parts_predicates_ids)\n",
    "    #print(\"focused_parts_words_ids\",focused_parts_words_ids)\n",
    "    #print(\"alterniative_words\",alterniative_words)\n",
    "    #print(\"predicates_enhanced_ids\",predicates_enhanced_ids)\n",
    "    ##print(\"predicates_enhanced\",predicates_enhanced)\n",
    "    #print(\"question_anchors\",question_anchors)\n",
    "    #print(\"in_attention_heads\",in_attention_heads)\n",
    "    #print(\"in_attention_tails\",in_attention_tails)\n",
    "    #print(\"focus_themes\",focus_themes)\n",
    "    #print(\"themes_enhanced_list\",themes_enhanced_list)\n",
    "    #print(\"focus_themes_enhanced\",focus_themes_enhanced)\n",
    "    #print(\"focus_path_by_tails\",focus_path_by_tails)\n",
    "    #print(\"focus_part_by_head\",focus_part_by_head)\n",
    "    #print(\"predicates_nlp\",predicates_nlp)\n",
    "    #print(\"predicates_lemma\",predicates_lemma)\n",
    "    #print(\"predicates_attention\",predicates_attention)\n",
    "    #print(\"predicates_attention_tails\",predicates_attention_tails)\n",
    "    #\n",
    "    #print(\"\\n\")\n",
    "    paths_keywords = []\n",
    "    [paths_keywords.append(e.lower()) for e in focused_parts_words + in_attention_heads + in_attention_tails + focus_themes + focus_themes_enhanced + focused_parts_words_ids_labeled if e.lower() not in paths_keywords]\n",
    "    #print(paths_keywords)\n",
    "    #paths_keywords = [p for p in it.permutations(paths_keywords)]\n",
    "    #print(paths_keywords)\n",
    "    \n",
    "    paths_keywords = [p for p in paths_keywords if p and len(p.split(\" \")) <= keywords_len_limit]\n",
    "    \n",
    "    paths_keywords_filtered = []\n",
    "    \n",
    "    #print(\"paths_keywords\",paths_keywords)\n",
    "    #for k in paths_keywords:\n",
    "    #    print(\"current k\",k)\n",
    "    #    #print(\"paths_keywords_filtered before\",paths_keywords_filtered)\n",
    "    #    is_in_list_by_similarity(k, paths_keywords_filtered,similarity_threshold)\n",
    "    \n",
    "    [paths_keywords_filtered.append(k) for k in paths_keywords if not is_in_list_by_similarity(k, paths_keywords_filtered,similarity_threshold)]\n",
    "    \n",
    "    #is_in_list_by_similarity(\"Moby Dick\", [\"moby-dick\",\"star wars\"],0.9)\n",
    "        \n",
    "    \n",
    "    return paths_keywords_filtered, alterniative_words, question_anchors\n",
    "    \n",
    "    #initial_paths = find_paths_for_themes(graph, themes)\n",
    "    #predicate_id_similarities = get_node_predicate_similarity_from_path(initial_paths, predicates)\n",
    "    #best_path = [p for p in initial_paths if predicate_id_similarities[0][0] == p[1][:p[1].find(\"-\")]]\n",
    "    #path_answer = get_wd_label(best_path[0][2]) if best_path else []\n",
    "    \n",
    "    #return (path_answer, best_path[0][2]) if path_answer else (False, False)\n",
    "\n",
    "#paths_keywords_2 = find_paths_keywords(graph_2, q_nlp_2, q_themes_2, q_themes_enhanced_2, q_predicates_2, q_focused_parts_2)\n",
    "#paths_keywords_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_simple_paths_worker(graph, in_mp_queue, out_mp_queue):\n",
    "    found_paths = []\n",
    "    sentinel = None\n",
    "    for source, target in iter(in_mp_queue.get, sentinel):\n",
    "        for path in nx.all_simple_paths(graph, source = source, target = target, cutoff = None):\n",
    "            found_paths.append(path)\n",
    "    out_mp_queue.put(found_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_keywords_nodes_worker(graph, threshold, in_mp_queue, out_mp_queue):\n",
    "    keywords_nodes = []\n",
    "    sentinel = None\n",
    "    for name in iter(in_mp_queue.get, sentinel):\n",
    "        #print(\"2 get_paths_keywords_nodes\")\n",
    "        nlp_lookup = get_nlp(name)\n",
    "        keywords_nodes = [x for x,y in graph.nodes(data=True) if get_nlp(y['name']).similarity(nlp_lookup) >= threshold]\n",
    "    \n",
    "    out_mp_queue.put(keywords_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_nothing(perm):\n",
    "    return perm\n",
    "    \n",
    "def get_paths_keywords_nodes(graph, keywords,threshold=0.9,top_performance=50, cores=mp.cpu_count()):\n",
    "    #print(\"1 get_paths_keywords_nodes\")\n",
    "    if cores <= 0: cores = 1\n",
    "    sentinel = None\n",
    "    \n",
    "    out_mp_queue = mp.Queue()\n",
    "    in_mp_queue = mp.Queue()\n",
    "    \n",
    "    for k in keywords:\n",
    "        in_mp_queue.put(k)\n",
    "\n",
    "    procs = [mp.Process(target = get_keywords_nodes_worker, args = (graph, threshold, in_mp_queue, out_mp_queue)) for i in range(cores)]\n",
    "    \n",
    "    keywords_nodes = []\n",
    "    \n",
    "    for proc in procs:\n",
    "        proc.daemon = True\n",
    "        proc.start()\n",
    "    for proc in procs:    \n",
    "        in_mp_queue.put(sentinel)\n",
    "    for proc in procs:\n",
    "        keywords_nodes.append(out_mp_queue.get())\n",
    "    for proc in procs:\n",
    "        proc.join()\n",
    "        \n",
    "    keywords_nodes = [k for k in keywords_nodes if k]\n",
    "    #print(\"3 get_paths_keywords_nodes\")\n",
    "    \n",
    "    #print(\"1 get_paths_keywords_nodes\")\n",
    "    #keywords_nodes = []\n",
    "    #print(\"len(keywords)\",len(keywords))\n",
    "    #for k in keywords:\n",
    "    #    nlp_lookup = get_nlp(k)\n",
    "    #    keywords_nodes.append([x for x,y in graph.nodes(data=True) if get_nlp(y['name']).similarity(nlp_lookup) >= threshold])\n",
    "    #    print(\"2 get_paths_keywords_nodes\")\n",
    "    #keywords_nodes = [k for k in keywords_nodes if k]\n",
    "    #print(\"3 get_paths_keywords_nodes\")\n",
    "    \n",
    "    #keywords_nodes [['Q17521117', 'Q17521118', 'Q557214', 'Q421946', 'Q11282976', 'Q4677712', 'Q33999'], ['Q7246', 'Q1307944', 'Q21070472', 'Q18356448', 'Q1863113', 'Q20983877', 'Q226755', 'Q22043340'], ['Q176198', 'Q967268', 'Q17553756', 'Q30060419', 'Q17985004', 'Q16614390', 'Q18647334', 'Q15628943'], ['Q176198', 'Q967268', 'Q17553756', 'Q30060419', 'Q17985004', 'Q16614390', 'Q18647334', 'Q15628943'], []]\n",
    "    #keywords_nodes[0] ['Q17521117', 'Q17521118', 'Q557214', 'Q421946', 'Q11282976', 'Q4677712', 'Q33999']\n",
    "    #keywords_nodes[1] ['Q7246', 'Q1307944', 'Q21070472', 'Q18356448', 'Q1863113', 'Q20983877', 'Q226755', 'Q22043340']\n",
    "    \n",
    "    keywords_nodes_per = []\n",
    "    if keywords_nodes:\n",
    "        #print(\"4 get_paths_keywords_nodes\")\n",
    "        if len(keywords_nodes) > 1:\n",
    "            #print(\"5 get_paths_keywords_nodes\")\n",
    "            for kn_i, kn in enumerate(keywords_nodes):\n",
    "                #print(\"6 get_paths_keywords_nodes\")\n",
    "                if kn_i + 1 < len(keywords_nodes):\n",
    "                    if len(kn) * len(keywords_nodes[kn_i+1]) > top_performance:\n",
    "                        if len(kn) <= int(sqrt(top_performance)):\n",
    "                            keywords_nodes[kn_i+1] = keywords_nodes[kn_i+1][:int(top_performance/len(kn))]\n",
    "                        elif len(kn) >= len(keywords_nodes[kn_i+1]):\n",
    "                            kn = kn[:int(top_performance/len(keywords_nodes[kn_i+1]))]\n",
    "                        else:\n",
    "                            kn = kn[:int(sqrt(top_performance))]\n",
    "                            keywords_nodes[kn_i+1] = keywords_nodes[kn_i+1][:int(sqrt(top_performance))]\n",
    "                #print(\"7 get_paths_keywords_nodes\")\n",
    "            \n",
    "            #print(\"8 get_paths_keywords_nodes\")\n",
    "            with mp.Pool() as pool:\n",
    "                keywords_nodes_per = pool.map(do_nothing, it.permutations(keywords_nodes, 2))\n",
    "            #print(\"9 get_paths_keywords_nodes\")\n",
    "            \n",
    "            keywords_nodes_per = [p for p in keywords_nodes_per]\n",
    "            #print(\">1 len(keywords_nodes_per\",len(keywords_nodes_per),keywords_nodes_per[0])\n",
    "                            \n",
    "        else:\n",
    "            keywords_nodes_per = [(keywords_nodes+keywords_nodes)]\n",
    "            #print(\"<1 len(keywords_nodes_per)\",len(keywords_nodes_per),keywords_nodes_per[0])\n",
    "            \n",
    " \n",
    "    #print(\"keywords_nodes_per\",keywords_nodes_per)\n",
    "    #return 0\n",
    "    paths_keyword_nodes = []\n",
    "    \n",
    "\n",
    "    targets = []\n",
    "    sources = []\n",
    "    for pkn in keywords_nodes_per:\n",
    "        [sources.append(pkn0) for pkn0 in pkn[0] if pkn0 not in sources]\n",
    "        [targets.append(pkn1) for pkn1 in pkn[1] if pkn1 not in targets]# and pkn1 not in pkn[0]]\n",
    "\n",
    "    #print(\"len(targets)\",len(targets))\n",
    "    #print(\"len(sources)\",len(sources))\n",
    "\n",
    "    out_mp_queue = mp.Queue()\n",
    "    in_mp_queue = mp.Queue()\n",
    "    sentinel = None\n",
    "\n",
    "    for source, target in it.product(sources, targets):\n",
    "        in_mp_queue.put((source, target))\n",
    "\n",
    "    procs = [mp.Process(target = get_all_simple_paths_worker, args = (graph, in_mp_queue, out_mp_queue)) for i in range(cores)]\n",
    "\n",
    "    for proc in procs:\n",
    "        proc.daemon = True\n",
    "        proc.start()\n",
    "    for proc in procs:    \n",
    "        in_mp_queue.put(sentinel)\n",
    "    for proc in procs:\n",
    "        paths_keyword_nodes.extend(out_mp_queue.get())\n",
    "    for proc in procs:\n",
    "        proc.join()\n",
    "    \n",
    "    paths_keyword_nodes = [p for p in paths_keyword_nodes if p]\n",
    "    #paths_keyword_nodes_filtered = []\n",
    "    #[paths_keyword_nodes_filtered.append(p) for p in paths_keyword_nodes if p not in paths_keyword_nodes_filtered]\n",
    "    #print(\"len(paths_keyword_nodes)\",len(paths_keyword_nodes))\n",
    "    \n",
    "    return paths_keyword_nodes\n",
    "\n",
    "def find_path_nodes_from_graph(nlp_question, graph, predicates_dict, keywords, threshold=0.9,special_pred_theshold=0.7, thres_inter=0.15, top_performance=50,min_paths=3000, cores=mp.cpu_count()):\n",
    "    #print(\"current threshold\", str(round(threshold, 1)))\n",
    "    \n",
    "    w_positions, w_names = w_converter(nlp_question)\n",
    "    w_names_only = [wn[1] for wn in w_names]\n",
    "    date_trigger = \"date\" in w_names_only\n",
    "    location_trigger = \"location\" in w_names_only\n",
    "    all_predicates = list(predicates_dict.keys())\n",
    "\n",
    "    option_keywords = []\n",
    "    \n",
    "    if date_trigger:\n",
    "        nlp_time = get_nlp(\"time\")\n",
    "        nlp_date = get_nlp(\"date\")\n",
    "        \n",
    "        for p in all_predicates:\n",
    "            #print(\"current p\", p)\n",
    "            p_label = get_wd_label(p)\n",
    "            nlp_p = get_nlp(p_label)\n",
    "            #print(\"nlp_p\",nlp_p)\n",
    "            p_date = nlp_p.similarity(nlp_date)\n",
    "            p_time = nlp_p.similarity(nlp_time)\n",
    "            \n",
    "            #print(\"p_date\",p_date)\n",
    "            #print(\"p_time\",p_time)\n",
    "            \n",
    "            if p_date > special_pred_theshold or p_time > special_pred_theshold:\n",
    "                if p not in option_keywords:\n",
    "                    #print(\"adding\",p)\n",
    "                    option_keywords.append(p_label)\n",
    "                    \n",
    "    if location_trigger:\n",
    "        nlp_location = get_nlp(\"location\")\n",
    "        nlp_place = get_nlp(\"place\")\n",
    "        \n",
    "        for p in all_predicates:\n",
    "            #print(\"current p\", p)\n",
    "            p_label = get_wd_label(p)\n",
    "            nlp_p = get_nlp(p_label)\n",
    "            #print(\"nlp_p\",nlp_p)\n",
    "            p_location = nlp_p.similarity(nlp_location)\n",
    "            p_place = nlp_p.similarity(nlp_place)\n",
    "            \n",
    "            #print(p_label, \"p_location\",p_location)\n",
    "            #print(p_label, \"p_place\",p_place)\n",
    "            \n",
    "            if p_location > special_pred_theshold or p_place > special_pred_theshold:\n",
    "                if p not in option_keywords:\n",
    "                    #print(\"adding\",p)\n",
    "                    option_keywords.append(p_label)\n",
    "    \n",
    "    for k in keywords[0]:\n",
    "        nlp_k = get_nlp(get_wd_label(k))\n",
    "        for p in all_predicates:\n",
    "            #print(\"current p\", p)\n",
    "            p_label = get_wd_label(p)\n",
    "            nlp_p = get_nlp(p_label)\n",
    "            #print(\"nlp_p\",nlp_p)\n",
    "            p_k_sim = nlp_p.similarity(nlp_k)\n",
    "\n",
    "            if p_k_sim > special_pred_theshold:\n",
    "                if p not in option_keywords:\n",
    "                    option_keywords.append(p_label)\n",
    "                    \n",
    "    #print(\"keywords[1]\",keywords[1])\n",
    "    \n",
    "    k1_predicates = keywords[1].values() #[[country of citizenship, ['P27']], [country of citizenship, ['P27']]]\n",
    "    #print(\"k1_predicates\",k1_predicates)\n",
    "    k1_predicates = sum([[get_wd_label(p) for p in e[1]] for e in k1_predicates],[])\n",
    "    #print(\"k1_predicates\",k1_predicates)\n",
    "\n",
    "                \n",
    "    #print(\"option_keywords\",option_keywords)\n",
    "    \n",
    "    all_keywords = []\n",
    "    [all_keywords.append(k) for k in keywords[0] + option_keywords + k1_predicates if k and k not in all_keywords]\n",
    "    \n",
    "    #print(\"all_keywords\",all_keywords)\n",
    "    \n",
    "    main_keyword_paths = get_paths_keywords_nodes(graph, all_keywords,threshold=threshold,top_performance=top_performance, cores=cores)\n",
    "    alternative_keyword_paths = []\n",
    "    \n",
    "    #for k_1 in keywords[1]:\n",
    "    #    for i, k_0 in enumerate(all_keywords):\n",
    "    #        if k_1==k_0:\n",
    "    #            tmp_keywords = all_keywords.copy()\n",
    "    #            tmp_keywords[i] = keywords[1][k_1][0].text\n",
    "    #            alternative_keyword_paths += get_paths_keywords_nodes(graph, all_keywords,threshold=threshold,top_performance=top_performance, cores=cores)\n",
    "    \n",
    "    keyword_paths = main_keyword_paths#+alternative_keyword_paths\n",
    "    \n",
    "    #print(\"BEFORE len(keyword_paths)\",len(keyword_paths))\n",
    "    keyword_paths_filtered=[]\n",
    "    [keyword_paths_filtered.append(p) for p in keyword_paths if p not in keyword_paths_filtered]\n",
    "    keyword_paths = keyword_paths_filtered\n",
    "    #print(\"keyword_paths\",len(keyword_paths))\n",
    "    \n",
    "    #print(\"len(keyword_paths)\",len(keyword_paths))\n",
    "    if len(keyword_paths) < min_paths:\n",
    "        if threshold == 0: return keyword_paths\n",
    "        threshold -= thres_inter\n",
    "        if threshold < 0: threshold = 0\n",
    "        keyword_paths = get_paths_keywords_nodes(graph, all_keywords,threshold=threshold,top_performance=top_performance, cores=cores)\n",
    "        \n",
    "        keyword_paths_filtered=[]\n",
    "        [keyword_paths_filtered.append(p) for p in keyword_paths if p not in keyword_paths_filtered]\n",
    "        keyword_paths = keyword_paths_filtered\n",
    "        \n",
    "    #keyword_paths_filtered = []\n",
    "    \n",
    "    #print(\"AFTER len(keyword_paths)\",len(keyword_paths))\n",
    "    #[keyword_paths_filtered.append(p) for p in keyword_paths if p not in keyword_paths_filtered]\n",
    "    \n",
    "    \n",
    "    \n",
    "    return keyword_paths\n",
    "\n",
    "\n",
    "#path_nodes_2 = find_path_nodes_from_graph(nlp_question,graph_2, predicates_dict,paths_keywords_2, threshold=0.9, special_pred_theshold=0.7, thres_inter=0.15, top_performance=50, min_paths=3000)\n",
    "#end_time = time.time()\n",
    "\n",
    "#start_time = time.time()\n",
    "#path_nodes = find_path_nodes_from_graph(nlp_question,graph, predicates_dict,paths_keywords, threshold=0.8, thres_inter=0.1, top_performance=graph.size(),min_paths=3000,cores=2)\n",
    "#print(\"--> len(path_nodes):\",len(path_nodes))\n",
    "#print(\"Finding path nodes ->\\tRunning time is {}s\".format(round(time.time()-start_time,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_sublist(a, b):\n",
    "    if not a: return True\n",
    "    if not b: return False\n",
    "    #if a == b: return False\n",
    "    return b[:len(a)] == a or is_sublist(a, b[1:])\n",
    "\n",
    "def paths_nodes_filter_is_sublist_worker(in_mp_queue, out_mp_queue, filtered_paths):\n",
    "    found_paths = []\n",
    "    sentinel = None\n",
    "    \n",
    "    #print(\"HI I AM A PROCESSOR\", filtered_paths)\n",
    "    for i, fp in iter(in_mp_queue.get, sentinel):\n",
    "        for fp_2 in filtered_paths:\n",
    "            #print(\"will process\",i,fp,fp_2)\n",
    "            if (is_sublist(fp, fp_2) and fp!=fp_2):\n",
    "                #print(\"processed\",i,fp,fp_2)\n",
    "                out_mp_queue.put(i)\n",
    "                break\n",
    "    \n",
    "    out_mp_queue.put(False)\n",
    "    #print(\"I AM TERMINATED\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "#node_predicates_names_2 = get_node_predicates_from_path(path_nodes_2)\n",
    "\n",
    "\n",
    "def paths_nodes_filter(path_nodes, graph, cores=mp.cpu_count(), with_sublists=True):\n",
    "    \n",
    "    filtered_paths = []\n",
    "    \n",
    "    for path in path_nodes:\n",
    "        filtered_row = []\n",
    "        for i,p in enumerate(path):\n",
    "            if is_wd_predicate(p[:p.find(\"-\")]):\n",
    "                if i == 0:\n",
    "                    #if p[:p.find(\"-\")] == \"P725\":\n",
    "                    #    print(p)\n",
    "                    neighbor = [k for k in graph[p].keys() if k != path[i+1]]\n",
    "                    if neighbor:\n",
    "                        filtered_row.append(neighbor[0])\n",
    "                        filtered_row.append(p[:p.find(\"-\")])\n",
    "                    else:\n",
    "                        continue\n",
    "                    #print(filtered_row)\n",
    "                elif i > 0 and i < len(path)-1:\n",
    "                    filtered_row.append(p[:p.find(\"-\")])\n",
    "                else:\n",
    "                    neighbor = [k for k in graph[p].keys() if k != path[i-1]]\n",
    "                    if neighbor:\n",
    "                        filtered_row.append(p[:p.find(\"-\")])\n",
    "                        filtered_row.append(neighbor[0])\n",
    "                    else:\n",
    "                        continue\n",
    "            else: filtered_row.append(p)\n",
    "        \n",
    "        #print(\"filtered_paths\",filtered_paths)\n",
    "        \n",
    "        if len(filtered_row) > 1 and filtered_row not in filtered_paths: \n",
    "            filtered_paths.append(filtered_row)\n",
    "    \n",
    "    if with_sublists:\n",
    "        if cores <= 0: cores = 1\n",
    "\n",
    "        out_mp_queue = mp.Queue()\n",
    "        in_mp_queue = mp.Queue()\n",
    "        sentinel = None\n",
    "\n",
    "        for i,fp in enumerate(filtered_paths):\n",
    "            in_mp_queue.put((i, fp))\n",
    "\n",
    "        procs = [mp.Process(target = paths_nodes_filter_is_sublist_worker, args = (in_mp_queue, out_mp_queue, filtered_paths)) for i in range(cores)]\n",
    "\n",
    "        to_remove_idexes = []\n",
    "        for proc in procs:\n",
    "            proc.daemon = True\n",
    "            proc.start()\n",
    "        for proc in procs:    \n",
    "            in_mp_queue.put(sentinel)\n",
    "        for proc in procs:\n",
    "            to_remove_idexes.append(out_mp_queue.get())\n",
    "        for proc in procs:\n",
    "            proc.join(1)\n",
    "        \n",
    "        #print(\"to_remove_idexes\",to_remove_idexes)\n",
    "        \n",
    "\n",
    "        for tri in to_remove_idexes:\n",
    "            if tri:\n",
    "                filtered_paths[tri] = []\n",
    "\n",
    "        unique_paths = [p for p in filtered_paths if p]\n",
    "\n",
    "        unique_paths_with_reversed = []\n",
    "\n",
    "        for up in unique_paths:\n",
    "            reversed_up = list(reversed(up))\n",
    "            if up not in unique_paths_with_reversed: \n",
    "                unique_paths_with_reversed.append(up)\n",
    "            if reversed_up not in unique_paths_with_reversed: \n",
    "                unique_paths_with_reversed.append(reversed_up)\n",
    "\n",
    "        #print(\"unique_paths\",len(unique_paths))\n",
    "\n",
    "        #for i, up in enumerate(unique_paths):\n",
    "        #    for up_2 in unique_paths:\n",
    "        #        if (list(reversed(up)) == up_2):\n",
    "        #            unique_paths[i] = []\n",
    "        #            break\n",
    "\n",
    "\n",
    "        #cleaned_paths = []\n",
    "        #unique_paths = [up for up in unique_paths if up]\n",
    "\n",
    "        #for up in unique_paths:\n",
    "        #    for i,e in enumerate(up):\n",
    "        #        if not is_wd_predicate(e):\n",
    "        #            for j,r in enumerate(list(reversed(up))): \n",
    "        #                if not is_wd_predicate(r):\n",
    "        #                    cleaned_paths.append(up[i:-j])\n",
    "        #            break\n",
    "\n",
    "        #print(\"cleaned_paths\",len(cleaned_paths))\n",
    "\n",
    "        #cleaned_paths = [c for c in cleaned_paths if len(c) > 2]\n",
    "\n",
    "        #unique_paths = cleaned_paths.copy()\n",
    "        #for i,fp in enumerate(cleaned_paths):\n",
    "        #    for fp_2 in cleaned_paths:\n",
    "        #        if (is_sublist(fp, fp_2) and fp!=fp_2):\n",
    "        #            unique_paths[i] = []\n",
    "        #            break\n",
    "\n",
    "        #unique_paths = [p for p in unique_paths if len(p) > 2]       \n",
    "\n",
    "        #for i, up in enumerate(unique_paths):\n",
    "        #    for up_2 in unique_paths:\n",
    "        #        if (list(reversed(up)) == up_2):\n",
    "        #            unique_paths[i] = []\n",
    "        #            break\n",
    "\n",
    "            #print(up)\n",
    "        #[up for up in unique_paths if up and not is_wd_predicate(up[-1]) and not is_wd_predicate(up[0])]\n",
    "        #print()\n",
    "        #for up in unique_paths:\n",
    "        #    print(up)\n",
    "        #    break\n",
    "        #    return []\n",
    "    else:\n",
    "        unique_paths_with_reversed = filtered_paths\n",
    "    \n",
    "    return [p for p in unique_paths_with_reversed if len(p) > 2] #False#[up for up in unique_paths if up and not is_wd_predicate(up[-1]) and not is_wd_predicate(up[0])]#False# [p for p in unique_paths if p]\n",
    "                \n",
    "#paths_nodes_filtered_2 = paths_nodes_filter(path_nodes_2, graph_2)\n",
    "#print(\"unique_paths\", len(paths_nodes_filtered_2))\n",
    "#for p in paths_nodes_filtered_2:\n",
    "#    print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def w_converter(nlp):\n",
    "    w_positions = []\n",
    "    w_names = []\n",
    "    for i_q,q in enumerate(nlp):\n",
    "        if q.lemma_ == \"where\": \n",
    "            w_positions.append((i_q))\n",
    "            w_names.append((i_q,\"location\"))\n",
    "        elif q.lemma_ == \"when\": \n",
    "            w_positions.append((i_q))\n",
    "            w_names.append((i_q,\"date\"))\n",
    "        elif q.lemma_ == \"who\": \n",
    "            w_positions.append((i_q))\n",
    "            w_names.append((i_q,\"person\"))    \n",
    "        elif q.lemma_ == \"why\": \n",
    "            w_positions.append(i_q)\n",
    "            w_names.append((i_q,\"cause\"))\n",
    "        elif q.lemma_ == \"which\": \n",
    "            w_positions.append(i_q)\n",
    "            w_names.append((i_q,\"which\"))\n",
    "        elif q.lemma_ == \"what\": \n",
    "            w_positions.append(i_q)\n",
    "            w_names.append((i_q,\"what\"))\n",
    "        elif i_q+1 < len(nlp) and q.lemma_ == \"how\" and (nlp[i_q+1].lemma_ == \"much\" or nlp[i_q+1].lemma_ == \"many\"): \n",
    "            w_positions.append(i_q)\n",
    "            w_names.append((i_q,\"quantity\"))\n",
    "    return w_positions, w_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_entity_similarity(word_id, entity_type, banned_labels=[], max_reward=2.0):\n",
    "    \n",
    "    LOCATION_FILTER = [\"GPE\", \"FAC\", \"LOC\",\"PERSON\"]\n",
    "    PERSON_FILTER = [\"PERSON\",\"NORP\",\"ORG\",\"PER\"]\n",
    "    DATE_FILTER = [\"DATE\",\"TIME\"]\n",
    "    CAUSE_FILTER = [\"NORP\",\"PRODUCT\",\"EVENT\",\"MISC\"]\n",
    "    WHICH_FILTER = PERSON_FILTER+DATE_FILTER+[\"GPE\",\"LOC\",\"PRODUCT\",\"EVENT\",\n",
    "                    \"WORK_OF_ART\",\"LAW\",\"LANGUAGE\",\"MISC\"]\n",
    "    WHAT_FILTER = LOCATION_FILTER+DATE_FILTER+CAUSE_FILTER+PERSON_FILTER+[\"WORK_OF_ART\",\"LAW\",\"LANGUAGE\"]\n",
    "                    \n",
    "    QUANTITY_FILTER = [\"PERCENT\", \"MONEY\", \"QUANTITY\", \"ORDINAL\", \"CARDINAL\"]\n",
    "    \n",
    "    ALL_FILTER = LOCATION_FILTER + PERSON_FILTER + DATE_FILTER + CAUSE_FILTER + WHICH_FILTER + WHAT_FILTER + QUANTITY_FILTER\n",
    "    \n",
    "    similarities = []\n",
    "    word_label = get_wd_label(word_id)\n",
    "    \n",
    "    is_banned_label = word_label.lower() in banned_labels\n",
    "    if word_label == \"\" and not is_timestamp(word_id):\n",
    "        return similarities\n",
    "\n",
    "    word_ents = get_kb_ents(word_label)\n",
    "    \n",
    "    #print(word_id,word_label,entity_type,[e.label_ for e in word_ents])\n",
    "    \n",
    "    #if is_timestamp(word_id):\n",
    "        #print(\"is_timestamp\")\n",
    "        #print(\"word_ents\",word_ents)\n",
    "    \n",
    "    if is_timestamp(word_id) and entity_type == \"date\" and not is_banned_label:\n",
    "        similarities.append(max_reward)\n",
    "        #print(\"in the condition\", word_id, entity_type, similarities)\n",
    "    elif word_ents and not is_banned_label:\n",
    "        for ent in word_ents:\n",
    "            if (entity_type in ALL_FILTER and ent.label_ == entity_type):\n",
    "                similarities.append(max_reward)\n",
    "            \n",
    "            elif ent.kb_id_ == word_id:\n",
    "                if entity_type == \"location\" and ent.label_ in LOCATION_FILTER:\n",
    "                    similarities.append(max_reward)\n",
    "                elif entity_type == \"person\" and ent.label_ in PERSON_FILTER:\n",
    "                    similarities.append(max_reward)\n",
    "                elif entity_type == \"date\" and (ent.label_ in DATE_FILTER):\n",
    "                    similarities.append(max_reward)\n",
    "                elif entity_type == \"cause\" and ent.label_ in CAUSE_FILTER:\n",
    "                    similarities.append(max_reward)\n",
    "                elif entity_type == \"which\" and ent.label_ in WHICH_FILTER:\n",
    "                    similarities.append(max_reward)\n",
    "                elif entity_type == \"what\" and ent.label_ in WHAT_FILTER:\n",
    "                    similarities.append(max_reward)\n",
    "                elif entity_type == \"quantity\" and ent.label_ in QUANTITY_FILTER:\n",
    "                    similarities.append(max_reward)\n",
    "                else:                     \n",
    "                    similarities.append(get_similarity_by_words(get_nlp(word_label),get_nlp(entity_type)))\n",
    "            else: similarities.append(get_similarity_by_words(get_nlp(word_label),get_nlp(entity_type)))\n",
    "    else:\n",
    "        similarities.append(get_similarity_by_words(get_nlp(word_label),get_nlp(entity_type)))\n",
    "    #print(\"get_entity_similarity:\",word_label, entity_type, similarities)\n",
    "    return similarities\n",
    "\n",
    "#get_entity_similarity(\"place of birth\", \"location\", [], max_reward=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "#    \"whats the name of the organization that was founded by  frei otto\",\n",
    "#    \"What is the name of the writer of The Secret Garden?\",\n",
    "#    \"Where did roger marquis die\",\n",
    "#    \"Which genre of album is harder.....faster?\",\n",
    "#    \"Which actor voiced the Unicorn in The Last Unicorn?\",\n",
    "#    \"Who voiced the Unicorn in The Last Unicorn?\",\n",
    "#    \"Which is the nation of Martha Mattox\",\n",
    "#    \"Who is the wife of Barack Obama?\",\n",
    "#    \"of what nationality is ken mcgoogan\",\n",
    "#    \"which stadium do the wests tigers play in\",\n",
    "#    \"Who is the author that wrote the book Moby Dick\",\n",
    "#    \"Which equestrian was is in dublin ?\",\n",
    "#    \"how does engelbert zaschka identify\t\",\n",
    "#    \"Who influenced michael mcdowell?\",\n",
    "#    \"what does  2674 pandarus orbit\"\n",
    "# \"Who is the author that wrote the book Moby Dick\"\n",
    "\n",
    "#question=\"Which actor voiced the Unicorn in The Last Unicorn?\"\n",
    "#verbose=True\n",
    "#aggressive=False\n",
    "#looped=False\n",
    "#deep_k=5\n",
    "#deep_k_step=1\n",
    "#graph_size_min=100\n",
    "#graph_size_max=5000\n",
    "#timer=True\n",
    "#g_paths=False\n",
    "#get_graph=False\n",
    "#cores=mp.cpu_count()\n",
    "#\n",
    "#\n",
    "#if verbose: start_time = time.time()\n",
    "#if timer: timer_time = time.time()\n",
    "#if verbose: print(\"Auto correcting question:\",question)\n",
    "#q_nlp = get_nlp(question, autocorrect=True)\n",
    "#if verbose: print(\"-> q_nlp:\",q_nlp)\n",
    "#q_themes = get_themes(q_nlp, top_k=2, online=True)\n",
    "#q_theme_names = [q[0].text for q in q_themes[0]]\n",
    "#if verbose: print(\"-> q_themes:\",q_themes)\n",
    "#q_themes_enhanced = get_enhanced_themes(q_themes, top_k=1, aggressive=aggressive)\n",
    "#q_theme_enhanced_names = [q[0] for q in q_themes_enhanced]\n",
    "#if verbose: print(\"-> q_themes_enhanced:\",q_themes_enhanced)\n",
    "#if verbose: print(\"--> Calculating predicates... (could be long.. depends on uncached unpure predicates)\")\n",
    "#q_predicates_db = get_predicates(q_nlp, q_themes, top_k=0)\n",
    "#q_predicates_online = get_predicates_online(q_nlp, top_k=2, aggressive=aggressive)\n",
    "#q_predicates = []\n",
    "#q_predicates_db_ids = [p[1] for p in q_predicates_db]\n",
    "#q_predicates_db_names = [p[0] for p in q_predicates_db]\n",
    "#q_predicates_online_ids = [p[1] for p in q_predicates_online]\n",
    "#q_predicates_online_names = [p[0] for p in q_predicates_online]\n",
    "#for i_n,n in enumerate(q_predicates_db_names):\n",
    "#    pn_online_text = [n.text for n in q_predicates_online_names]\n",
    "#    tmp_ids = q_predicates_db_ids[i_n]\n",
    "#    if n.text in pn_online_text:\n",
    "#        for p_o in q_predicates_online_ids[pn_online_text.index(n.text)]:\n",
    "#            if p_o not in tmp_ids:\n",
    "#                tmp_ids.append(p_o)\n",
    "#    q_predicates.append((n,tmp_ids))\n",
    "#\n",
    "#for i_n_o,n_o in enumerate(q_predicates_online_names):\n",
    "#    n_db_text = [n.text for n in q_predicates_db_names]\n",
    "#    if n_o.text not in n_db_text:\n",
    "#        q_predicates.append((n_o, q_predicates_online_ids[i_n_o]))\n",
    "#\n",
    "#if verbose: print(\"-> q_predicates:\",q_predicates)\n",
    "#if timer: \n",
    "#    print(\"->\\tRunning time is {}s\".format(round(time.time()-timer_time,2)))\n",
    "#    timer_time = time.time()\n",
    "#q_focused_parts = get_focused_parts(q_nlp, q_themes, top_k=2)\n",
    "#if verbose: print(\"-> q_focused_parts:\",q_focused_parts)\n",
    "#if verbose: print(\"-> Building the graph with k_deep\",str(deep_k),\"... (could be long)\")\n",
    "#\n",
    "#if deep_k<=1:\n",
    "#    deep_k = 1\n",
    "#    graph, predicates_dict = build_graph(q_nlp, q_themes, q_themes_enhanced, q_predicates, deep_k=deep_k, cores=cores)\n",
    "#    if timer: \n",
    "#        print(\"->\\tRunning time is {}s\".format(round(time.time()-timer_time,2)))\n",
    "#        timer_time = time.time()\n",
    "#else:\n",
    "#    for k in range(1, deep_k, deep_k_step):\n",
    "#        graph, predicates_dict = build_graph(q_nlp, q_themes, q_themes_enhanced, q_predicates, deep_k=deep_k, cores=cores)\n",
    "#        if timer: \n",
    "#            print(\"->\\tRunning time is {}s\".format(round(time.time()-timer_time,2)))\n",
    "#            timer_time = time.time()\n",
    "#\n",
    "#        if verbose: print(\"--> \",len(graph), \"nodes and\", graph.size(), \"edges\")\n",
    "#        if verbose: print(\"--> Removing meaningless subgraphs\")\n",
    "#        graph = filter_graph_by_names(graph, q_theme_names+q_theme_enhanced_names, entities=True, predicates=False)\n",
    "#        if verbose: print(\"--> New graph of:\",len(graph), \"nodes and\", graph.size(), \"edges\")\n",
    "#\n",
    "#        if graph.size() > graph_size_max or len(graph) > graph_size_max:\n",
    "#            deep_k -= deep_k_step\n",
    "#            if verbose: print(\"---> Rebuilding the graph with k_deep\",str(deep_k), \"... Previously:\",len(graph), \"nodes or\", graph.size(), \"edges was above the limit of\",graph_size_max)\n",
    "#        else: break\n",
    "#\n",
    "### Auto-scaling the number of nodes in graph\n",
    "##previous_deep_k = deep_k\n",
    "##if graph.size() <= graph_size_min:\n",
    "##    for k in range(deep_k, 10, deep_k_step):\n",
    "##        if verbose: print(\"---> Rebuilding the graph with k_deep\",str(k), \"... Previously:\",len(graph), \"nodes or\", graph.size(), \"edges was below the limit of\",graph_size_min)\n",
    "##        graph, predicates_dict = build_graph(q_nlp, q_themes, q_themes_enhanced, q_predicates, deep_k=k, cores=cores)\n",
    "##        if timer: \n",
    "##            print(\"->\\tRunning time is {}s\".format(round(time.time()-timer_time,2)))\n",
    "##            timer_time = time.time()\n",
    "#\n",
    "#if get_graph: \n",
    "#    plot_graph(graph, \"file_name_graph\", \"Graph_title\")\n",
    "#\n",
    "##if graph.size() > 510 or len(graph) > 510:\n",
    "##    if verbose: print(\"Stopping the computing here, too computational.\")\n",
    "##    return False\n",
    "#if verbose: print(\"-> predicates_dict:\",predicates_dict)\n",
    "#paths_keywords = find_paths_keywords(graph, q_nlp, q_themes, q_themes_enhanced, q_predicates, q_focused_parts)\n",
    "#if verbose: print(\"-> paths_keywords:\",paths_keywords)\n",
    "#if timer: timer_time = time.time()\n",
    "#if verbose: print(\"-> Computing possible paths... (could be long)\")\n",
    "#if graph.size() < 500:\n",
    "#    top_perf = graph.size()\n",
    "#else:\n",
    "##    top_perf = 500\n",
    "#path_nodes = find_path_nodes_from_graph(nlp_question, graph, predicates_dict,paths_keywords, threshold=0.8,special_pred_theshold=0.7, thres_inter=0.1, top_performance=graph.size(), min_paths=100, cores=cores)\n",
    "##for p in path_nodes:\n",
    "##    print(\"path_nodes\",path_nodes)\n",
    "#if verbose: print(\"--> len(path_nodes):\",len(path_nodes))\n",
    "#if timer: \n",
    "#    print(\"->\\tRunning time is {}s\".format(round(time.time()-timer_time,2)))\n",
    "#    timer_time = time.time()\n",
    "#\n",
    "#if len(path_nodes) < 20000:\n",
    "#    if verbose: print(\"-> Filtering paths... (could be long)\")\n",
    "#    paths_nodes_filtered = paths_nodes_filter(path_nodes, graph)\n",
    "#    if verbose: print(\"--> len(paths_nodes_filtered):\",len(paths_nodes_filtered))\n",
    "#    if timer: \n",
    "#        print(\"->\\tRunning time is {}s\".format(round(time.time()-timer_time,2)))\n",
    "#        timer_time = time.time()\n",
    "#else: \n",
    "#    if verbose: print(\"--> Skipping paths filtering... (too much paths)\")\n",
    "#    paths_nodes_filtered = path_nodes\n",
    "#    \n",
    "#    \n",
    "#if verbose: print(\"-> Computing hypothesises...\")\n",
    "#hypothesises = get_hypothesises(q_nlp, q_predicates, q_themes, paths_keywords, paths_nodes_filtered, threshold=0.5, max_reward=2.0) \n",
    "#if verbose: print(\"\\n\\n--> test_hypothesises:\",hypothesises)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hypothesises(nlp, predicates_dict, predicates, themes, paths_keywords, filtered_paths, threshold=0.5, special_pred_theshold=0.7, max_reward=2.0):#, themes, themes_enhanced):\n",
    "    \n",
    "    w_positions, w_names = w_converter(nlp)\n",
    "    w_names_only = [wn[1] for wn in w_names]\n",
    "    #print(\"w_positions\",w_positions)\n",
    "    #print(\"w_names\",w_names)\n",
    "    #print(\"w_names_only\",w_names_only)\n",
    "    \n",
    "    date_trigger = \"date\" in w_names_only\n",
    "    print(\"date_trigger\",date_trigger)\n",
    "    \n",
    "    BANNED_PREDICATATE_IDS = [\"P31\"]\n",
    "    complementary_predicates = paths_keywords[0]+[p[0] for p in list(paths_keywords[1].values())]\n",
    "\n",
    "    nlp_time = get_nlp(\"time\")\n",
    "    nlp_date = get_nlp(\"date\")\n",
    "    \n",
    "    #locate positions   \n",
    "    anchors_positions = []\n",
    "    anchors_focuses = []\n",
    "    #keywords_positions = []\n",
    "    #predicates_positions = []\n",
    "    \n",
    "    theme_keywords = [t[0] for t in themes[0]]\n",
    "    \n",
    "    predicate_ids = sum([p[1] for p in predicates if p[1]],[])\n",
    "    predicate_names = [get_nlp(p[0].text) for p in predicates]\n",
    "    #print(\"predicate_ids\",predicate_ids)\n",
    "    #print(\"predicate_names\",predicate_names)\n",
    "    \n",
    "    \n",
    "    \n",
    "    [anchors_positions.append(i) for i, w in enumerate(nlp) if w in paths_keywords[2]]\n",
    "    #print(\"\\nanchors_positions:\",anchors_positions)\n",
    "    \n",
    "    #anchors_childrens\n",
    "    for p in anchors_positions:\n",
    "        children = [c for c in nlp[p].children]\n",
    "        if children == []: \n",
    "            children = [c for c in nlp[p].head.children]\n",
    "        else: \n",
    "            if nlp[p].head:\n",
    "                children.append(nlp[p].head)\n",
    "        \n",
    "        anchors_focuses += ([c for c in children\n",
    "               if c not in [nlp[a] for a in anchors_positions]\n",
    "               and c.pos_ != \"PUNCT\"])\n",
    "        \n",
    "        if not anchors_focuses:\n",
    "            anchors_focuses = [nlp[p].head]\n",
    "        \n",
    "        anchors_focuses += complementary_predicates\n",
    "        #print(\"\\nanchors_focuses\",anchors_focuses)\n",
    "    \n",
    "    anchors_focuses_filtered = []\n",
    "    \n",
    "    for af in anchors_focuses:\n",
    "        if isinstance(af, str):\n",
    "            anchors_focuses_filtered.append(af)\n",
    "        else:\n",
    "            anchors_focuses_filtered.append(af.text)\n",
    "        \n",
    "    anchors_focuses = []\n",
    "    [anchors_focuses.append(af) for af in anchors_focuses_filtered if af not in anchors_focuses and af]\n",
    "    #print(\"\\nanchors_focuses\",anchors_focuses)\n",
    "    \n",
    "    #find anchor position in paths\n",
    "    anchors_predicates = []\n",
    "    \n",
    "    main_predicate_ids = []\n",
    "    main_predicate_names = []\n",
    "    [main_predicate_ids.append(p) for p in predicate_ids+sum([p[1] for p in list(paths_keywords[1].values())],[]) if p not in main_predicate_ids]\n",
    "    #print(\"paths_keywords[1]\",paths_keywords[1])\n",
    "    #print(\"main_predicate_ids\",main_predicate_ids)\n",
    "    \n",
    "    #print(\"[p[0] for p in list(paths_keywords[1].values())]\",[p[0].text for p in list(paths_keywords[1].values())])\n",
    "    [main_predicate_names.append(p) for p in predicate_names+[get_nlp(p[0].text) for p in list(paths_keywords[1].values())] if p not in main_predicate_names]\n",
    "    #print(\"paths_keywords[1]\",paths_keywords[1])\n",
    "    #print(\"main_predicate_names\",main_predicate_names)\n",
    "    #\n",
    "    #return 0\n",
    "    \n",
    "    for p in filtered_paths:\n",
    "        p_len = len(p)\n",
    "        for i_e, e in enumerate(p):\n",
    "            if is_wd_predicate(e):\n",
    "                #print(\"predicate\",e)\n",
    "                if main_predicate_ids:\n",
    "                    if e in main_predicate_ids and e not in BANNED_PREDICATATE_IDS:\n",
    "                        if e not in [ap[0] for ap in anchors_predicates]:\n",
    "                            if date_trigger:\n",
    "                                time_similarity = get_similarity_by_words(get_nlp(get_wd_label(e)),nlp_time)\n",
    "                                date_similarity = get_similarity_by_words(get_nlp(get_wd_label(e)),nlp_date)\n",
    "                                #print(\"main_predicate_ids\", e, \"time_similarity\",time_similarity)\n",
    "                                #print(\"main_predicate_ids\", e, \"date_similarity\",date_similarity)\n",
    "                                if time_similarity > date_similarity:\n",
    "                                    anchors_predicates.append((e, time_similarity))\n",
    "                                else: anchors_predicates.append((e, date_similarity))\n",
    "                            else:\n",
    "                                anchors_predicates.append((e, max_reward))\n",
    "                    elif e not in [ap[0] for ap in anchors_predicates]:\n",
    "                        stat_count = 0\n",
    "                        stat_current = 0\n",
    "                        for pn in main_predicate_names:\n",
    "                            stat_current += get_similarity_by_words(get_nlp(get_wd_label(e)),pn)\n",
    "                            stat_count += 1\n",
    "                        for pi in main_predicate_ids:\n",
    "                            stat_current += get_similarity_by_words(get_nlp(get_wd_label(e)),get_nlp(get_wd_label(pi)))\n",
    "                            stat_count += 1\n",
    "                        if date_trigger:\n",
    "                            time_similarity = get_similarity_by_words(get_nlp(get_wd_label(e)),nlp_time)\n",
    "                            date_similarity = get_similarity_by_words(get_nlp(get_wd_label(e)),nlp_date)\n",
    "                            #print(\"if main_pred -> date_trigger -> elif e not\",e)\n",
    "                            #print(\"time_similarity\",time_similarity)\n",
    "                            #print(\"date_similarity\",date_similarity)\n",
    "                            if time_similarity > special_pred_theshold or date_similarity > special_pred_theshold:\n",
    "                                if stat_count > 1: \n",
    "                                    stat_count -= 1\n",
    "                                else: stat_count += 1\n",
    "                                if time_similarity > date_similarity:\n",
    "                                    anchors_predicates.append((e, time_similarity))\n",
    "                                else: anchors_predicates.append((e, date_similarity))\n",
    "                        anchors_predicates.append((e, stat_current/stat_count))\n",
    "                        \n",
    "                elif e not in [ap[0] for ap in anchors_predicates]:\n",
    "                    stat_count = 0\n",
    "                    stat_current = 0\n",
    "                    for af in anchors_focuses:\n",
    "                        stat_current += get_similarity_by_words(get_nlp(get_wd_label(e)),get_nlp(af))\n",
    "                        stat_count += 1\n",
    "                    if date_trigger:\n",
    "                        time_similarity = get_similarity_by_words(get_nlp(get_wd_label(e)),nlp_time)\n",
    "                        date_similarity = get_similarity_by_words(get_nlp(get_wd_label(e)),nlp_date)\n",
    "                        #print(\"if not main_pred -> date_trigger -> elif e not\",e)\n",
    "                        #print(\"time_similarity\",time_similarity)\n",
    "                        #print(\"date_similarity\",date_similarity)\n",
    "                        if time_similarity > special_pred_theshold or date_similarity > special_pred_theshold:\n",
    "                            if stat_count > 1:\n",
    "                                stat_count -= 1\n",
    "                            else: stat_count += 1\n",
    "                            if time_similarity > date_similarity:\n",
    "                                anchors_predicates.append((e, time_similarity))\n",
    "                            else: anchors_predicates.append((e, time_similarity))\n",
    "                    anchors_predicates.append((e, stat_current/stat_count))\n",
    "            \n",
    "    \n",
    "    #print(\"filtered_paths\",filtered_paths)\n",
    "    #for p in filtered_paths:\n",
    "    #    for af in anchors_focuses:\n",
    "    #        #print(af, p)\n",
    "    #        for e in p:\n",
    "    #            #print(af,get_wd_label(e))\n",
    "    #            if is_wd_predicate(e):# and e not in [ap[0] for ap in anchors_predicates]:\n",
    "    #                #print(af,get_wd_label(e))\n",
    "    #                anchors_predicates.append([e, get_similarity_by_words(get_nlp(get_wd_label(e)),get_nlp(af))])\n",
    "                \n",
    "    #print(\"\\nanchors_predicates\",anchors_predicates)\n",
    "    \n",
    "    anchors_predicates_filtered = []\n",
    "    [anchors_predicates_filtered.append(ap) for ap in anchors_predicates if ap not in anchors_predicates_filtered]\n",
    "    \n",
    "    #print(\"\\anchors_predicates_filtered\",anchors_predicates_filtered)\n",
    "    \n",
    "    #anchors_predicates = [a for a in sorted(anchors_predicates_filtered, key=lambda x: x[-1], reverse=True) if a[1] > threshold]\n",
    "    \n",
    "    for thres in [e/100 for e in reversed(range(10, int(threshold*100)+10, 10))]:\n",
    "        #print(\"anchors_predicates current thres\",thres)\n",
    "        anchors_predicates = [a for a in sorted(anchors_predicates_filtered, key=lambda x: x[-1], reverse=True) if a[1] > thres]\n",
    "        if anchors_predicates:\n",
    "            break\n",
    "            \n",
    "    \n",
    "    #print(\"len(anchors_predicates sorted)\",len(anchors_predicates))\n",
    "    #print(\"anchors_predicates sorted\",anchors_predicates)\n",
    "    \n",
    "    #anchors_predicates_filtered = []\n",
    "    #for ap in anchors_predicates:\n",
    "    #    for af in anchors_focuses:\n",
    "    #        anchors_predicates_filtered.append([ap[0],get_similarity_by_words(get_nlp(get_wd_label(ap[0])),get_nlp(af))])\n",
    "    #\n",
    "    #anchors_predicates_filtered = [a for a in sorted(anchors_predicates_filtered, key=lambda x: x[-1], reverse=True) if a[1] > 0]\n",
    "    \n",
    "    #for thres in [e/100 for e in reversed(range(10, int(threshold*100)+10, 10))]:\n",
    "    #    print(\"anchors_predicates_filtered current thres\",thres)\n",
    "    #    if not anchors_predicates_filtered:\n",
    "    #        anchors_predicates_filtered = anchors_predicates\n",
    "    #        break\n",
    "    #    anchors_predicates_filtered = [a for a in sorted(anchors_predicates_filtered, key=lambda x: x[-1], reverse=True) if a[1] > thres]\n",
    "    #    if len(anchors_predicates) > 10:\n",
    "    #        break\n",
    "    \n",
    "    #print(\"len(anchors_predicates_filtered)\",len(anchors_predicates_filtered))\n",
    "    #print(\"anchors_predicates_filtered\",anchors_predicates_filtered)\n",
    "    #\n",
    "    #anchors_predicates=[]\n",
    "    #[anchors_predicates.append(apf) for apf in anchors_predicates_filtered if apf not in anchors_predicates]\n",
    "    #print(\"len(anchors_predicates)\",len(anchors_predicates))\n",
    "    #print(\"anchors_predicates\",anchors_predicates)\n",
    "    \n",
    "    tuples_unique_ids = []\n",
    "    tuples_unique_predicate_ids = []\n",
    "    \n",
    "    hypothesises_tuples = []\n",
    "    for ap in anchors_predicates:\n",
    "        #print(\"ap\",ap)\n",
    "        for fp in filtered_paths:\n",
    "            #if \"Q4985\" in fp:\n",
    "            #    print(\"Q4985 in fp\",fp, ap)\n",
    "            for i, e in enumerate(fp):\n",
    "                #print(e)\n",
    "                if e == ap[0] and i>1 and i<len(fp)-1:\n",
    "                    #print(i, [fp[i-1], fp[i], fp[i+1]])\n",
    "                    hypothesis_tuple = [fp[i-1], fp[i], fp[i+1]]\n",
    "                    if hypothesis_tuple not in hypothesises_tuples:\n",
    "                        hypothesises_tuples.append(hypothesis_tuple)\n",
    "                        if hypothesis_tuple[0] not in tuples_unique_ids:\n",
    "                            tuples_unique_ids.append(hypothesis_tuple[0])\n",
    "                        if hypothesis_tuple[1] not in tuples_unique_predicate_ids:\n",
    "                            tuples_unique_predicate_ids.append(hypothesis_tuple[1])\n",
    "                        if hypothesis_tuple[2] not in tuples_unique_ids:\n",
    "                            tuples_unique_ids.append(hypothesis_tuple[2])\n",
    "                        #if \"Q4985\" in hypothesis_tuple:\n",
    "                        #    print(\"Q4985 hypothesis_tuple\",hypothesis_tuple, ap,fp)\n",
    "                        \n",
    "    #print(\"tuples_unique_ids\",tuples_unique_ids)\n",
    "    #print(\"tuples_unique_predicate_ids\",tuples_unique_predicate_ids)\n",
    "    \n",
    "    hypothesises_unique_ids = [t for t in tuples_unique_ids if get_wd_label(t).lower() not in anchors_focuses]\n",
    "    if len(hypothesises_unique_ids)>0 and len(tuples_unique_ids)>0:\n",
    "        max_reward *= len(hypothesises_unique_ids)/len(tuples_unique_ids)\n",
    "    \n",
    "    #print(\"hypothesises_unique_ids\",hypothesises_unique_ids)\n",
    "    \n",
    "    #print(\"hypothesises_tuples\",hypothesises_tuples)\n",
    "    #print(\"hypothesises_tuples\",hypothesises_tuples)\n",
    "    #print([a[0] for a in anchors_predicates])\n",
    "\n",
    "    #keywords_ids = [i for j in [get_wd_ids(k) for k in anchors_focuses if get_wd_ids(k)] for i in j]\n",
    "    #print(\"anchors_focuses\",keywords_ids)\n",
    "    #print(extract_ids(themes[0]))\n",
    "    #print(extract_ids(themes_enhanced))\n",
    "    #keywords_ids = []\n",
    "    #[keywords_ids.append(i) for i in extract_ids(themes[0]) + extract_ids(themes_enhanced) if i not in keywords_ids]\n",
    "    #print(\"keywords_ids\",keywords_ids)\n",
    "    \n",
    "    #print(\"anchors_predicates\",anchors_predicates)\n",
    "    \n",
    "    #print(\"-------START FILTERING-------\")\n",
    "    \n",
    "    hypothesises = []\n",
    "    hypothesises_all = []\n",
    "    hypothesises_tuples_len = len(hypothesises_tuples)\n",
    "    \n",
    "    keywords_similarity_threshold = 0.9\n",
    "    \n",
    "    tmp_to_find=\"\" # for debugging, set the ID of the element to track in the log as print\n",
    "    for ht in hypothesises_tuples:\n",
    "        if tmp_to_find in ht: print(\"ht\",ht)\n",
    "        if ht[1] in [a[0] for a in anchors_predicates]:\n",
    "            for i_af, af in enumerate(anchors_focuses):\n",
    "                hypo_sum = 0\n",
    "                nlp_af = get_nlp(af)\n",
    "                nlp_ht0 = get_nlp(get_wd_label(ht[0]))\n",
    "                nlp_ht2 = get_nlp(get_wd_label(ht[2]))\n",
    "                if not nlp_ht2:\n",
    "                    break\n",
    "\n",
    "                af_lemma = ' '.join([e.lower_ for e in nlp_af if e.pos_ != \"DET\"])\n",
    "                ht0_lemma = ' '.join([e.lower_ for e in nlp_ht0 if e.pos_ != \"DET\"])\n",
    "                ht2_lemma = ' '.join([e.lower_ for e in nlp_ht2 if e.pos_ != \"DET\"])\n",
    "                \n",
    "                #if get_wd_label(ht[0]).lower() not in anchors_focuses and get_wd_label(ht[2]).lower() not in anchors_focuses:\n",
    "                #    for es in get_entity_similarity(ht[0], wn[1], anchors_focuses, max_reward=max_reward):\n",
    "                #        hypo_sum += es\n",
    "                \n",
    "                if (\n",
    "                    nlp_af.text.lower() != nlp_ht2.text.lower() \n",
    "                    and af_lemma != nlp_ht2[0].text.lower()\n",
    "                    and nlp_af.text.lower() != ht2_lemma\n",
    "                    and af_lemma != ht2_lemma\n",
    "                   ):\n",
    "                    \n",
    "                    if date_trigger:\n",
    "                        if is_timestamp(ht[0]):\n",
    "                            for es in get_entity_similarity(ht[0], \"date\", anchors_focuses, max_reward=max_reward):\n",
    "                                hypo_sum += es\n",
    "                                #print(\"if date hypo_sum\",ht[0], \"date\",ht[0], es, hypo_sum)\n",
    "                        \n",
    "                        else: hypo_sum += get_similarity_by_words(nlp_ht2, nlp_af)\n",
    "                    else: hypo_sum += get_similarity_by_words(nlp_ht2, nlp_af)\n",
    "                    \n",
    "                    if i_af in w_positions:\n",
    "                        for wn in w_names:\n",
    "                            if i_af == wn[0]:\n",
    "                                for es in get_entity_similarity(ht[0], wn[1], anchors_focuses, max_reward=max_reward):\n",
    "                                    hypo_sum += es\n",
    "                                    if tmp_to_find in ht: print(\"if i_af hypo_sum\",\"ht[0], wn[1], es\",ht[0], wn[1], es,hypo_sum)\n",
    "                    \n",
    "                    ht0_sum = 0\n",
    "                    ht2_sum = 0\n",
    "                    \n",
    "                    if is_timestamp(ht[0]): ht0_label = ht[0]\n",
    "                    else: ht0_label = get_wd_label(ht[0]).lower()\n",
    "                    if is_timestamp(ht[2]): ht2_label = ht[2]\n",
    "                    else: ht2_label = get_wd_label(ht[2]).lower()\n",
    "                    \n",
    "                    for tk in theme_keywords:\n",
    "                        if tmp_to_find in ht: print(\"tk\",tk)\n",
    "                        if tmp_to_find in ht: print(\"ht0_label\",ht0_label)\n",
    "                        if tmp_to_find in ht: print(\"ht2_label\",ht2_label)\n",
    "                        \n",
    "                        nlp_tk = get_nlp(tk.text.lower())\n",
    "                        ht0_label_similarity = get_nlp(ht0_label).similarity(nlp_tk)\n",
    "                        ht2_label_similarity = get_nlp(ht2_label).similarity(nlp_tk)\n",
    "                        \n",
    "                        if tmp_to_find in ht: print(\"ht0_label_similarity\",ht0_label_similarity)\n",
    "                        if tmp_to_find in ht: print(\"ht2_label_similarity\",ht2_label_similarity)\n",
    "#\n",
    "                        if ht0_label_similarity > keywords_similarity_threshold and ht[1] in main_predicate_ids:\n",
    "                            if tmp_to_find in ht: print(\"ht0_label\",ht0_label)\n",
    "                            for wn in w_names_only:\n",
    "                                for es in get_entity_similarity(ht[2], wn, anchors_focuses, max_reward=max_reward*3):\n",
    "                                    if tmp_to_find in ht: print(\"ht0_sum main_predicate_ids before\",ht0_sum)\n",
    "                                    ht0_sum += es\n",
    "                                    if tmp_to_find in ht: print(\"theme_keywords ht0_sum ht[2], wn, es\",ht[2], wn, es, ht0_sum)\n",
    "                                    if tmp_to_find in ht: print(\"ht0_label\",ht2_label,es, ht0_sum, ht)\n",
    "                        elif ht0_label_similarity > keywords_similarity_threshold:\n",
    "                            if tmp_to_find in ht: print(\"ht0_label\",ht0_label)\n",
    "                            for wn in w_names_only:\n",
    "                                for es in get_entity_similarity(ht[2], wn, anchors_focuses, max_reward=max_reward*2):\n",
    "                                    if tmp_to_find in ht: print(\"ht0_sum before\",ht0_sum)\n",
    "                                    ht0_sum += es\n",
    "                                    if tmp_to_find in ht: print(\"theme_keywords not main_predicate_ids ht0_sum ht[2], wn, es\",ht[2], wn, es, ht0_sum)\n",
    "                                    if tmp_to_find in ht: print(\"ht0_label\",ht2_label,es, ht0_sum, ht)\n",
    "#\n",
    "                        if ht2_label_similarity > keywords_similarity_threshold and ht[1] in main_predicate_ids:\n",
    "                            if tmp_to_find in ht: print(\"ht2_label\",ht2_label)\n",
    "                            for wn in w_names_only:\n",
    "                                for es in get_entity_similarity(ht[0], wn, anchors_focuses, max_reward=max_reward*3):\n",
    "                                    if tmp_to_find in ht: print(\"ht2_sum before\",ht0_sum)\n",
    "                                    ht2_sum += es\n",
    "                                    if tmp_to_find in ht: print(\"theme_keywords main_predicate_ids ht2_sum ht[0], wn, es\",ht[0], wn, es, ht2_sum)\n",
    "                                    if tmp_to_find in ht: print(\"ht2_label\",ht0_label,es, ht2_sum, ht)\n",
    "                        elif ht2_label_similarity > keywords_similarity_threshold:\n",
    "                            if tmp_to_find in ht: print(\"ht2_label\",ht2_label)\n",
    "                            for wn in w_names_only:\n",
    "                                for es in get_entity_similarity(ht[0], wn, anchors_focuses, max_reward=max_reward*2):\n",
    "                                    if tmp_to_find in ht: print(\"ht2_sum before\",ht0_sum)\n",
    "                                    ht2_sum += es\n",
    "                                    if tmp_to_find in ht: print(\"theme_keywords not main_predicate_ids ht2_sum ht[0], wn, es\",ht[0], wn, es, ht2_sum)\n",
    "                                    if tmp_to_find in ht: print(\"ht2_label\",ht0_label,es, ht2_sum, ht)\n",
    "                                                       \n",
    "                    for ap in anchors_predicates:\n",
    "                        if ap[0] == ht[1]:\n",
    "                            for wn in w_names_only:\n",
    "                                for es in get_entity_similarity(ht[0], wn, anchors_focuses, max_reward=max_reward*2):\n",
    "                                    ht0_sum += es\n",
    "                                    if tmp_to_find in ht: print(\"anchors_predicates w_names_only ht0_sum ht[0], wn, es\",ht[0], wn, es, ht0_sum)\n",
    "                                for es in get_entity_similarity(ht[2], wn, anchors_focuses, max_reward=max_reward*2):\n",
    "                                    ht2_sum += es\n",
    "                                    if tmp_to_find in ht: print(\"anchors_predicates w_names_only ht2_sum ht[2], wn, es\",ht[2], wn, es, ht2_sum)\n",
    "                                    \n",
    "                            for tk in theme_keywords:\n",
    "                                if tmp_to_find in ht: print(\"anchors_predicates tk\",tk)\n",
    "                                if tmp_to_find in ht: print(\"anchors_predicates ht0_label\",ht0_label)\n",
    "                                if tmp_to_find in ht: print(\"anchors_predicates ht2_label\",ht2_label)\n",
    "\n",
    "                                nlp_tk = get_nlp(tk.text.lower())\n",
    "                                ht0_label_similarity = get_nlp(ht0_label).similarity(nlp_tk)\n",
    "                                ht2_label_similarity = get_nlp(ht2_label).similarity(nlp_tk)\n",
    "\n",
    "                                if tmp_to_find in ht: print(\"anchors_predicates ht0_label_similarity\",ht0_label_similarity)\n",
    "                                if tmp_to_find in ht: print(\"anchors_predicates ht2_label_similarity\",ht2_label_similarity)\n",
    "\n",
    "                                if ht0_label_similarity > keywords_similarity_threshold and ht[1] in main_predicate_ids:\n",
    "                                    if tmp_to_find in ht: print(\"anchors_predicates ht0_label\",ht0_label)\n",
    "                                    for wn in w_names_only:\n",
    "                                        for es in get_entity_similarity(ht[2], wn, anchors_focuses, max_reward=max_reward*2):\n",
    "                                            if tmp_to_find in ht: print(\"anchors_predicates ht0_sum main_predicate_ids before\",ht0_sum)\n",
    "                                            ht0_sum += es\n",
    "                                            if tmp_to_find in ht: print(\"anchors_predicates theme_keywords ht0_sum ht[2], wn, es\",ht[2], wn, es, ht0_sum)\n",
    "                                            if tmp_to_find in ht: print(\"anchors_predicates ht0_label\",ht2_label,es, ht0_sum, ht)\n",
    "                                elif ht0_label_similarity > keywords_similarity_threshold:\n",
    "                                    if tmp_to_find in ht: print(\"anchors_predicates ht0_label\",ht0_label)\n",
    "                                    for wn in w_names_only:\n",
    "                                        for es in get_entity_similarity(ht[2], wn, anchors_focuses, max_reward=max_reward):\n",
    "                                            if tmp_to_find in ht: print(\"anchors_predicates ht0_sum before\",ht0_sum)\n",
    "                                            ht0_sum += es\n",
    "                                            if tmp_to_find in ht: print(\"anchors_predicates theme_keywords not main_predicate_ids ht0_sum ht[2], wn, es\",ht[2], wn, es, ht0_sum)\n",
    "                                            if tmp_to_find in ht: print(\"anchors_predicates ht0_label\",ht2_label,es, ht0_sum, ht)\n",
    "                                                \n",
    "                                if ht2_label_similarity > keywords_similarity_threshold and ht[1] in main_predicate_ids:\n",
    "                                    if tmp_to_find in ht: print(\"anchors_predicates ht2_label\",ht2_label)\n",
    "                                    for wn in w_names_only:\n",
    "                                        for es in get_entity_similarity(ht[0], wn, anchors_focuses, max_reward=max_reward*2):\n",
    "                                            if tmp_to_find in ht: print(\"anchors_predicates ht2_sum before\",ht0_sum)\n",
    "                                            ht2_sum += es\n",
    "                                            if tmp_to_find in ht: print(\"anchors_predicates theme_keywords main_predicate_ids ht2_sum ht[0], wn, es\",ht[0], wn, es, ht2_sum)\n",
    "                                            if tmp_to_find in ht: print(\"anchors_predicates ht2_label\",ht0_label,es, ht2_sum, ht)\n",
    "                                elif ht2_label_similarity > keywords_similarity_threshold:\n",
    "                                    if tmp_to_find in ht: print(\"anchors_predicates ht2_label\",ht2_label)\n",
    "                                    for wn in w_names_only:\n",
    "                                        for es in get_entity_similarity(ht[0], wn, anchors_focuses, max_reward=max_reward):\n",
    "                                            if tmp_to_find in ht: print(\"anchors_predicates ht2_sum before\",ht0_sum)\n",
    "                                            ht2_sum += es\n",
    "                                            if tmp_to_find in ht: print(\"anchors_predicates theme_keywords not main_predicate_ids ht2_sum ht[0], wn, es\",ht[0], wn, es, ht2_sum)\n",
    "                                            if tmp_to_find in ht: print(\"anchors_predicates ht2_label\",ht0_label,es, ht2_sum, ht)\n",
    "                                \n",
    "                                    \n",
    "                            \n",
    "                            if date_trigger and is_timestamp(ht0_label) and ht2_label in anchors_focuses:\n",
    "                                hypo_sum += ht0_sum\n",
    "                                #print(\"is_timestamp(ht0_label) hypo_sum\", hypo_sum)\n",
    "                            elif date_trigger and is_timestamp(ht2_label) and ht0_label in anchors_focuses:\n",
    "                                hypo_sum += ht2_sum\n",
    "                                #print(\"is_timestamp(ht2_label) hypo_sum\", hypo_sum)\n",
    "                            elif ht2_label in anchors_focuses and ht0_label not in anchors_focuses:\n",
    "                                hypo_sum += ht2_sum\n",
    "                                if tmp_to_find in ht: print(\"ht2_label hypo_sum in anchors_focuses\", hypo_sum)\n",
    "                            elif ht0_label in anchors_focuses and ht2_label not in anchors_focuses:\n",
    "                                hypo_sum += ht0_sum\n",
    "                                if tmp_to_find in ht: print(\"ht0_label hypo_sum in anchors_focuses\", hypo_sum)\n",
    "                            else:\n",
    "                                hypo_sum += ht0_sum\n",
    "                                hypo_sum += ht2_sum\n",
    "                                if tmp_to_find in ht: print(\"else in anchors_focuses hypo_sum\", hypo_sum)\n",
    "                                \n",
    "                                \n",
    "                            if tmp_to_find in ht: print(\"hypo_sum\",hypo_sum)\n",
    "                            if tmp_to_find in ht: print(\"ap[1]\",ap[1])\n",
    "                            hypo_sum *= ap[1]\n",
    "                            \n",
    "                            if tmp_to_find in ht: print(\"ht[0], ht[2], hypo_sum\",ht[0], ht[2], hypo_sum)\n",
    "                            \n",
    "                            #if get_wd_label(ht[0]).lower() in anchors_focuses:\n",
    "                            #    if not i_af in w_positions: \n",
    "                            #        hypo_sum += abs(ap[1])\n",
    "                            #    else: hypo_sum -= abs(ap[1])\n",
    "                            \n",
    "                                \n",
    "                                    \n",
    "                            #if ht[0] == \"Q202725\": print(\"hypo_sum\",hypo_sum)\n",
    "                            \n",
    "                                        \n",
    "\n",
    "                            #else: hypo_sum = ap[1]\n",
    "                            #hypo_sum *= abs(ap[1])\n",
    "                                                        \n",
    "                            \n",
    "                            #break\n",
    "                            #print(\"ap\",ap, \"ht\",ht, \"hypo_sum\",hypo_sum)\n",
    "                            #print(ht)\n",
    "                            #break\n",
    "                            #hypo_sum = abs(hypo_sum)\n",
    "                            #hypo_sum += abs(ap[1])\n",
    "                            #hypo_sum += abs(ap[1])\n",
    "                            #hypo_sum += ap[1]\n",
    "                            #hypo_sum += abs(hypo_sum)\n",
    "                            #hypo_sum *= abs(ap[1])\n",
    "                            \n",
    "                            \n",
    "                            #hypo_sum = abs(hypo_sum)\n",
    "                            #hypo_sum /= ap[1]\n",
    "                            #hypo_sum -= ap[1]\n",
    "                            #hypo_sum += hypo_sum/ap[1]\n",
    "                    \n",
    "                    #print(\"ht[0]\",ht[0])\n",
    "                    #print(\"ht[2]\",ht[2])\n",
    "                    \n",
    "                    if (date_trigger and is_timestamp(ht[0]) \n",
    "                        and not is_timestamp(ht[2]) \n",
    "                        and is_in_list_by_similarity(get_wd_label(ht[2]).lower(), anchors_focuses, keywords_similarity_threshold)):\n",
    "                        #print(\"is_timestamp(ht[0]\")\n",
    "                        hypo = ht[0]\n",
    "                    \n",
    "                    elif (date_trigger and is_timestamp(ht[2]) \n",
    "                        and not is_timestamp(ht[0]) \n",
    "                        and is_in_list_by_similarity(get_wd_label(ht[0]).lower(), anchors_focuses, keywords_similarity_threshold)):\n",
    "                        #print(\"is_timestamp(ht[2]\")\n",
    "                        hypo = ht[2]\n",
    "                    \n",
    "                    elif date_trigger and is_timestamp(ht[0]) and is_timestamp(ht[2]): break\n",
    "                    \n",
    "                    #is_in_list_by_similarity(\"Moby Dick\", [\"moby-dick\",\"star wars\"],0.9)\n",
    "                    \n",
    "                    #elif get_wd_label(ht[0]).lower() in anchors_focuses:\n",
    "                    #    #print(\"get_wd_label(ht[0]).lower()\",get_wd_label(ht[0]).lower())\n",
    "                    #    if not get_wd_label(ht[2]).lower() in anchors_focuses:\n",
    "                    #        hypo = ht[2]\n",
    "                    #    if get_wd_label(ht[2]).lower() in anchors_focuses:\n",
    "                    #        break\n",
    "                    \n",
    "                    elif is_in_list_by_similarity(get_wd_label(ht[0]).lower(), anchors_focuses, keywords_similarity_threshold):\n",
    "                        if not is_in_list_by_similarity(get_wd_label(ht[2]).lower(), anchors_focuses, keywords_similarity_threshold):\n",
    "                            hypo = ht[2]\n",
    "                        else: break\n",
    "                            \n",
    "                    elif not is_in_list_by_similarity(get_wd_label(ht[0]).lower(), anchors_focuses, keywords_similarity_threshold):\n",
    "                        if is_in_list_by_similarity(get_wd_label(ht[2]).lower(), anchors_focuses, keywords_similarity_threshold):\n",
    "                            hypo = ht[0]\n",
    "                        else:\n",
    "                            hypothesises_all.append(ht[0])\n",
    "                            if not hypothesises: hypothesises.append([ht[0], hypo_sum])\n",
    "                            else: \n",
    "                                if ht[0] in [h[0] for h in hypothesises]:\n",
    "                                    for i, h in enumerate(hypothesises):\n",
    "                                        if ht[0] == h[0]: hypothesises[i] = [ht[0], hypo_sum+hypothesises[i][1]]\n",
    "                                else: hypothesises.append([ht[0], hypo_sum])\n",
    "                            \n",
    "                            hypo = ht[2]\n",
    "                    \n",
    "                    #elif not get_wd_label(ht[0]).lower() in anchors_focuses:\n",
    "                    #    if get_wd_label(ht[2]).lower() in anchors_focuses:\n",
    "                    #        hypo = ht[0]\n",
    "                    #    if not get_wd_label(ht[2]).lower() in anchors_focuses:\n",
    "                    #        hypothesises_all.append(ht[0])\n",
    "                    #        if not hypothesises: hypothesises.append([ht[0], hypo_sum])\n",
    "                    #        else: \n",
    "                    #            if ht[0] in [h[0] for h in hypothesises]:\n",
    "                    #                for i, h in enumerate(hypothesises):\n",
    "                    #                    if ht[0] == h[0]: hypothesises[i] = [ht[0], hypo_sum+hypothesises[i][1]]\n",
    "                    #            else: hypothesises.append([ht[0], hypo_sum])\n",
    "                    #        \n",
    "                    #        #if \"Q4985\" in ht: print(\"Q4985 ALONE hypo and sum:\", ht[0], hypo_sum)\n",
    "                    #        hypo = ht[2]\n",
    "\n",
    "                    else:\n",
    "                        #print(\"BREAK\", ht)\n",
    "                        break\n",
    "                    \n",
    "                    #print(\"hypothesises\",hypothesises)\n",
    "                    #if \"Q4985\" in ht:\n",
    "                    #    print(\"Q4985 hypo and sum:\", hypo, hypo_sum)\n",
    "                    \n",
    "                    hypothesises_all.append(hypo)\n",
    "                    if not hypothesises: hypothesises.append([hypo, hypo_sum])\n",
    "                    else: \n",
    "                        if hypo in [h[0] for h in hypothesises]:\n",
    "                            for i, h in enumerate(hypothesises):\n",
    "                                if hypo == h[0]: hypothesises[i] = [hypo, hypo_sum+hypothesises[i][1]]\n",
    "                        else: hypothesises.append([hypo, hypo_sum])\n",
    "    \n",
    "    #print(\"len(hypothesises_all)\",len(hypothesises_all))\n",
    "    for i_h, h in enumerate(hypothesises):\n",
    "        h_sum = hypothesises_all.count(h[0])\n",
    "        #print(\"h_sum\",h_sum)\n",
    "        #print(\"BEFORE: hypothesises[i_h][1]\",hypothesises[i_h][1])\n",
    "        hypothesises[i_h][1] = hypothesises[i_h][1]/h_sum\n",
    "        #print(\"AFTER: hypothesises[i_h][1]\",hypothesises[i_h][1])\n",
    "    \n",
    "    #print(\"hypothesises_all\",hypothesises_all)\n",
    "    \n",
    "    hypothesises = sorted(hypothesises, key=lambda x: x[-1], reverse=True)\n",
    "    \n",
    "    return hypothesises\n",
    "\n",
    "#if verbose: print(\"-> Computing hypothesises...\")\n",
    "#hypothesises = get_hypothesises(q_nlp, q_predicates, q_themes, paths_keywords, paths_nodes_filtered, threshold=0.5, max_reward=2.0) \n",
    "#if verbose: print(\"\\n\\n--> hypothesises:\",hypothesises)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_unique_hypo_paths(hypothesis, other_hypothesis, hypo_paths):\n",
    "    BANNED_IDS = [\"P31\"]\n",
    "    filtered_hypo_paths = []\n",
    "    \n",
    "    other_hypothesis = other_hypothesis[:]+[hypothesis]\n",
    "    \n",
    "    for hp in hypo_paths:\n",
    "        #print(\"hp\",hp)\n",
    "        path_is_used = False\n",
    "        len_hp = len(hp)\n",
    "        if len_hp >= 3:\n",
    "            #if \"P31\" in hp: print(\"hp\",hp)\n",
    "            for e in hp:\n",
    "                if e in other_hypothesis:\n",
    "                    e_index = hp.index(e)\n",
    "                    for step_index in range(1, len_hp, 2): #len_hp-e_index\n",
    "                        #print(\"step_index\",step_index)\n",
    "                        if e_index-step_index-2 >= 0:\n",
    "                            if hp[e_index-step_index] == hp[e_index-step_index-2] and hp[e_index-step_index] not in BANNED_IDS:\n",
    "                                #print(hp.index(e), hp)\n",
    "                                #print(\"IN\")\n",
    "                                part_1 = hp[:e_index-step_index-0]\n",
    "                                part_2 = hp[e_index-step_index-1:]\n",
    "\n",
    "                                #print(\"hp[:\",e_index-step_index-0,\"]\",part_1)\n",
    "                                #print(\"hp[\",e_index-step_index-1,\":]\",part_2)\n",
    "                                #hp[e_index:]\n",
    "\n",
    "                                sub_part_1 = None\n",
    "                                sub_part_2 = None\n",
    "\n",
    "                                if hypothesis in part_1:\n",
    "                                    sub_part_1 = get_unique_hypo_paths(hypothesis, other_hypothesis, [part_1,[]])\n",
    "                                if hypothesis in part_2:\n",
    "                                    sub_part_2 = get_unique_hypo_paths(hypothesis, other_hypothesis, [part_2,[]])\n",
    "\n",
    "                                if sub_part_1 != None:\n",
    "                                    if sub_part_1:\n",
    "                                        [filtered_hypo_paths.append(sp) for sp in sub_part_1]\n",
    "                                    else:\n",
    "                                        for e in hp:\n",
    "                                            if hp.count(e) > 1 and e not in BANNED_IDS: flag_too_much=True\n",
    "                                        if not flag_too_much: filtered_hypo_paths.append(part_1)\n",
    "\n",
    "                                if sub_part_2 != None:\n",
    "                                    if sub_part_2:\n",
    "                                        [filtered_hypo_paths.append(sp) for sp in sub_part_2]\n",
    "                                    else:\n",
    "                                        for e in hp:\n",
    "                                            if hp.count(e) > 1 and e not in BANNED_IDS: flag_too_much=True\n",
    "                                        if not flag_too_much: filtered_hypo_paths.append(part_2)\n",
    "                                        \n",
    "                                path_is_used=True\n",
    "\n",
    "                        else: break\n",
    "\n",
    "                    for step_index in range(1, len_hp, 2):\n",
    "                        #print(\"step_index\",step_index)\n",
    "                        if e_index+step_index+2 < len_hp:\n",
    "                            if hp[e_index+step_index] == hp[e_index+step_index+2] and hp[e_index+step_index] not in BANNED_IDS:\n",
    "                                #print(hp.index(e), hp)\n",
    "                                part_1 = hp[:e_index+step_index+2]\n",
    "                                part_2 = hp[e_index+step_index+1:]\n",
    "\n",
    "                                #print(\"hp[:\",e_index+step_index+2,\"]\",part_1)\n",
    "                                #print(\"hp[\",e_index+step_index+1,\":]\",part_2)\n",
    "\n",
    "                                #print(\"part_1\",part_1)\n",
    "                                #print(\"part_2\",part_2)\n",
    "\n",
    "                                sub_part_1 = None\n",
    "                                sub_part_2 = None\n",
    "\n",
    "                                if hypothesis in part_1:\n",
    "                                    sub_part_1 = get_unique_hypo_paths(hypothesis, other_hypothesis, [part_1,[]])\n",
    "                                if hypothesis in part_2:\n",
    "                                    sub_part_2 = get_unique_hypo_paths(hypothesis, other_hypothesis, [part_2,[]])\n",
    "\n",
    "                                if sub_part_1 != None:\n",
    "                                    if sub_part_1:\n",
    "                                        [filtered_hypo_paths.append(sp) for sp in sub_part_1]\n",
    "                                    else:\n",
    "                                        for e in hp:\n",
    "                                            if hp.count(e) > 1 and e not in BANNED_IDS: flag_too_much=True\n",
    "                                        if not flag_too_much: filtered_hypo_paths.append(part_1)\n",
    "\n",
    "                                if sub_part_2 != None:\n",
    "                                    if sub_part_2:\n",
    "                                        [filtered_hypo_paths.append(sp) for sp in sub_part_2]\n",
    "                                    else:\n",
    "                                        flag_too_much = False\n",
    "                                        for e in hp:\n",
    "                                            if hp.count(e) > 1 and e not in BANNED_IDS: flag_too_much=True\n",
    "                                        if not flag_too_much: filtered_hypo_paths.append(part_2)\n",
    "                                        \n",
    "                                path_is_used=True\n",
    "\n",
    "                        else: break\n",
    "            \n",
    "            if path_is_used == False:\n",
    "                flag_too_much = False\n",
    "                for e in hp:\n",
    "                    if hp.count(e) > 1 and e not in BANNED_IDS: flag_too_much=True\n",
    "                if not flag_too_much: filtered_hypo_paths.append(hp)\n",
    "        #else:\n",
    "        #    filtered_hypo_paths.append(hp) \n",
    "    \n",
    "    return filtered_hypo_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_hypothesises_worker(in_mp_queue, out_mp_queue):\n",
    "    golden_paths = []\n",
    "    sentinel = None\n",
    "    \n",
    "    for mpu in iter(in_mp_queue.get, sentinel):\n",
    "    \n",
    "    #for mp in mp_similarities_untagged:\n",
    "        #print(\"AFTER mpu\",mpu)\n",
    "        for i_e, e in enumerate(mpu[1]):\n",
    "            if i_e <= 1 or i_e >= len(mpu[1])-2:\n",
    "                continue\n",
    "            if not is_wd_entity(e):\n",
    "                continue\n",
    "\n",
    "            mp_e_statements = get_all_statements_of_entity(e)\n",
    "            \n",
    "            mp_predicate_tagging_index = mpu[1][i_e+1].find(\"-\")\n",
    "            if mp_predicate_tagging_index != -1:\n",
    "                mp_predicate = mpu[1][i_e+1][:mp_predicate_tagging_index]\n",
    "            else:\n",
    "                mp_predicate = mpu[1][i_e+1]\n",
    "            extended_paths = get_statements_by_id(mp_e_statements, e, mp_predicate, qualifier=False, statement_type=\"predicate\")\n",
    "            extended_paths_qualifier = get_statements_by_id(mp_e_statements, e, mp_predicate, qualifier=True, statement_type=\"qualifier_predicate\")\n",
    "            \n",
    "            ep_predicate_tagging_index_plus_1 = mpu[1][i_e+1].find(\"-\")\n",
    "            if ep_predicate_tagging_index_plus_1 != -1:\n",
    "                ep_predicate_plus_1 = mpu[1][i_e+1][:ep_predicate_tagging_index_plus_1]\n",
    "            else:\n",
    "                ep_predicate_plus_1 = mpu[1][i_e+1]\n",
    "\n",
    "            ep_predicate_tagging_index_minus_1 = mpu[1][i_e-1].find(\"-\")\n",
    "            if ep_predicate_tagging_index_minus_1 != -1:\n",
    "                ep_predicate_minus_1 = mpu[1][i_e-1][:ep_predicate_tagging_index_minus_1]\n",
    "            else:\n",
    "                ep_predicate_minus_1 = mpu[1][i_e-1]\n",
    "                    \n",
    "            for ep in extended_paths_qualifier:\n",
    "                if (ep['entity']['id'] == mpu[1][i_e] and \n",
    "                    ep['predicate']['id'] == ep_predicate_minus_1 and\n",
    "                    ep['object']['id'] == mpu[1][i_e-2] and\n",
    "                    ep['qualifiers']):\n",
    "                    for q in ep['qualifiers']:\n",
    "                        if(q['qualifier_predicate'][\"id\"] == ep_predicate_plus_1 and\n",
    "                          q['qualifier_object'][\"id\"] == mpu[1][i_e+2]):\n",
    "                            if mpu[1] not in golden_paths:\n",
    "                                golden_paths.append(mpu[1])\n",
    "\n",
    "                if (ep['entity']['id'] == mpu[1][i_e+2] and \n",
    "                    ep['predicate']['id'] == ep_predicate_plus_1 and\n",
    "                    ep['object']['id'] == mpu[1][i_e] and\n",
    "                    ep['qualifiers']):\n",
    "                    for q in ep['qualifiers']:\n",
    "                        if(q['qualifier_predicate'][\"id\"] == ep_predicate_minus_1 and\n",
    "                          q['qualifier_object'][\"id\"] == mpu[1][i_e-2]):\n",
    "                            if mpu[1] not in golden_paths:\n",
    "                                golden_paths.append(mpu[1])\n",
    "\n",
    "            for ep in extended_paths:\n",
    "                if (ep['entity']['id'] == mpu[1][i_e] and \n",
    "                    ep['predicate']['id'] == ep_predicate_minus_1 and\n",
    "                    ep['object']['id'] == mpu[1][i_e-2] and\n",
    "                    ep['qualifiers']):\n",
    "                    for q in ep['qualifiers']:\n",
    "                        if(q['qualifier_predicate'][\"id\"] == ep_predicate_plus_1 and\n",
    "                          q['qualifier_object'][\"id\"] == mpu[1][i_e+2]):\n",
    "                            if mpu[1] not in golden_paths:\n",
    "                                golden_paths.append(mpu[1])\n",
    "\n",
    "                if (ep['entity']['id'] == mpu[1][i_e+2] and \n",
    "                    ep['predicate']['id'] == ep_predicate_plus_1 and\n",
    "                    ep['object']['id'] == mpu[1][i_e] and\n",
    "                    ep['qualifiers']):\n",
    "                    for q in ep['qualifiers']:\n",
    "                        if(q['qualifier_predicate'][\"id\"] == ep_predicate_minus_1 and\n",
    "                          q['qualifier_object'][\"id\"] == mpu[1][i_e-2]):\n",
    "                            if mpu[1] not in golden_paths:\n",
    "                                golden_paths.append(mpu[1])  \n",
    "\n",
    "    \n",
    "    out_mp_queue.put(golden_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO / REDO\n",
    "# is_wd_entity not taking care of timestamps\n",
    "\n",
    "def list_by_n(l, i):\n",
    "    list_n = []\n",
    "    for j in range(0, len(l)+1, 1):\n",
    "        tmp = l[j-i:i+j-i]\n",
    "        if tmp:\n",
    "            list_n.append(tmp)\n",
    "    return list_n\n",
    "\n",
    "def match_hypothesises(graph, question, themes, predicates, hypothesises, paths, threshold=0.8, max_reward=2.0, winner_threshold_diff=7, time_sensitive=False, cores=mp.cpu_count()):\n",
    "    BANNED_IDS = [\"P31\"]\n",
    "    LOCATION_FILTER = [\"GPE\", \"FAC\", \"LOC\",\"PERSON\"]\n",
    "    \n",
    "    filtered_paths = []\n",
    "    \n",
    "    #print(\"hypothesises\",hypothesises)  \n",
    "    if time_sensitive:\n",
    "        hypothesises_time = [h for h in hypothesises if is_timestamp(h[0])]\n",
    "        if hypothesises_time:\n",
    "            hypothesises = hypothesises_time\n",
    "    \n",
    "    #if 'where' in [t.lower_ for t in question if t.tag_==\"WRB\"]:\n",
    "    #    print(\"in where condition\")\n",
    "    #    hypothesises_location = []\n",
    "    #    for h in hypothesises:\n",
    "    #        word_label = get_wd_label(h[0])\n",
    "    #        word_ents = get_kb_ents(word_label)\n",
    "    #        entities = [e.label_ for e in word_ents]\n",
    "    #        [hypothesises_location.append(e) for e in entities if e in LOCATION_FILTER]\n",
    "    #        print(h[0],word_label,word_ents,entities)\n",
    "    #    \n",
    "    #    if hypothesises_location:\n",
    "    #        hypothesises = hypothesises_location\n",
    "    \n",
    "    \n",
    "    #print(word_id,word_label,entity_type,)\n",
    "        \n",
    "    #print(\"hypothesises\",hypothesises)  \n",
    "    for h in hypothesises:\n",
    "        other_hypothesis = [e[0] for e in hypothesises if e != h]\n",
    "        hypo_paths = [p for p in paths if h[0] in p]\n",
    "\n",
    "        filtered_hypo_paths = []\n",
    "        [filtered_hypo_paths.append(p) for p in get_unique_hypo_paths(h[0], other_hypothesis, hypo_paths) if p not in filtered_hypo_paths]\n",
    "\n",
    "        for p in filtered_hypo_paths:\n",
    "            for e in p:\n",
    "                if p.count(e) > 1 and e not in BANNED_IDS: \n",
    "                    continue\n",
    "            if p[-1] == h[0]:\n",
    "                reversed_path = list(reversed(p))\n",
    "                if reversed_path not in filtered_paths:\n",
    "                    filtered_paths.append(reversed_path)\n",
    "            else:\n",
    "                if p not in filtered_paths:\n",
    "                    filtered_paths.append(p)\n",
    "                    \n",
    "    #print(\"filtered_paths\",filtered_paths)\n",
    "    \n",
    "    #print(\"1 hypothesises\",hypothesises)\n",
    "    \n",
    "    # check if first hypothesis is clear winner\n",
    "    winner_threshold_diff = 2*max_reward # this is coded in hard ! TBD\n",
    "    first_is_winner = False\n",
    "    if len(hypothesises) > 1:\n",
    "        hypo_diff = hypothesises[0][1]-hypothesises[1][1]\n",
    "        print(\"hypo_diff\",hypo_diff)\n",
    "        if hypo_diff > winner_threshold_diff:\n",
    "            first_is_winner = True \n",
    "    print(\"first_is_winner\",first_is_winner)\n",
    "    \n",
    "    if first_is_winner:\n",
    "        sorted_golden_paths = []\n",
    "\n",
    "        if not sorted_golden_paths:\n",
    "            for p in filtered_paths:\n",
    "                if len(p)>1 and p[0] == hypothesises[0][0]:\n",
    "                    if p not in sorted_golden_paths:\n",
    "                        sorted_golden_paths.append(p)\n",
    "                if len(p)>1 and p[-1] == hypothesises[0][0]:\n",
    "                    p = list(reversed(p))\n",
    "                    if p not in sorted_golden_paths:\n",
    "                        sorted_golden_paths.append(p)\n",
    "\n",
    "        if not sorted_golden_paths:\n",
    "            for p in filtered_paths:\n",
    "                if len(p)>1 and hypothesises[0][0] in p:\n",
    "                    if p not in sorted_golden_paths:\n",
    "                        sorted_golden_paths.append(p)\n",
    "    \n",
    "    else:    \n",
    "        meaningful_paths = []\n",
    "\n",
    "        w_positions, w_names = w_converter(question)\n",
    "\n",
    "        theme_ids = sum([t[1] for t in themes[0]],[])\n",
    "        for p in filtered_paths:\n",
    "            counter = 0\n",
    "\n",
    "            for ti in theme_ids:\n",
    "                if ti in p and p not in meaningful_paths:\n",
    "                    counter += 1\n",
    "            for pred in [p[1] for p in predicates]:\n",
    "                for e in pred:\n",
    "                    if e in p:\n",
    "                        counter += 1\n",
    "                    else:\n",
    "                        counter = 0\n",
    "\n",
    "            for i_wp, wp in enumerate(w_positions):\n",
    "                if w_names[i_wp][1] and wp<len(p):\n",
    "                    for es in get_entity_similarity(p[wp], w_names[i_wp][1], [], max_reward=max_reward):\n",
    "                        counter += es\n",
    "                        #print(\"p[wp], w_names[i_wp][1], es\",p[wp], w_names[i_wp][1], es)\n",
    "\n",
    "            for hypo in hypothesises:\n",
    "                if hypo[0] in p:\n",
    "                    counter += 1\n",
    "                if hypo[0] == p[0]:\n",
    "                    counter += 1\n",
    "                if hypo[0] == p[-1]:\n",
    "                    counter += 1\n",
    "\n",
    "            if counter > 0: meaningful_paths.append((counter, p))\n",
    "\n",
    "        meaningful_paths = sorted(meaningful_paths, key=lambda x: x[0], reverse=True)\n",
    "\n",
    "        #print(\"meaningful_paths\",meaningful_paths)\n",
    "        #print(\"len(meaningful_paths):\",len(meaningful_paths))\n",
    "        #print(\"\\n\")\n",
    "\n",
    "        #looped_paths = []\n",
    "        #for hypo in hypothesises:\n",
    "        #    for mp in meaningful_paths:\n",
    "        #        if mp[1][0] == hypo[0] or mp[1][-1] == hypo[0]:\n",
    "        #            if graph.has_node(mp[1][0]) and graph.has_node(mp[1][-1]):\n",
    "        #                path_tmp = list(nx.all_simple_paths(graph, mp[1][0],mp[1][-1]))\n",
    "        #                if len(path_tmp)>1:\n",
    "        #                    for p in path_tmp:\n",
    "        #                        if p not in [lp[1] for lp in looped_paths]:\n",
    "        #                            looped_paths.append((mp[0],p))\n",
    "        #            #else:\n",
    "        #            #    if not graph.has_node(mp[1][0]):\n",
    "        #            #        print(\"MISSING NODE:\", mp[1][0], get_wd_label(mp[1][0]))\n",
    "        #            #    if not graph.has_node(mp[1][-1]):\n",
    "        #            #        print(\"MISSING NODE:\", mp[1][-1], get_wd_label(mp[1][-1]))\n",
    "        #            \n",
    "        ##print(\"len(looped_paths)\", len(looped_paths))\n",
    "        #print(\"looped_paths\",looped_paths)\n",
    "\n",
    "        #looped_paths_untagged = []\n",
    "        #for lp in looped_paths:\n",
    "        #    row_tmp = []\n",
    "        #    for w in lp[1]:\n",
    "        #        if w.find(\"-\") > 0:\n",
    "        #            row_tmp.append(w[:w.find(\"-\")])\n",
    "        #        else:\n",
    "        #            row_tmp.append(w)\n",
    "        #    looped_paths_untagged.append((lp[0],row_tmp))\n",
    "        #\n",
    "    #\n",
    "        #    \n",
    "        #print(\"looped_paths_untagged\",looped_paths_untagged)\n",
    "\n",
    "        looped_paths_untagged = meaningful_paths\n",
    "\n",
    "        mp_similarities_untagged = []\n",
    "        #mp_similarities_tagged = []\n",
    "        mp_similarities_untagged_hypo = []\n",
    "        #mp_similarities_tagged_hypo = []\n",
    "\n",
    "        question_enhanced = []\n",
    "        for q in question:\n",
    "            if q.lemma_ == \"where\": question_enhanced.append(\"location\")\n",
    "            elif q.lemma_ == \"when\": question_enhanced.append(\"date\")\n",
    "            elif q.lemma_ == \"who\": question_enhanced.append(\"person\")    \n",
    "            elif q.lemma_ == \"why\": question_enhanced.append(\"cause\")\n",
    "            else: question_enhanced.append(q.text)\n",
    "\n",
    "        question_enhanced = nlp(\" \".join([q for q in question_enhanced]))\n",
    "\n",
    "        #print(\"question\",question)\n",
    "        #print(\"question_enhanced\",question_enhanced)\n",
    "\n",
    "        #print(\"[h[0] for h in hypothesises]\",[h[0] for h in hypothesises])\n",
    "\n",
    "        for i_lp, lp in enumerate(looped_paths_untagged):\n",
    "            #print(lp)\n",
    "            sentence = get_nlp(\" \".join([get_wd_label(w) for w in lp[1]]))\n",
    "            similarity = get_similarity_by_words(sentence, question)\n",
    "            similarity_enhanced = get_similarity_by_words(sentence, question_enhanced)\n",
    "            similarity_avg = (similarity+similarity_enhanced)/2*lp[0]\n",
    "            #print(sentence,question,question_enhanced)\n",
    "            #print(\"similarity\", similarity)\n",
    "            #print(\"question_enhanced\", similarity_enhanced)\n",
    "            #mp_similarities_untagged.append((similarity_enhanced,lp[1]))\n",
    "            #mp_similarities_tagged.append((similarity_enhanced,looped_paths[i_lp][1]))\n",
    "\n",
    "            if lp[1][0] in [h[0] for h in hypothesises]:\n",
    "                #print(\"lp[1][0]\",lp[1][0])\n",
    "                mp_similarities_untagged_hypo.append((similarity_avg, lp[1]))\n",
    "                #mp_similarities_tagged_hypo.append((similarity_avg, looped_paths[i_lp][1]))\n",
    "\n",
    "            mp_similarities_untagged.append((similarity_avg, lp[1]))\n",
    "            #mp_similarities_tagged.append((similarity_avg, looped_paths[i_lp][1]))\n",
    "\n",
    "        #print(\"mp_similarities_untagged\",len(mp_similarities_untagged))\n",
    "        #print(\"mp_similarities_untagged_hypo\",len(mp_similarities_untagged_hypo))\n",
    "        #print(\"mp_similarities_untagged\",mp_similarities_untagged)\n",
    "\n",
    "        #mp_similarities_tagged = sorted(mp_similarities_tagged, key=lambda x: x[0], reverse=True)\n",
    "        #mp_similarities_tagged = [mp for mp in mp_similarities_tagged if mpu[0] > threshold]\n",
    "\n",
    "        mp_similarities_untagged = sorted(mp_similarities_untagged, key=lambda x: x[0], reverse=True)\n",
    "        mp_similarities_untagged = [mpu for mpu in mp_similarities_untagged if mpu[0] > threshold]\n",
    "\n",
    "        #print(\"mp_similarities_untagged\",len(mp_similarities_untagged))\n",
    "        #print(\"mp_similarities_untagged\",mp_similarities_untagged)\n",
    "\n",
    "        [mp_similarities_untagged.append(suh) for suh in mp_similarities_untagged_hypo if not suh in mp_similarities_untagged]\n",
    "        #[mp_similarities_tagged.append(sth) for sth in mp_similarities_tagged_hypo if not sth in mp_similarities_tagged]\n",
    "\n",
    "        #print(\"mp_similarities_untagged\",len(mp_similarities_untagged))\n",
    "        #print(\"mp_similarities_tagged\",len(mp_similarities_tagged))\n",
    "\n",
    "        #WH_FILTER = [\"WDT\", \"WP\", \"WP$\", \"WRB\"]\n",
    "        #wh_position = [w.i for w in question if w.tag_ in WH_FILTER][0]\n",
    "        #question_list = [w.lower_ for w in question if not w.is_punct]\n",
    "        #question_list_filtered = [w.lower_ for w in question if not w.is_punct and w.tag_ not in WH_FILTER]\n",
    "\n",
    "\n",
    "\n",
    "        if cores <= 0: cores = 1\n",
    "        sentinel = None\n",
    "\n",
    "        out_mp_queue = mp.Queue()\n",
    "        in_mp_queue = mp.Queue()\n",
    "\n",
    "        for mpu in mp_similarities_untagged:\n",
    "            #print(\"BEFORE mpu\",mpu)\n",
    "            in_mp_queue.put(mpu)\n",
    "\n",
    "        procs = [mp.Process(target = match_hypothesises_worker, args = (in_mp_queue, out_mp_queue)) for i in range(cores)]\n",
    "\n",
    "        golden_paths = []\n",
    "\n",
    "        for proc in procs:\n",
    "            proc.daemon = True\n",
    "            proc.start()\n",
    "        for proc in procs:    \n",
    "            in_mp_queue.put(sentinel)\n",
    "        for proc in procs:\n",
    "            golden_paths.extend(out_mp_queue.get())\n",
    "        for proc in procs:\n",
    "            proc.join()\n",
    "\n",
    "        golden_paths = [gp for gp in golden_paths if gp]\n",
    "        #print(\"golden_paths\",golden_paths)\n",
    "\n",
    "        #print(\"len(golden_paths)\",len(golden_paths))\n",
    "        sorted_golden_paths = []\n",
    "        for gp in golden_paths:\n",
    "            tmp_gp = []\n",
    "            #if gp[0] in [h[0] for h in hypothesises]:\n",
    "            for e in gp:\n",
    "                if is_wd_entity(e):\n",
    "                    tmp_gp.append(get_wd_label(e))\n",
    "                else:\n",
    "                    tmp_gp.append(get_wd_label(e[:e.find(\"-\")]))\n",
    "            nlp_gp = get_nlp(\" \".join(tmp_gp))\n",
    "            sorted_golden_paths.append((get_similarity_by_words(question,nlp_gp), gp))\n",
    "    #\n",
    "        sorted_golden_paths = sorted(sorted_golden_paths, key=lambda x: x[0], reverse=True)\n",
    "\n",
    "        #print(\"sorted_golden_paths\",sorted_golden_paths)\n",
    "        #print(\"len(sorted_golden_paths) BEFORE\",len(sorted_golden_paths))\n",
    "\n",
    "        sorted_golden_paths = [sgp[1] for sgp in sorted_golden_paths]\n",
    "\n",
    "        if not sorted_golden_paths: \n",
    "            for hypo in hypothesises:\n",
    "                #print(\"hypo\",hypo[0])\n",
    "                for lp in [lp[1] for lp in meaningful_paths]:\n",
    "                    if len(lp)>1 and lp[0] == hypo[0]:\n",
    "                        if lp not in sorted_golden_paths:\n",
    "                            sorted_golden_paths.append(lp)\n",
    "                    if len(lp)>1 and lp[-1] == hypo[0]:\n",
    "                        lp = list(reversed(lp))\n",
    "                        if lp not in sorted_golden_paths:\n",
    "                            sorted_golden_paths.append(lp)\n",
    "                if len(sorted_golden_paths) >= 1: break\n",
    "\n",
    "\n",
    "        #print(\"len(sorted_golden_paths) AFTER\",len(sorted_golden_paths))\n",
    "\n",
    "        if not sorted_golden_paths:\n",
    "            for hypo in hypothesises:\n",
    "                for p in filtered_paths:\n",
    "                    if len(p)>1 and p[0] == hypo[0]:\n",
    "                        #print(p)\n",
    "                        if p not in sorted_golden_paths:\n",
    "                            sorted_golden_paths.append(p)\n",
    "                    if len(p)>1 and p[-1] == hypo[0]:\n",
    "                        p = list(reversed(p))\n",
    "                        if p not in sorted_golden_paths:\n",
    "                            sorted_golden_paths.append(p)\n",
    "                if len(sorted_golden_paths) >= 1: break\n",
    "\n",
    "        #print(\"len(sorted_golden_paths) AFTER AFTER\",len(sorted_golden_paths))\n",
    "\n",
    "        if not sorted_golden_paths:\n",
    "            for hypo in hypothesises:\n",
    "                for p in filtered_paths:\n",
    "                    if len(p)>1 and hypo[0] in p:\n",
    "                        if p not in sorted_golden_paths:\n",
    "                            sorted_golden_paths.append(p)\n",
    "                if len(sorted_golden_paths) >= 1: break\n",
    "\n",
    "        #print(\"len(sorted_golden_paths) AFTER AFTER AFTER\",len(sorted_golden_paths))\n",
    "    \n",
    "    golden_paths_filtered = []\n",
    "    for gp in sorted_golden_paths:\n",
    "        tmp_path = []\n",
    "        for i_e, e in enumerate(gp):\n",
    "            if i_e < len(gp)-2 and not is_wd_entity(e):\n",
    "                if e == gp[i_e+2]:\n",
    "                    golden_paths_filtered.append(gp[:gp.index(e)+2])\n",
    "                    break\n",
    "                else:\n",
    "                    tmp_path.append(e)\n",
    "            else:\n",
    "                tmp_path.append(e)\n",
    "        \n",
    "        if tmp_path:\n",
    "            for i_e, e in enumerate(tmp_path):\n",
    "                if is_wd_entity(e):\n",
    "                    if tmp_path.count(e) > 1:\n",
    "                        pass\n",
    "                    else:\n",
    "                        if tmp_path not in golden_paths_filtered:\n",
    "                            golden_paths_filtered.append(tmp_path)\n",
    "                            \n",
    "    #print(\"len(golden_paths_filtered)\",len(golden_paths_filtered))\n",
    "    \n",
    "    #print(\"golden_paths_filtered\",golden_paths_filtered)\n",
    "    \n",
    "    golden_unique_paths = golden_paths_filtered.copy()\n",
    "    for i_sgp, sgp in enumerate(golden_paths_filtered):\n",
    "        for sgp_2 in golden_paths_filtered:\n",
    "            if (is_sublist(sgp, sgp_2) and sgp!=sgp_2):\n",
    "                golden_unique_paths[i_sgp] = []\n",
    "                break\n",
    "    \n",
    "    golden_unique_paths = [gup for gup in golden_unique_paths if gup]\n",
    "    hypothesises_names = [h[0] for h in hypothesises]\n",
    "    \n",
    "    #print(\"golden_unique_paths\",golden_unique_paths)\n",
    "    #print(\"before hypothesises_names\",hypothesises_names)\n",
    "    #print(\"golden_unique_paths[0][0]\",golden_unique_paths[0][0])\n",
    "    #print(\"hypothesises_names\",hypothesises_names)\n",
    "    \n",
    "    #if is_valide_wd_id(hypothesises_names[0]):\n",
    "    \n",
    "    if not first_is_winner:\n",
    "        if golden_unique_paths and hypothesises_names:\n",
    "            if golden_unique_paths[0] and hypothesises_names[0]:\n",
    "                if golden_unique_paths[0][0]:\n",
    "                    if (not is_wd_entity(hypothesises_names[0])\n",
    "                        and is_wd_entity(golden_unique_paths[0][0]) \n",
    "                        or hypothesises_names[0] != golden_unique_paths[0][0]):\n",
    "                        if golden_unique_paths[0][0] in hypothesises_names:\n",
    "                            #hypothesises_names.pop(hypothesises_names.index(golden_unique_paths[0][0]))\n",
    "                            hypothesises_names.insert(0,golden_unique_paths[0][0])\n",
    "        \n",
    "        golden_unique_paths = [gup for gup in golden_unique_paths if gup[0] == hypothesises_names[0]]\n",
    "    \n",
    "    #print(\"after hypothesises_names\",hypothesises_names)\n",
    "            \n",
    "    \n",
    "            \n",
    "    #elif hypothesises_names[0] != golden_unique_paths[0][0]\n",
    "    \n",
    "    golden_unique_paths = [hypothesises_names]+golden_unique_paths\n",
    "    \n",
    "    return golden_unique_paths\n",
    "\n",
    "\n",
    "\n",
    "#if verbose: print(\"-> Matching hypothesises...\")\n",
    "#start_time = time.time()\n",
    "#golden_paths = match_hypothesises(graph, q_nlp, q_themes, q_predicates, hypothesises, paths_nodes_filtered, threshold=0.8, max_reward=2.0)\n",
    "#end_time = time.time()\n",
    "#print(\"Golden paths ->\\tRunning time is {}s\".format(round(end_time-start_time,2)))\n",
    "#print(golden_paths)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## questions = (\"what was the cause of death of yves klein\",\n",
    "#               \"Who is the wife of Barack Obama?\",\n",
    "#               \"Who is the president of the United States?\",\n",
    "#               \"When was produced the first Matrix movie?\",\n",
    "#               \"Who made the soundtrack of the The Last Unicorn movie?\",\n",
    "#               \"Who is the author of Le Petit Prince?\",\n",
    "#               \"Which actor voiced the Unicorn in The Last Unicorn?\",\n",
    "#               \"how is called the rabbit in Alice in Wonderland?\"\n",
    "#              )\n",
    "\n",
    "#def print_running_time(start_time, end_time=time.time()):\n",
    "#    print(\"->\\tRunning time is {}s\".format(round(end_time-start_time,2)))\n",
    "\n",
    "def answer_question(question, verbose=False, aggressive=True, looped=False, deep_k=2, deep_k_step=1, deep_k_max=20,graph_size_min=100, graph_size_max=350, timer=False, g_paths=True, show_graph=False, cores=mp.cpu_count()):\n",
    "    if verbose: start_time = time.time()\n",
    "    if timer: timer_time = time.time()\n",
    "    if verbose: print(\"Auto correcting question:\",question)\n",
    "    q_nlp = get_nlp(question, autocorrect=True)\n",
    "    if verbose: print(\"-> q_nlp:\",q_nlp)\n",
    "    time_sensitive = False\n",
    "    if 'when' in [t.lower_ for t in q_nlp if t.tag_==\"WRB\"]: time_sensitive = True\n",
    "    q_themes = get_themes(q_nlp, question, top_k=2, online=True)\n",
    "    q_theme_names = [q[0].text for q in q_themes[0]]\n",
    "    if verbose: print(\"-> q_themes:\",q_themes)\n",
    "    q_themes_enhanced = get_enhanced_themes(q_themes, top_k=1, aggressive=True)\n",
    "    q_theme_enhanced_names = [q[0] for q in q_themes_enhanced]\n",
    "    if verbose: print(\"-> q_themes_enhanced:\",q_themes_enhanced)\n",
    "    if verbose: print(\"--> Calculating predicates... (could be long.. depends on uncached unpure predicates)\")\n",
    "    q_predicates_db = get_predicates(q_nlp, q_themes, top_k=0)\n",
    "    q_predicates_online = get_predicates_online(q_nlp, top_k=2, aggressive=aggressive)\n",
    "    q_predicates = []\n",
    "    q_predicates_db_ids = [p[1] for p in q_predicates_db]\n",
    "    q_predicates_db_names = [p[0] for p in q_predicates_db]\n",
    "    q_predicates_online_ids = [p[1] for p in q_predicates_online]\n",
    "    q_predicates_online_names = [p[0] for p in q_predicates_online]\n",
    "    for i_n,n in enumerate(q_predicates_db_names):\n",
    "        pn_online_text = [n.text for n in q_predicates_online_names]\n",
    "        tmp_ids = q_predicates_db_ids[i_n]\n",
    "        if n.text in pn_online_text:\n",
    "            for p_o in q_predicates_online_ids[pn_online_text.index(n.text)]:\n",
    "                if p_o not in tmp_ids:\n",
    "                    tmp_ids.append(p_o)\n",
    "        q_predicates.append((n,tmp_ids))\n",
    "    \n",
    "    for i_n_o,n_o in enumerate(q_predicates_online_names):\n",
    "        n_db_text = [n.text for n in q_predicates_db_names]\n",
    "        if n_o.text not in n_db_text:\n",
    "            q_predicates.append((n_o, q_predicates_online_ids[i_n_o]))\n",
    "    \n",
    "    if verbose: print(\"-> q_predicates:\",q_predicates)\n",
    "    if timer: \n",
    "        print(\"->\\tRunning time is {}s\".format(round(time.time()-timer_time,2)))\n",
    "        timer_time = time.time()\n",
    "    q_focused_parts = get_focused_parts(q_nlp, q_themes, top_k=2)\n",
    "    if verbose: print(\"-> q_focused_parts:\",q_focused_parts)\n",
    "    if verbose: print(\"-> Building the graph with k_deep\",str(deep_k),\"... (could be long)\")\n",
    "    \n",
    "    # Auto-scaling the graph size with deepness\n",
    "    if deep_k > deep_k_max and looped:\n",
    "        deep_k_max+=int(deep_k_max/2)\n",
    "    \n",
    "    previous_graph_size = 0\n",
    "    previous_graph_len = 0\n",
    "    if deep_k<1:\n",
    "        deep_k = 1\n",
    "        graph, predicates_dict = build_graph(q_nlp, q_themes, q_themes_enhanced, q_predicates, deep_k=deep_k, time_sensitive=time_sensitive, cores=cores)\n",
    "        if verbose: print(\"--> \",len(graph), \"nodes and\", graph.size(), \"edges\")\n",
    "        if verbose: print(\"--> Removing meaningless subgraphs\")\n",
    "        graph = filter_graph_by_names(graph, q_theme_names+q_theme_enhanced_names, entities=True, predicates=False)\n",
    "        if verbose: print(\"--> New graph of:\",len(graph), \"nodes and\", graph.size(), \"edges\")\n",
    "        if timer: \n",
    "            print(\"->\\tRunning time is {}s\".format(round(time.time()-timer_time,2)))\n",
    "            timer_time = time.time()\n",
    "    else:\n",
    "        if deep_k > deep_k_max:\n",
    "            graph, predicates_dict = build_graph(q_nlp, q_themes, q_themes_enhanced, q_predicates, deep_k=deep_k, time_sensitive=time_sensitive, cores=cores)\n",
    "            if verbose: print(\"---> deep_k > deep_k_max, running graph as last trial with deep_k:\",deep_k)\n",
    "            if timer: \n",
    "                print(\"->\\tRunning time is {}s\".format(round(time.time()-timer_time,2)))\n",
    "                timer_time = time.time()\n",
    "            if verbose: print(\"--> Removing meaningless subgraphs\")\n",
    "            graph = filter_graph_by_names(graph, q_theme_names+q_theme_enhanced_names, entities=True, predicates=False)\n",
    "            if verbose: print(\"--> New graph of:\",len(graph), \"nodes and\", graph.size(), \"edges\")\n",
    "                \n",
    "        else:\n",
    "            for k in range(deep_k, deep_k_max, deep_k_step):\n",
    "                graph, predicates_dict = build_graph(q_nlp, q_themes, q_themes_enhanced, q_predicates, deep_k=deep_k, time_sensitive=time_sensitive, cores=cores)\n",
    "                if timer: \n",
    "                    print(\"->\\tRunning time is {}s\".format(round(time.time()-timer_time,2)))\n",
    "                    timer_time = time.time()\n",
    "\n",
    "                if verbose: print(\"--> \",len(graph), \"nodes and\", graph.size(), \"edges\")\n",
    "                if verbose: print(\"--> Removing meaningless subgraphs\")\n",
    "                graph = filter_graph_by_names(graph, q_theme_names+q_theme_enhanced_names, entities=True, predicates=False)\n",
    "                if verbose: print(\"--> New graph of:\",len(graph), \"nodes and\", graph.size(), \"edges\")\n",
    "\n",
    "                if previous_graph_size == graph.size() and previous_graph_len == len(graph):\n",
    "                    if verbose: print(\"---> Loop detected, returning the graph in the current state\")\n",
    "                    break\n",
    "                else:\n",
    "                    previous_graph_size = graph.size()\n",
    "                    previous_graph_len = len(graph)\n",
    "\n",
    "                if (graph.size() > graph_size_max or len(graph) > graph_size_max) and deep_k > 1:\n",
    "                    deep_k -= deep_k_step\n",
    "                    if verbose: print(\"---> Rebuilding the graph with k_deep\",str(deep_k), \"... Previously:\",len(graph), \"nodes or\", graph.size(), \"edges was above the limit of\",graph_size_max)\n",
    "                    graph, predicates_dict = build_graph(q_nlp, q_themes, q_themes_enhanced, q_predicates, deep_k=deep_k, time_sensitive=time_sensitive, cores=cores)\n",
    "                    break\n",
    "                elif graph.size() <= graph_size_min or len(graph) <= graph_size_min:\n",
    "                    if graph.size() < graph_size_min/3 or len(graph) < graph_size_min/3:\n",
    "                        deep_k += deep_k_step*3\n",
    "                    elif graph.size() < graph_size_min/4*3 or len(graph) < graph_size_min/4*3:\n",
    "                        deep_k += deep_k_step*2\n",
    "                    else:\n",
    "                        deep_k += deep_k_step\n",
    "                    if verbose: print(\"---> Rebuilding the graph with k_deep\",str(deep_k), \"... Previously:\",len(graph), \"nodes or\", graph.size(), \"edges was below the limit of\",graph_size_min)\n",
    "                else: break\n",
    "\n",
    "    if show_graph:\n",
    "        if verbose: print(\"---> Ploting the graph\")\n",
    "        plot_graph(graph, \"file_name_graph\", \"Graph_title\")\n",
    "    \n",
    "    if verbose: print(\"-> predicates_dict:\",predicates_dict)\n",
    "    paths_keywords = find_paths_keywords(graph, q_nlp, q_themes, q_themes_enhanced, q_predicates, q_focused_parts)\n",
    "    if verbose: print(\"-> paths_keywords:\",paths_keywords)\n",
    "    if timer: timer_time = time.time()\n",
    "    \n",
    "    if verbose: print(\"-> Computing possible paths... (could be long)\")\n",
    "    path_nodes = find_path_nodes_from_graph(q_nlp, graph, predicates_dict, paths_keywords, threshold=0.8,special_pred_theshold=0.7, thres_inter=0.1, top_performance=len(graph), min_paths=100, cores=cores)\n",
    "    if verbose: print(\"--> len(path_nodes):\",len(path_nodes))\n",
    "    if timer: \n",
    "        print(\"->\\tRunning time is {}s\".format(round(time.time()-timer_time,2)))\n",
    "        timer_time = time.time()\n",
    "    \n",
    "    if len(path_nodes) < 20000:\n",
    "        if verbose: print(\"-> Filtering paths... (could be long)\")\n",
    "        paths_nodes_filtered = paths_nodes_filter(path_nodes, graph)\n",
    "        if verbose: print(\"--> len(paths_nodes_filtered):\",len(paths_nodes_filtered))\n",
    "        if timer: \n",
    "            print(\"->\\tRunning time is {}s\".format(round(time.time()-timer_time,2)))\n",
    "            timer_time = time.time()\n",
    "    else: \n",
    "        if verbose: print(\"--> Skipping paths filtering... (too much paths)\")\n",
    "        paths_nodes_filtered = paths_nodes_filter(path_nodes, graph, with_sublists=False)\n",
    "\n",
    "    if verbose: print(\"-> Computing hypothesises...\")\n",
    "    hypothesises = get_hypothesises(q_nlp, predicates_dict, q_predicates, q_themes, paths_keywords, paths_nodes_filtered, threshold=0.5, max_reward=2.0) \n",
    "    if verbose: print(\"--> hypothesises:\",hypothesises)\n",
    "    if timer: \n",
    "        print(\"->\\tRunning time is {}s\".format(round(time.time()-timer_time,2)))\n",
    "        timer_time = time.time()\n",
    "    if g_paths:\n",
    "        if hypothesises:\n",
    "            if verbose: print(\"-> Computing golden paths...\")\n",
    "            golden_paths = match_hypothesises(graph, q_nlp, q_themes, q_predicates, hypothesises, paths_nodes_filtered, threshold=0.8, max_reward=2.0,winner_threshold_diff=4.0, time_sensitive=time_sensitive)\n",
    "            if verbose: print(\"--> len(golden_paths):\",len(golden_paths)-1)\n",
    "            if timer: \n",
    "                print(\"->\\tRunning time is {}s\".format(round(time.time()-timer_time,2)))\n",
    "                timer_time = time.time()\n",
    "        else:\n",
    "            if not looped:\n",
    "                if verbose: print(\"-> Looping on aggressive mode...\")\n",
    "                golden_paths = answer_question(question, verbose=verbose, aggressive=True, looped=True, deep_k=deep_k, deep_k_step=deep_k_step, deep_k_max=deep_k_max,graph_size_min=graph_size_min, graph_size_max=graph_size_max, timer=timer, g_paths=g_paths, show_graph=show_graph, cores=cores)\n",
    "                 \n",
    "            else: \n",
    "                if verbose: print(\"--> End of loop\")\n",
    "                golden_paths=[]\n",
    "    \n",
    "    save_cache_data()\n",
    "    \n",
    "    if g_paths:\n",
    "        if golden_paths:\n",
    "            cleared_golden_paths = [golden_paths[0].copy()]\n",
    "            for p in golden_paths[1:]:\n",
    "                tmp_labeling = []\n",
    "                for e in p:\n",
    "                    tmp_labeling.append(get_wd_label(e))\n",
    "                if tmp_labeling not in cleared_golden_paths:\n",
    "                    cleared_golden_paths.append(tmp_labeling)\n",
    "\n",
    "            if verbose: print(\"--> len(cleared_golden_paths):\",len(cleared_golden_paths)-1)\n",
    "            if len(cleared_golden_paths) > 1:\n",
    "                if verbose: print(\"---> First path:\",cleared_golden_paths[1])\n",
    "            if timer: timer_time = time.time()\n",
    "            \n",
    "    if verbose: print(\"->\\tTotal Running time is {}s\".format(round(time.time()-start_time,2)))\n",
    "    \n",
    "    if g_paths:\n",
    "        if golden_paths:\n",
    "            return cleared_golden_paths\n",
    "        else: return False\n",
    "    else:\n",
    "        if hypothesises:\n",
    "            return [[a[0] for a in hypothesises]] + [[hypothesises[0][0]]]\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "\n",
    "#answer = answer_question(\"what film is by the writer phil hay?\", verbose=True, timer=True) #444.36s\n",
    "#answer = answer_question(\"When was produced the first Matrix movie?\", verbose=True, timer=True) #70.67s\n",
    "#answer = answer_question(\"Which actor voiced the Unicorn in The Last Unicorn?\", verbose=True, timer=True, g_paths=True, show_graph=True) #works 312.12s\n",
    "#answer = answer_question(\"Who voiced the Unicorn in The Last Unicorn?\", verbose=True, timer=True) #works 323.52s\n",
    "#answer = answer_question(\"How many actors voiced the Unicorn in The Last Unicorn?\", verbose=True, timer=True) #592.22s \n",
    "#answer = answer_question(\"Which is the nation of Martha Mattox\", verbose=True, timer=True) #97.89s\n",
    "#answer = answer_question(\"Who made the soundtrack of the The Last Unicorn movie?\", verbose=True, timer=True)\n",
    "#answer = answer_question(\"Who is the author of Le Petit Prince?\", verbose=True, timer=True)\n",
    "\n",
    "#answer = answer_question(\"When was produced the first Matrix movie?\", verbose=True, timer=True)\n",
    "#answer = answer_question(\"Who is the president of the United States?\", verbose=True, timer=True) #node Q76 not in graph 324.88s\n",
    "#answer = answer_question(\"Who is the wife of Barack Obama?\", verbose=True, timer=True) #works 275.94s\n",
    "#answer = answer_question(\"what was the cause of death of yves klein\", verbose=True, timer=True) #309.06s\n",
    "#answer = answer_question(\"what city was alex golfis born in\", verbose=True, timer=True)\n",
    "#answer = answer_question(\"which stadium do the wests tigers play in\", verbose=True, timer=True) #462.47s\n",
    "#answer = answer_question(\"lol\", verbose=True, timer=True)\n",
    "#answer = answer_question(\"what's akbar tandjung's ethnicity\", verbose=True, timer=True)\n",
    "\n",
    "#answer = answer_question(\"Which equestrian was is in dublin ?\", verbose=True, timer=True)\n",
    "#answer = answer_question(\"how does engelbert zaschka identify\t\", verbose=True, timer=True)\n",
    "#answer = answer_question(\"Who influenced michael mcdowell?\", verbose=True, timer=True)\n",
    "#answer = answer_question(\"what does  2674 pandarus orbit\", verbose=True, timer=True)\n",
    "#answer = answer_question(\"what production company was involved in smokin' aces 2: assasins' ball\", verbose=True, timer=True)\n",
    "#answer = answer_question(\"who's a kung fu star from hong kong\", verbose=True, timer=True)\n",
    "\n",
    "#answer = answer_question(\"Which genre of album is harder.....faster?\", verbose=True, timer=True)\n",
    "#answer = answer_question(\"Which equestrian was born in dublin?\", verbose=True, timer=True)\n",
    "#answer = answer_question(\"Who is the author that wrote the book Moby Dick\", verbose=True, timer=True, show_graph=True) #314.04s works\n",
    "#answer = answer_question(\"Name a person who died from bleeding.\", verbose=True, timer=True) # 117.35s\n",
    "#answer = answer_question(\"What is the name of the person who created Saved by the Bell?\", verbose=True, timer=True)\n",
    "#answer = answer_question(\"of what nationality is ken mcgoogan\", verbose=True, timer=True) #works 51.39s\n",
    "#\n",
    "#answer = answer_question(\"What is a tv action show?\", verbose=True, timer=True, g_paths=False)\n",
    "##answer = answer_question(\"who published neo contra\", verbose=True, timer=True, g_paths=False)\n",
    "#answer = answer_question(\"When was the publication date of the movie Grease?\", verbose=True, timer=True)\n",
    "#answer = answer_question(\"When did the movie Grease come out?\", verbose=True, timer=True, show_graph=True)\n",
    "#answer = answer_question(\"whats the name of the organization that was founded by  frei otto\", verbose=True, timer=True, g_paths=False)\n",
    "#answer = answer_question(\"where was johannes messenius born\", verbose=True, timer=True, g_paths=False)\n",
    "#answer = answer_question(\"What is a type of gameplay available to gamers playing custom robo v2\", verbose=True, timer=True, g_paths=False)\n",
    "#answer = answer_question(\"Which genre of album is Harder ... Faster?\", verbose=True, timer=True, show_graph=True)\n",
    "#answer = answer_question(\"Who is the author that wrote the book Moby Dick\", verbose=True, timer=True, show_graph=True)\n",
    "#answer = answer_question(\"how does engelbert zaschka identify\", verbose=True, timer=True, show_graph=True)\n",
    "\n",
    "#answer = answer_question(\"where was shigeyasu suzuki's place of birth\", verbose=True, timer=True, show_graph=True)\n",
    "#answer = answer_question(\"What is the name of the writer of The Secret Garden?\", verbose=True, timer=True, show_graph=True)\n",
    "#answer = answer_question(\"Who was an influential figure for miško Šuvaković\", verbose=True, timer=True, show_graph=True)\n",
    "#\n",
    "#answer = answer_question(\"When did the movie Grease come out?\", verbose=True, timer=True, show_graph=True)\n",
    "\n",
    "#answer = answer_question(\"of what nationality is ken mcgoogan\", verbose=True, timer=True) #works 51.39s\n",
    "#answer = answer_question(\"Where did roger marquis die\", verbose=True, timer=True, show_graph=True) # works 64.56s\n",
    "\n",
    "#answer = answer_question(\"How many people were in The Beatles?\", verbose=True, timer=True, show_graph=True)\n",
    "#if answer:\n",
    "#    print(\"Answer:\",get_wd_label(answer[0][0]), \"(\"+str(answer[0][0])+\")\")\n",
    "#    #print(\"Paths:\",[[get_wd_label(e) for e in row] for row in answer[1:]])\n",
    "\n",
    "\n",
    "#graph = answer_question(\"When did the movie Grease come out?\", verbose=True, timer=True, g_paths=False, get_graph=True)\n",
    "\n",
    "#graph = answer_question(\"When was the publication date of the movie Grease?\", verbose=True, timer=True, g_paths=False)\n",
    "\n",
    "#graph = answer_question(\"Which actor voiced the Unicorn in The Last Unicorn?\", verbose=True, timer=True, g_paths=False, get_graph=True)\n",
    "\n",
    "#graph = answer_question(\"whats the name of the organization that was founded by  frei otto\", verbose=True, timer=True, g_paths=False, get_graph=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#questions = [\n",
    "#    \"When was the publication date of the movie Grease?\",\n",
    "#    \"When was produced the first Matrix movie?\",\n",
    "#    \n",
    "#    \"Which is the nation of Martha Mattox\",\n",
    "#    \"Where did roger marquis die\",\n",
    "#    \"Who is the author that wrote the book Moby Dick\",\n",
    "#    \"Who is the wife of Barack Obama?\",\n",
    "#    \"of what nationality is ken mcgoogan\",\n",
    "#    \n",
    "#    \"What is the name of the writer of The Secret Garden?\",\n",
    "#    \"whats the name of the organization that was founded by  frei otto\",\n",
    "#    \n",
    "#    \"Which genre of album is harder.....faster?\",\n",
    "#    \"Which genre of album is Harder ... Faster?\",\n",
    "#    \"Which actor voiced the Unicorn in The Last Unicorn?\",\n",
    "#    \"Who voiced the Unicorn in The Last Unicorn?\",\n",
    "#    \n",
    "#    \"When did the movie Grease come out?\",\n",
    "#    \n",
    "#    \"which stadium do the wests tigers play in\",\n",
    "#    \n",
    "#    \"Which equestrian was is in dublin ?\",\n",
    "#    \"how does engelbert zaschka identify\t\",\n",
    "#    \"Who influenced michael mcdowell?\",\n",
    "#    \"what does  2674 pandarus orbit\"\n",
    "#            ]\n",
    "#\n",
    "#for i_q, question in enumerate(questions):\n",
    "#    if i_q >= 0:\n",
    "#        answer = answer_question(question, verbose=True, timer=True, show_graph=True)\n",
    "#        if answer:\n",
    "#            print(\"Answer:\",get_wd_label(answer[0][0]), \"(\"+str(answer[0][0])+\")\\n\")\n",
    "#    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#to_translate = ['Q13133', 'P26', 'Q76', 'P31', 'Q5', 'P31', 'Q24039104', 'P21', 'Q6581072', 'P1552', 'Q188830', 'P26', 'Q18531596']\n",
    "#to_translate = ['Q202725', 'P725', 'Q176198', 'P453', 'Q30060419', 'P31', 'Q30167264', 'P1889', 'Q7246', 'P138', 'Q18356448']\n",
    "#\n",
    "#masked = []\n",
    "#for tt in to_translate:\n",
    "#    masked.append(get_wd_label(tt))\n",
    "#    masked.append(\"[MASK]\")\n",
    "#print(\"marked\",masked)\n",
    "#\n",
    "#print(\"->\",[get_wd_label(e) for e in to_translate])\n",
    "#print(\"-->\",\" \".join([get_wd_label(e) for e in to_translate]))\n",
    "#\n",
    "#print(\"-->\",\" [MASK] \".join([get_wd_label(e) for e in to_translate]))\n",
    "#print(\"-->\",\"[\" +\" , [MASK] , \".join([get_wd_label(e) for e in to_translate])+\"]\")\n",
    "#\n",
    "#FILTER_ELEMENTS = ['P31']\n",
    "#filtered_by_elements = [\"[MASK]\" if x in FILTER_ELEMENTS else get_wd_label(x) for x in to_translate]\n",
    "#\n",
    "#print(\"-->\",\" [MASK] \".join([e for e in filtered_by_elements]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#graph_2.has_node(\"Q13133\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get_similarity_by_words(get_nlp(\"mia farrow\"), get_nlp(\"farrow mia\")) #1.000000077374981\n",
    "#get_similarity_by_words(get_nlp(\"actor voiced\"), get_nlp(\"voice actor\")) #0.8541489425987572 \n",
    "#get_similarity_by_words(get_nlp(\"actor voiced the unicorn in the last unicorn\"), \n",
    "#                        get_nlp(\"the unicorn last unicorn actor voiced\")) #0.9573255410217848\n",
    "#get_similarity_by_words(get_nlp(\"voice actor\"),get_nlp(\"instance of\")) #0.30931508860569823\n",
    "#get_similarity_by_words(get_nlp(\"voice actor\"),get_nlp(\"present in work\")) #0.34966764303274056\n",
    "#get_similarity_by_words(get_nlp(\"voice actor\"),get_nlp(\"subject has role\")) #0.5026860362728758\n",
    "\n",
    "#get_similarity_by_words(get_nlp(\"voice actor\"),get_nlp(\"protagonist\")) #0.4688377364169893\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#subgraphs = [graph.subgraph(c) for c in nx.connected_components(graph)]\n",
    "#print(len(subgraphs))\n",
    "#[len(s.nodes) for s in subgraphs]\n",
    "#len(subgraphs[0].nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for path in nx.all_simple_paths(graph, source=\"Q176198\", target=\"Q202725\"):\n",
    "#    print(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nx.shortest_path(graph, source=\"Q176198\", target=\"Q202725\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nlp_lookup_test = get_nlp(\"klein yves\")\n",
    "#[y['name'] for x,y in graph.nodes(data=True) if get_nlp(y['name']).similarity(nlp_lookup_test) >= 0.9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#list(nx.dfs_labeled_edges(graph, source=get_themes(q0_nlp, top_k=3)[0][0][1][0], depth_limit=4))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot_graph(graph_2, \"test_file_name_graph\", \"Graph_title\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nlp_lookup_test = get_nlp(\"klein yves\")\n",
    "#[y['name'] for x,y in graph.nodes(data=True) if get_nlp(y['name']).similarity(nlp_lookup_test) >= 0.9]\n",
    "#[y['name'] for x,y in graph_2.nodes(data=True) if y['type'] == 'predicate']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get_nlp(get_wd_label(\"Q13133\")).similarity(get_nlp(\"person\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get_similarity_by_words(get_nlp(\"PERSON\"),get_nlp(\"person\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get_similarity_by_words(get_nlp(\"GPE\"),get_nlp(\"location\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get_most_similar(\"Michelle Obama\", top_k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#doc_ents_tmp = get_kb_ents(\"Michelle Obama\")\n",
    "#for ent in doc_ents_tmp:\n",
    "#    print(\" \".join([\"ent\", ent.text, ent.label_, ent.kb_id_]))\n",
    "#\n",
    "#doc_ents_tmp = get_kb_ents(\"New York\")\n",
    "#for ent in doc_ents_tmp:\n",
    "#    print(\" \".join([\"ent\", ent.text, ent.label_, ent.kb_id_]))\n",
    "#    \n",
    "#doc_ents_tmp = get_kb_ents(\"Electrocution\")\n",
    "#for ent in doc_ents_tmp:\n",
    "#    print(\" \".join([\"ent\", ent.text, ent.label_, ent.kb_id_]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nlp.get_vector(\"Q42\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:qa]",
   "language": "python",
   "name": "conda-env-qa-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
